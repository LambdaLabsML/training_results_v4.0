+ echo 'Beginning trial 8 of 10'
Beginning trial 8 of 10
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-llama2_70b_lora:latest 122 4 calvin-training-node-[015-018] lambda_1cc 1cc_4x8x1xtp4pp1cp1'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-llama2_70b_lora:latest 122 4 calvin-training-node-[015-018] lambda_1cc 1cc_4x8x1xtp4pp1cp1
++ srun --ntasks=1 --container-name=llama2_70b_lora_122 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 4 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=4 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on calvin-training-node-017
Clearing cache on calvin-training-node-018
Clearing cache on calvin-training-node-016
Clearing cache on calvin-training-node-015
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=12235
+ SEED=12235
+ srun -l --kill-on-bad-exit=0 --mpi=pmix --ntasks=32 --ntasks-per-node=8 --container-name=llama2_70b_lora_122 --container-mounts=/home/ubuntu/ml-1cc/data/mlperf/llama2_70b_lora/data:/data:ro,/home/ubuntu/ml-1cc/data/mlperf/llama2_70b_lora/ckpt:/ckpt:ro,./results/1cc_4x8x1xtp4pp1cp1_24-09-28_17-31-14:/results:rw,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --export=ALL,MASTER_PORT=29500,MASTER_ADDR=calvin-training-node-015 slurm2pytorch ./run_and_time.sh
 0: STARTING TIMING RUN AT 2024-09-28 07:43:18 PM
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
17: FlashAttention Installed
18: FlashAttention Installed
21: FlashAttention Installed
23: FlashAttention Installed
16: FlashAttention Installed
 8: FlashAttention Installed
12: FlashAttention Installed
15: FlashAttention Installed
 9: FlashAttention Installed
11: FlashAttention Installed
30: FlashAttention Installed
26: FlashAttention Installed
25: FlashAttention Installed
31: FlashAttention Installed
14: FlashAttention Installed
24: FlashAttention Installed
27: FlashAttention Installed
28: FlashAttention Installed
10: FlashAttention Installed
19: FlashAttention Installed
29: FlashAttention Installed
20: FlashAttention Installed
22: FlashAttention Installed
 4: FlashAttention Installed
 6: FlashAttention Installed
 7: FlashAttention Installed
 2: FlashAttention Installed
 0: FlashAttention Installed
 3: FlashAttention Installed
 5: FlashAttention Installed
 1: FlashAttention Installed
13: FlashAttention Installed
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-28 19:43:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'megatron_gpt_peft_tuning_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
 0:       warnings.warn(msg, UserWarning)
 0:     
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-28 19:43:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-28 19:43:35 train:59] 
 0:     
 0:     ************** Experiment configuration ***********
 0: [NeMo I 2024-09-28 19:43:35 train:60] 
 0:     model:
 0:       ub_tp_comm_overlap_cfg:
 0:         qkv_fprop:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         fc1_fprop:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         proj_dgrad:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         fc2_dgrad:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         proj_fprop:
 0:           method: pipeline
 0:           num_sm: 32
 0:           cga_size: 2
 0:           num_splits: 4
 0:           set_sm_margin: 1
 0:           atomic_gemm: 1
 0:         fc2_fprop:
 0:           method: pipeline
 0:           num_sm: 16
 0:           cga_size: 2
 0:           num_splits: 4
 0:           set_sm_margin: 1
 0:           atomic_gemm: 1
 0:         qkv_dgrad:
 0:           method: bulk
 0:           num_sm: 4
 0:           cga_size: 2
 0:           set_sm_margin: 0
 0:         fc1_dgrad:
 0:           method: bulk
 0:           num_sm: 2
 0:           cga_size: 2
 0:           set_sm_margin: 0
 0:       mcore_gpt: true
 0:       seed: 12235
 0:       tensor_model_parallel_size: 4
 0:       pipeline_model_parallel_size: 1
 0:       context_parallel_size: 1
 0:       cpu_offloading: false
 0:       global_batch_size: 8
 0:       micro_batch_size: 1
 0:       max_position_embeddings: 8192
 0:       encoder_seq_length: 8192
 0:       restore_from_path: /ckpt
 0:       resume_from_checkpoint: null
 0:       save_nemo_on_validation_end: false
 0:       sync_batch_comm: false
 0:       megatron_amp_O2: true
 0:       sequence_parallel: 1
 0:       activations_checkpoint_granularity: null
 0:       activations_checkpoint_method: null
 0:       activations_checkpoint_num_layers: null
 0:       activations_checkpoint_layers_per_pipeline: null
 0:       answer_only_loss: true
 0:       gradient_as_bucket_view: false
 0:       hidden_dropout: 0.0
 0:       attention_dropout: 0.0
 0:       ffn_dropout: 0.0
 0:       bias_activation_fusion: true
 0:       bias_dropout_add_fusion: false
 0:       transformer_engine: true
 0:       fp8: true
 0:       fp8_params: true
 0:       fp8_hybrid: true
 0:       fp8_amax_history_len: 32
 0:       fp8_amax_compute_algo: max
 0:       reduce_amax: true
 0:       fp8_e4m3: false
 0:       fp8_interval: 1
 0:       fp8_margin: 0
 0:       fp8_dot_product_attention: 1
 0:       fp8_activation_input_store: 0
 0:       apply_rope_fusion: true
 0:       disable_parameter_transpose_cache: true
 0:       ub_tp_comm_overlap: true
 0:       tp_comm_overlap_ag: true
 0:       tp_comm_overlap_rs: true
 0:       tp_comm_overlap_rs_dgrad: false
 0:       tp_comm_disable_qkv: false
 0:       batch_p2p_comm: 'False'
 0:       virtual_pipeline_model_parallel_size: 1
 0:       sharp: true
 0:       nccl_communicator_config_path: null
 0:       peft:
 0:         peft_scheme: lora
 0:         restore_from_path: null
 0:         lora_tuning:
 0:           adapter_dim: 16
 0:           alpha: 32
 0:           adapter_dropout: 0.1
 0:           dropout_position: pre
 0:           target_modules:
 0:           - attention
 0:           column_init_method: kaiming
 0:           row_init_method: zero
 0:           layer_selection: null
 0:           weight_tying: false
 0:           position_embedding_strategy: null
 0:           a2a_experimental: 1
 0:       data:
 0:         multiprocessing_context: spawn
 0:         pin_memory: true
 0:         sample_weight: constant
 0:         validation_drop_last: false
 0:         train_ds:
 0:           file_names:
 0:           - /data/train.npy
 0:           packed_sequence: true
 0:           packed_sequence_return_cu_seqlen: false
 0:           index_mapping_dir: /results/data_index/train
 0:           global_batch_size: 8
 0:           micro_batch_size: 1
 0:           shuffle: true
 0:           num_workers: 1
 0:           memmap_workers: 2
 0:           pin_memory: true
 0:           max_seq_length: 8192
 0:           min_seq_length: 1
 0:           drop_last: true
 0:           concat_sampling_probabilities:
 0:           - 1.0
 0:           label_key: output
 0:           add_eos: true
 0:           add_sep: false
 0:           add_bos: false
 0:           truncation_field: input
 0:           prompt_template: '{input} {output}'
 0:           truncation_method: right
 0:           seed: 12235
 0:         validation_ds:
 0:           file_names:
 0:           - /data/validation.npy
 0:           packed_sequence: true
 0:           packed_sequence_return_cu_seqlen: false
 0:           index_mapping_dir: /results/data_index/val
 0:           names: null
 0:           global_batch_size: 8
 0:           micro_batch_size: 1
 0:           shuffle: false
 0:           num_workers: 1
 0:           memmap_workers: 2
 0:           pin_memory: true
 0:           max_seq_length: 8192
 0:           min_seq_length: 1
 0:           drop_last: false
 0:           label_key: output
 0:           add_eos: true
 0:           add_sep: false
 0:           add_bos: false
 0:           write_predictions_to_file: false
 0:           output_file_path_prefix: null
 0:           truncation_field: input
 0:           prompt_template: '{input} {output}'
 0:           tokens_to_generate: 32
 0:           truncation_method: right
 0:           metric:
 0:             name: loss
 0:             average: null
 0:             num_classes: null
 0:       optim:
 0:         name: fused_adam
 0:         lr: 0.00036
 0:         weight_decay: 0.0001
 0:         betas:
 0:         - 0.9
 0:         - 0.999
 0:         eps: 1.0e-08
 0:         amsgrad: false
 0:         sched:
 0:           name: CosineAnnealing
 0:           warmup_ratio: 0.0
 0:           min_lr: 0.0
 0:           constant_steps: 0
 0:           monitor: val_loss
 0:           reduce_on_plateau: false
 0:       custom:
 0:         warmup: true
 0:         warmup_train_steps: 5
 0:         warmup_validation_steps: 5
 0:         reset_fp8_stats_after_warmup: 1
 0:     name: megatron_gpt_peft_lora_tuning
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 4
 0:       accelerator: gpu
 0:       precision: bf16-mixed
 0:       max_steps: 1024
 0:       val_check_interval: 192
 0:       check_val_every_n_epoch: null
 0:       log_every_n_steps: 0
 0:       gradient_clip_val: 0.3
 0:       gradient_clip_algorithm: norm
 0:       num_sanity_val_steps: 0
 0:       max_epochs: 1000
 0:       limit_val_batches: 1.0
 0:       limit_train_batches: 1.0
 0:       limit_test_batches: 0
 0:       logger: false
 0:       enable_checkpointing: false
 0:       use_distributed_sampler: false
 0:       enable_progress_bar: false
 0:     exp_manager:
 0:       explicit_log_dir: null
 0:       exp_dir: /results
 0:       create_wandb_logger: false
 0:       resume_if_exists: false
 0:       resume_ignore_no_checkpoint: true
 0:       create_checkpoint_callback: false
 0:       log_global_rank_0_only: true
 0:       create_early_stopping_callback: false
 0:       create_tensorboard_logger: false
 0:     
 0: [NeMo W 2024-09-28 19:43:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
 0:     
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
 0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
 0: [NeMo I 2024-09-28 19:43:35 save_restore_connector:134] Restoration will occur within pre-extracted directory : `/ckpt`.
 0: [NeMo I 2024-09-28 19:43:35 save_restore_connector:134] Restoration will occur within pre-extracted directory : `/ckpt`.
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:265] Rank 0 has data parallel group : [0, 4, 8, 12, 16, 20, 24, 28]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 4, 8, 12, 16, 20, 24, 28]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:298] Rank 0 has model parallel group: [0, 1, 2, 3]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:299] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:308] Rank 0 has tensor model parallel group: [0, 1, 2, 3]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:312] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-28 19:43:35 megatron_init:354] Rank 0 has embedding rank: 0
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: 24-09-28 19:43:35 - PID:2595795 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo I 2024-09-28 19:43:35 tokenizer_utils:187] Getting SentencePiece with model: /ckpt/d9a3fcd8246a432f95ed96c9011cd224_tokenizer.model
 0: [NeMo I 2024-09-28 19:43:35 megatron_base_model:586] Padded vocab_size: 32256, original vocab_size: 32000, dummy tokens: 256.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:499] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-28 19:43:35 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/32
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/32
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/32
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/32
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/32
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/32
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/32
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/32
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/32
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/32
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/32
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/32
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/32
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/32
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/32
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/32
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/32
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/32
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/32
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/32
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/32
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/32
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/32
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/32
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/32
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/32
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/32
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/32
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/32
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/32
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/32
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/32
 2: NCCL version 2.21.5+cuda12.4
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 32 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
 3: NCCL version 2.21.5+cuda12.4
 1: NCCL version 2.21.5+cuda12.4
 0: NCCL version 2.21.5+cuda12.4
 0: [NeMo I 2024-09-28 19:43:53 nlp_overrides:1127] Restoration will occur within pre-extracted directory : `/ckpt`.
 0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
 0: [NeMo I 2024-09-28 19:47:36 nlp_overrides:1156] Model CustomMegatronGPTSFTModel was successfully restored from /ckpt.
 0: [NeMo W 2024-09-28 19:47:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.
 0:     
 0: [NeMo I 2024-09-28 19:47:36 nlp_adapter_mixins:203] Before adding PEFT params:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 133 M 
 0:     ----------------------------------------
 0:     0         Trainable params
 0:     133 M     Non-trainable params
 0:     133 M     Total params
 0:     533.758   Total estimated model params size (MB)
 0: [NeMo I 2024-09-28 19:47:44 nlp_adapter_mixins:208] After adding PEFT params:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 144 M 
 0:     ----------------------------------------
 0:     11.1 M    Trainable params
 0:     133 M     Non-trainable params
 0:     144 M     Total params
 0:     578.322   Total estimated model params size (MB)
 0: [NeMo W 2024-09-28 19:47:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `CustomMegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
 0:     
 0: [NeMo I 2024-09-28 19:47:50 megatron_gpt_sft_model:804] Building GPT SFT validation datasets.
 0: [NeMo I 2024-09-28 19:47:50 megatron_gpt_sft_model:807] Length of val dataset: 173
 0: [NeMo I 2024-09-28 19:47:50 megatron_gpt_sft_model:814] Building GPT SFT traing datasets.
24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
24: make: Nothing to be done for 'default'.
24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: make: Nothing to be done for 'default'.
16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: make: Nothing to be done for 'default'.
 0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Nothing to be done for 'default'.
 8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: > building indices for blendable datasets ...
 0:  > sample ratios:
 0:    dataset 0, input: 1, achieved: 1
 0: [NeMo I 2024-09-28 19:47:52 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.53 (sec)
 0: [NeMo I 2024-09-28 19:47:52 megatron_gpt_sft_model:816] Length of train dataset: 8233
 0: [NeMo I 2024-09-28 19:47:52 megatron_gpt_sft_model:821] Building dataloader with consumed samples: 0
 0: [NeMo I 2024-09-28 19:47:52 megatron_gpt_sft_model:821] Building dataloader with consumed samples: 0
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo W 2024-09-28 19:47:52 megatron_base_model:1191] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1024.
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-28 19:47:52 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-28 19:47:52 nlp_adapter_mixins:269] Optimizer groups set:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 144 M 
 0:     ----------------------------------------
 0:     11.1 M    Trainable params
 0:     133 M     Non-trainable params
 0:     144 M     Total params
 0:     578.322   Total estimated model params size (MB)
 0: [NeMo I 2024-09-28 19:47:52 modelPT:724] Optimizer config = FusedAdam (
 0:     Parameter Group 0
 0:         betas: [0.9, 0.999]
 0:         bias_correction: True
 0:         eps: 1e-08
 0:         lr: 0.00036
 0:         weight_decay: 0.0001
 0:     )
 0: [NeMo I 2024-09-28 19:47:52 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7a612e9dd120>" 
 0:     will be used during training (effective maximum steps = 1024) - 
 0:     Parameters : 
 0:     (warmup_ratio: 0.0
 0:     min_lr: 0.0
 0:     constant_steps: 0
 0:     max_steps: 1024
 0:     )
 0: [NeMo I 2024-09-28 19:47:52 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7a612ed1f970>" 
 0:     will be used during training (effective maximum steps = 1024) - 
 0:     Parameters : 
 0:     (warmup_ratio: 0.0
 0:     min_lr: 0.0
 0:     constant_steps: 0
 0:     max_steps: 1024
 0:     )
 0: 
 0:   | Name  | Type          | Params
 0: ----------------------------------------
 0: 0 | model | Float16Module | 144 M 
 0: ----------------------------------------
 0: 11.1 M    Trainable params
 0: 133 M     Non-trainable params
 0: 144 M     Total params
 0: 578.322   Total estimated model params size (MB)
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872399, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872400, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 207}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "4xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "seed", "value": 12235, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 209}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552872401, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 215}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873149, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 220}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873175, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873175, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 228}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873175, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873175, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873175, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873176, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 242}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873176, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00036, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 243}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873176, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 244}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552873176, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 245}}
17: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [NeMo W 2024-09-28 19:47:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.
 0:     
 0: !!! [UB] Create UbufP2PCommOverlap Communicator
 0: MC initialized succesfully, window size = 549755813888
29: gdrcopy open failed
 1: gdrcopy open failed
24: gdrcopy open failed
23: gdrcopy open failed
 4: gdrcopy open failed
26: gdrcopy open failed
 6: gdrcopy open failed
21: gdrcopy open failed
11: gdrcopy open failed
31: gdrcopy open failed
18: gdrcopy open failed
 2: gdrcopy open failed
 8: gdrcopy open failed
27: gdrcopy open failed
22: gdrcopy open failed
 7: gdrcopy open failed
12: gdrcopy open failed
17: gdrcopy open failed
 0: gdrcopy open failed
15: gdrcopy open failed
 3: gdrcopy open failed
25: gdrcopy open failed
13: gdrcopy open failed
 5: gdrcopy open failed
 9: gdrcopy open failed
28: gdrcopy open failed
20: gdrcopy open failed
30: gdrcopy open failed
14: gdrcopy open failed
10: gdrcopy open failed
19: gdrcopy open failed
16: gdrcopy open failed
 0: !!! [UBP2P] Register UBuf 1
 0: !!! [UBP2P] Register UBuf 2
 0: !!! [UBP2P] Register UBuf 3
 0: !!! [UBP2P] Register UBuf 4
 0: [NeMo W 2024-09-28 19:48:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:119: UserWarning: Atomic GEMM uses a beta API from cublas and is not tested for all use cases.
 0:       warnings.warn(
 0:     
 0: !!! [UB] Register UBuf 5
 0: !!! [UB] Register UBuf 6
 0: !!! [UB] Register UBuf 7
 0: !!! [UB] Register UBuf 8
 0: !!! [UB] Register UBuf 9
 0: !!! [UB] Register UBuf 10
20: NCCL version 2.21.5+cuda12.4
16: NCCL version 2.21.5+cuda12.4
 4: NCCL version 2.21.5+cuda12.4
24: NCCL version 2.21.5+cuda12.4
28: NCCL version 2.21.5+cuda12.4
 8: NCCL version 2.21.5+cuda12.4
12: NCCL version 2.21.5+cuda12.4
 0: [NeMo W 2024-09-28 19:48:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2954: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
 0:       warnings.warn(
 0:     
 0: [NeMo I 2024-09-28 19:49:18 custom_callbacks:40] Finished training warmup: 71.69307851791382s. Starting validation warmup
 0: [NeMo I 2024-09-28 19:49:22 custom_callbacks:54] Time spent in run_training_warmup: 75.08285140991211s
 0: [NeMo I 2024-09-28 19:49:22 custom_callbacks:59] Forcing FP8 stats reinitialization
 0: :::MLLOG {"namespace": "", "time_ms": 1727552962066, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 145}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552962066, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 145}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727552962066, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 146, "samples_count": 0}}
 0: [NeMo W 2024-09-28 19:52:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.
 0:     
 0: :::MLLOG {"namespace": "", "time_ms": 1727553177272, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 7.551271654261221}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553177273, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553177273, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 1536}}
 0: [NeMo W 2024-09-28 19:52:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
 0:       warnings.warn("This function is only for unittest")
 0:     
 0: :::MLLOG {"namespace": "", "time_ms": 1727553188752, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9390575289726257, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553188752, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553188752, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553234830, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 8.340132586553707}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553234831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553234831, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553246005, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9376082420349121, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553246005, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553246005, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553292121, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 8.333196582930068}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553292122, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553292122, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553303435, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9335319399833679, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553303436, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553303436, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553349605, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 8.322992087964597}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553349606, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553349606, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553361165, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9288037419319153, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553361165, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553361165, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553407283, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 8.332307337079731}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553407284, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553407284, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553418868, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9256064295768738, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553418868, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553418868, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553464956, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 8.33765252345346}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553464956, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553464957, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553476536, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9228390455245972, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553476536, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727553476536, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9228390455245972, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 195, "samples_count": 3456, "status": "success"}}
 1: FlashAttention Installed
 2: FlashAttention Installed
21: FlashAttention Installed
 5: FlashAttention Installed
20: FlashAttention Installed
18: FlashAttention Installed
11: FlashAttention Installed
 6: FlashAttention Installed
 0: FlashAttention Installed
17: FlashAttention Installed
 9: FlashAttention Installed
14: FlashAttention Installed
 1: FlashAttention Installed
 2: FlashAttention Installed
21: FlashAttention Installed
28: FlashAttention Installed
27: FlashAttention Installed
26: FlashAttention Installed
 6: FlashAttention Installed
20: FlashAttention Installed
 0: FlashAttention Installed
14: FlashAttention Installed
 5: FlashAttention Installed
18: FlashAttention Installed
11: FlashAttention Installed
 9: FlashAttention Installed
17: FlashAttention Installed
27: FlashAttention Installed
28: FlashAttention Installed
26: FlashAttention Installed
10: FlashAttention Installed
12: FlashAttention Installed
15: FlashAttention Installed
13: FlashAttention Installed
 7: FlashAttention Installed
 4: FlashAttention Installed
 8: FlashAttention Installed
16: FlashAttention Installed
22: FlashAttention Installed
23: FlashAttention Installed
 3: FlashAttention Installed
19: FlashAttention Installed
15: FlashAttention Installed
13: FlashAttention Installed
 4: FlashAttention Installed
 7: FlashAttention Installed
12: FlashAttention Installed
16: FlashAttention Installed
10: FlashAttention Installed
 8: FlashAttention Installed
19: FlashAttention Installed
 3: FlashAttention Installed
23: FlashAttention Installed
22: FlashAttention Installed
29: FlashAttention Installed
30: FlashAttention Installed
31: FlashAttention Installed
24: FlashAttention Installed
25: FlashAttention Installed
29: FlashAttention Installed
31: FlashAttention Installed
24: FlashAttention Installed
30: FlashAttention Installed
25: FlashAttention Installed
 2: [1727553522.885904] [calvin-training-node-015:2595744:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 2: [1727553522.885938] [calvin-training-node-015:2595744:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 2: [1727553522.885943] [calvin-training-node-015:2595744:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 4: [1727553522.886217] [calvin-training-node-015:2595776:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 4: [1727553522.886244] [calvin-training-node-015:2595776:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 4: [1727553522.886251] [calvin-training-node-015:2595776:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 1: [1727553522.886414] [calvin-training-node-015:2595760:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 1: [1727553522.886432] [calvin-training-node-015:2595760:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 1: [1727553522.886438] [calvin-training-node-015:2595760:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 3: [1727553522.886345] [calvin-training-node-015:2595728:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 3: [1727553522.886368] [calvin-training-node-015:2595728:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 3: [1727553522.886373] [calvin-training-node-015:2595728:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 5: [1727553522.886282] [calvin-training-node-015:2595696:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 5: [1727553522.886314] [calvin-training-node-015:2595696:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 5: [1727553522.886320] [calvin-training-node-015:2595696:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 6: [1727553522.886391] [calvin-training-node-015:2595680:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 6: [1727553522.886408] [calvin-training-node-015:2595680:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 6: [1727553522.886412] [calvin-training-node-015:2595680:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 0: [1727553522.886499] [calvin-training-node-015:2595795:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 0: [1727553522.886516] [calvin-training-node-015:2595795:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 0: [1727553522.886522] [calvin-training-node-015:2595795:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 7: [1727553522.886607] [calvin-training-node-015:2595712:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 7: [1727553522.886628] [calvin-training-node-015:2595712:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 7: [1727553522.886634] [calvin-training-node-015:2595712:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
21: [1727553524.128088] [calvin-training-node-017:2746780:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
21: [1727553524.128120] [calvin-training-node-017:2746780:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
21: [1727553524.128125] [calvin-training-node-017:2746780:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
22: [1727553524.128369] [calvin-training-node-017:2746698:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
22: [1727553524.128403] [calvin-training-node-017:2746698:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
22: [1727553524.128409] [calvin-training-node-017:2746698:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
18: [1727553524.128439] [calvin-training-node-017:2746730:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
18: [1727553524.128458] [calvin-training-node-017:2746730:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
18: [1727553524.128463] [calvin-training-node-017:2746730:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
16: [1727553524.128673] [calvin-training-node-017:2746682:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
16: [1727553524.128695] [calvin-training-node-017:2746682:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
16: [1727553524.128699] [calvin-training-node-017:2746682:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
17: [1727553524.128560] [calvin-training-node-017:2746763:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
17: [1727553524.128577] [calvin-training-node-017:2746763:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
17: [1727553524.128581] [calvin-training-node-017:2746763:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
19: [1727553524.128629] [calvin-training-node-017:2746747:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
19: [1727553524.128648] [calvin-training-node-017:2746747:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
19: [1727553524.128653] [calvin-training-node-017:2746747:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
20: [1727553524.128482] [calvin-training-node-017:2746714:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
20: [1727553524.128501] [calvin-training-node-017:2746714:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
20: [1727553524.128506] [calvin-training-node-017:2746714:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
23: [1727553524.128741] [calvin-training-node-017:2746796:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
23: [1727553524.128763] [calvin-training-node-017:2746796:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
23: [1727553524.128768] [calvin-training-node-017:2746796:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 8: [1727553524.985500] [calvin-training-node-016:2587296:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 8: [1727553524.985532] [calvin-training-node-016:2587296:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 8: [1727553524.985538] [calvin-training-node-016:2587296:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 9: [1727553524.985531] [calvin-training-node-016:2587193:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 9: [1727553524.985555] [calvin-training-node-016:2587193:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 9: [1727553524.985560] [calvin-training-node-016:2587193:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
12: [1727553524.985684] [calvin-training-node-016:2587257:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
12: [1727553524.985708] [calvin-training-node-016:2587257:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
12: [1727553524.985715] [calvin-training-node-016:2587257:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
10: [1727553524.985742] [calvin-training-node-016:2587297:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
10: [1727553524.985764] [calvin-training-node-016:2587297:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
10: [1727553524.985769] [calvin-training-node-016:2587297:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
11: [1727553524.985802] [calvin-training-node-016:2587273:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
11: [1727553524.985828] [calvin-training-node-016:2587273:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
11: [1727553524.985833] [calvin-training-node-016:2587273:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
13: [1727553524.985919] [calvin-training-node-016:2587241:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
13: [1727553524.985940] [calvin-training-node-016:2587241:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
13: [1727553524.985946] [calvin-training-node-016:2587241:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
15: [1727553524.985853] [calvin-training-node-016:2587225:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
15: [1727553524.985875] [calvin-training-node-016:2587225:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
15: [1727553524.985880] [calvin-training-node-016:2587225:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
14: [1727553524.986226] [calvin-training-node-016:2587209:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
14: [1727553524.986255] [calvin-training-node-016:2587209:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
14: [1727553524.986259] [calvin-training-node-016:2587209:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
30: [1727553528.193843] [calvin-training-node-018:2578547:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
30: [1727553528.193882] [calvin-training-node-018:2578547:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
30: [1727553528.193889] [calvin-training-node-018:2578547:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
24: [1727553528.194018] [calvin-training-node-018:2578582:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
24: [1727553528.194044] [calvin-training-node-018:2578582:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
24: [1727553528.194049] [calvin-training-node-018:2578582:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
28: [1727553528.194091] [calvin-training-node-018:2578469:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
28: [1727553528.194110] [calvin-training-node-018:2578469:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
28: [1727553528.194114] [calvin-training-node-018:2578469:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
25: [1727553528.194204] [calvin-training-node-018:2578485:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
25: [1727553528.194233] [calvin-training-node-018:2578485:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
25: [1727553528.194239] [calvin-training-node-018:2578485:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
27: [1727553528.194242] [calvin-training-node-018:2578549:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
27: [1727553528.194261] [calvin-training-node-018:2578549:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
27: [1727553528.194266] [calvin-training-node-018:2578549:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
26: [1727553528.194353] [calvin-training-node-018:2578517:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
26: [1727553528.194373] [calvin-training-node-018:2578517:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
26: [1727553528.194377] [calvin-training-node-018:2578517:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
29: [1727553528.194463] [calvin-training-node-018:2578548:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
29: [1727553528.194491] [calvin-training-node-018:2578548:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
29: [1727553528.194497] [calvin-training-node-018:2578548:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
31: [1727553528.194539] [calvin-training-node-018:2578501:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
31: [1727553528.194566] [calvin-training-node-018:2578501:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
31: [1727553528.194572] [calvin-training-node-018:2578501:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 0: ENDING TIMING RUN AT 2024-09-28 07:59:04 PM
 0: RESULT,LLM_FINETUNING,,946,nvidia,2024-09-28 07:43:18 PM
