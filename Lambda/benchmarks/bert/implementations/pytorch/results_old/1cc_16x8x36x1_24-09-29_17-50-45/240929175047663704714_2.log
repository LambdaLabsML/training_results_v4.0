+ echo 'Beginning trial 2 of 3'
Beginning trial 2 of 3
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] '\''unknown'\'' 1CC'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] 'unknown' 1CC
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
Clearing cache on calvin-training-node-017
Clearing cache on calvin-training-node-025
Clearing cache on calvin-training-node-019
Clearing cache on calvin-training-node-024
Clearing cache on calvin-training-node-029
Clearing cache on calvin-training-node-031
Clearing cache on calvin-training-node-020
Clearing cache on calvin-training-node-021
Clearing cache on calvin-training-node-022
Clearing cache on calvin-training-node-027
Clearing cache on calvin-training-node-028
Clearing cache on calvin-training-node-026
Clearing cache on calvin-training-node-030
Clearing cache on calvin-training-node-023
Clearing cache on calvin-training-node-018
Clearing cache on calvin-training-node-016
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=16 --container-name=language_model_137 python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
:::MLLOG {"namespace": "", "time_ms": 1727632581192, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581203, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581206, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581235, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581252, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581290, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581319, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581373, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581424, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581457, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581473, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581745, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581760, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632581791, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632582399, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --container-name=language_model_137 --container-mounts=/home/ubuntu/ml-1cc/data/mlperf/bert/packed_data:/workspace/data_phase2,/home/ubuntu/ml-1cc/data/mlperf/bert/phase1:/workspace/phase1,/home/ubuntu/ml-1cc/data/mlperf/bert/hdf5/eval_varlength:/workspace/evaldata,./results/1cc_16x8x36x1_24-09-29_17-50-45:/results,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --container-workdir=/workspace/bert slurm2pytorch ./run_and_time.sh
124: Run vars: id 137 gpus 8 mparams ''
 40: Run vars: id 137 gpus 8 mparams ''
124: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 40: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
114: Run vars: id 137 gpus 8 mparams ''
 80: Run vars: id 137 gpus 8 mparams ''
114: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
126: Run vars: id 137 gpus 8 mparams ''
 80: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
126: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
104: Run vars: id 137 gpus 8 mparams ''
 43: Run vars: id 137 gpus 8 mparams ''
104: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 43: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
117: Run vars: id 137 gpus 8 mparams ''
117: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 65: Run vars: id 137 gpus 8 mparams ''
 58: Run vars: id 137 gpus 8 mparams ''
 65: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 58: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 89: Run vars: id 137 gpus 8 mparams ''
 27: Run vars: id 137 gpus 8 mparams ''
 89: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
118: Run vars: id 137 gpus 8 mparams ''
 27: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 19: Run vars: id 137 gpus 8 mparams ''
118: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 98: Run vars: id 137 gpus 8 mparams ''
 12: Run vars: id 137 gpus 8 mparams ''
 82: Run vars: id 137 gpus 8 mparams ''
 98: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 19: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 82: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 42: Run vars: id 137 gpus 8 mparams ''
 12: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 14: Run vars: id 137 gpus 8 mparams ''
 42: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 60: Run vars: id 137 gpus 8 mparams ''
 14: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 60: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 18: Run vars: id 137 gpus 8 mparams ''
 18: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 59: Run vars: id 137 gpus 8 mparams ''
 45: Run vars: id 137 gpus 8 mparams ''
 59: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 45: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
108: Run vars: id 137 gpus 8 mparams ''
108: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 74: Run vars: id 137 gpus 8 mparams ''
 64: Run vars: id 137 gpus 8 mparams ''
111: Run vars: id 137 gpus 8 mparams ''
 64: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 74: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 84: Run vars: id 137 gpus 8 mparams ''
 15: Run vars: id 137 gpus 8 mparams ''
111: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 84: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
107: Run vars: id 137 gpus 8 mparams ''
 15: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
107: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 24: Run vars: id 137 gpus 8 mparams ''
 93: Run vars: id 137 gpus 8 mparams ''
 71: Run vars: id 137 gpus 8 mparams ''
 24: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
106: Run vars: id 137 gpus 8 mparams ''
 93: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 71: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
106: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 96: Run vars: id 137 gpus 8 mparams ''
100: Run vars: id 137 gpus 8 mparams ''
 76: Run vars: id 137 gpus 8 mparams ''
 96: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 88: Run vars: id 137 gpus 8 mparams ''
100: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 76: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 88: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
127: Run vars: id 137 gpus 8 mparams ''
127: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 79: Run vars: id 137 gpus 8 mparams ''
103: Run vars: id 137 gpus 8 mparams ''
 79: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
103: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
112: Run vars: id 137 gpus 8 mparams ''
112: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 46: Run vars: id 137 gpus 8 mparams ''
 46: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 78: Run vars: id 137 gpus 8 mparams ''
 37: Run vars: id 137 gpus 8 mparams ''
 78: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 37: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 34: Run vars: id 137 gpus 8 mparams ''
 34: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
115: Run vars: id 137 gpus 8 mparams ''
 38: Run vars: id 137 gpus 8 mparams ''
 73: Run vars: id 137 gpus 8 mparams ''
115: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 38: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 73: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
121: Run vars: id 137 gpus 8 mparams ''
123: Run vars: id 137 gpus 8 mparams ''
 87: Run vars: id 137 gpus 8 mparams ''
 23: Run vars: id 137 gpus 8 mparams ''
121: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 87: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
123: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 52: Run vars: id 137 gpus 8 mparams ''
 23: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 52: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 16: Run vars: id 137 gpus 8 mparams ''
 39: Run vars: id 137 gpus 8 mparams ''
 17: Run vars: id 137 gpus 8 mparams ''
 54: Run vars: id 137 gpus 8 mparams ''
 81: Run vars: id 137 gpus 8 mparams ''
 39: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 16: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 17: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 54: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 81: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 62: Run vars: id 137 gpus 8 mparams ''
116: Run vars: id 137 gpus 8 mparams ''
 68: Run vars: id 137 gpus 8 mparams ''
 62: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
109: Run vars: id 137 gpus 8 mparams ''
116: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 68: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 61: Run vars: id 137 gpus 8 mparams ''
109: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 57: Run vars: id 137 gpus 8 mparams ''
 61: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 32: Run vars: id 137 gpus 8 mparams ''
 57: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 32: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
101: Run vars: id 137 gpus 8 mparams ''
101: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 31: Run vars: id 137 gpus 8 mparams ''
 25: Run vars: id 137 gpus 8 mparams ''
 99: Run vars: id 137 gpus 8 mparams ''
 31: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 25: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 99: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  8: Run vars: id 137 gpus 8 mparams ''
  8: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
110: Run vars: id 137 gpus 8 mparams ''
110: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 94: Run vars: id 137 gpus 8 mparams ''
 55: Run vars: id 137 gpus 8 mparams ''
 94: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 55: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
105: Run vars: id 137 gpus 8 mparams ''
  9: Run vars: id 137 gpus 8 mparams ''
 63: Run vars: id 137 gpus 8 mparams ''
  9: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
105: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 63: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 50: Run vars: id 137 gpus 8 mparams ''
 95: Run vars: id 137 gpus 8 mparams ''
119: Run vars: id 137 gpus 8 mparams ''
 47: Run vars: id 137 gpus 8 mparams ''
 50: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
119: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 41: Run vars: id 137 gpus 8 mparams ''
 90: Run vars: id 137 gpus 8 mparams ''
 95: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 47: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 91: Run vars: id 137 gpus 8 mparams ''
 41: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 75: Run vars: id 137 gpus 8 mparams ''
 90: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 77: Run vars: id 137 gpus 8 mparams ''
 75: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 91: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 77: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
122: Run vars: id 137 gpus 8 mparams ''
122: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
120: Run vars: id 137 gpus 8 mparams ''
125: Run vars: id 137 gpus 8 mparams ''
120: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
125: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 22: Run vars: id 137 gpus 8 mparams ''
 22: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 83: Run vars: id 137 gpus 8 mparams ''
 83: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 56: Run vars: id 137 gpus 8 mparams ''
 56: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 33: Run vars: id 137 gpus 8 mparams ''
 20: Run vars: id 137 gpus 8 mparams ''
 21: Run vars: id 137 gpus 8 mparams ''
 29: Run vars: id 137 gpus 8 mparams ''
 21: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 20: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 33: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 29: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 13: Run vars: id 137 gpus 8 mparams ''
 44: Run vars: id 137 gpus 8 mparams ''
 13: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 44: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 28: Run vars: id 137 gpus 8 mparams ''
 28: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 49: Run vars: id 137 gpus 8 mparams ''
 49: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 70: Run vars: id 137 gpus 8 mparams ''
 70: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 35: Run vars: id 137 gpus 8 mparams ''
 35: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 53: Run vars: id 137 gpus 8 mparams ''
 53: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 92: Run vars: id 137 gpus 8 mparams ''
 48: Run vars: id 137 gpus 8 mparams ''
 92: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 85: Run vars: id 137 gpus 8 mparams ''
 48: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 30: Run vars: id 137 gpus 8 mparams ''
 85: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 30: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 86: Run vars: id 137 gpus 8 mparams ''
 86: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  4: Run vars: id 137 gpus 8 mparams ''
 66: Run vars: id 137 gpus 8 mparams ''
 67: Run vars: id 137 gpus 8 mparams ''
 69: Run vars: id 137 gpus 8 mparams ''
 66: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 67: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
102: Run vars: id 137 gpus 8 mparams ''
 69: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
102: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 72: Run vars: id 137 gpus 8 mparams ''
 72: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  4: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  5: Run vars: id 137 gpus 8 mparams ''
  5: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  6: Run vars: id 137 gpus 8 mparams ''
  6: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 11: Run vars: id 137 gpus 8 mparams ''
 11: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 26: Run vars: id 137 gpus 8 mparams ''
 26: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 51: Run vars: id 137 gpus 8 mparams ''
 51: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 97: Run vars: id 137 gpus 8 mparams ''
 97: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
113: Run vars: id 137 gpus 8 mparams ''
113: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 10: Run vars: id 137 gpus 8 mparams ''
 10: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  0: Run vars: id 137 gpus 8 mparams ''
  0: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  2: Run vars: id 137 gpus 8 mparams ''
  2: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 36: Run vars: id 137 gpus 8 mparams ''
 36: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  1: Run vars: id 137 gpus 8 mparams ''
  1: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  7: Run vars: id 137 gpus 8 mparams ''
  7: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
  3: Run vars: id 137 gpus 8 mparams ''
  3: STARTING TIMING RUN AT 2024-09-29 05:56:30 PM
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 19: [W929 17:56:32.955750678 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [W929 17:56:32.151547529 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 19:   warnings.warn(msg, DeprecatedFeatureWarning)
 89: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 89:   warnings.warn(msg, DeprecatedFeatureWarning)
 93: [W929 17:56:32.215175678 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [W929 17:56:32.573344095 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W929 17:56:32.573344116 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 93:   warnings.warn(msg, DeprecatedFeatureWarning)
 64: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 64:   warnings.warn(msg, DeprecatedFeatureWarning)
 65: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 65:   warnings.warn(msg, DeprecatedFeatureWarning)
 40: [W929 17:56:32.106709268 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W929 17:56:32.106707002 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 40:   warnings.warn(msg, DeprecatedFeatureWarning)
 43: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 43:   warnings.warn(msg, DeprecatedFeatureWarning)
 90: [W929 17:56:32.255791000 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W929 17:56:32.124050661 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:56:32.124085488 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W929 17:56:32.124050099 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 90:   warnings.warn(msg, DeprecatedFeatureWarning)
 80: [W929 17:56:32.530025243 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 12:   warnings.warn(msg, DeprecatedFeatureWarning)
 14: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 14:   warnings.warn(msg, DeprecatedFeatureWarning)
 15: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 15:   warnings.warn(msg, DeprecatedFeatureWarning)
 58: [W929 17:56:32.154791166 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W929 17:56:32.154795400 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 80:   warnings.warn(msg, DeprecatedFeatureWarning)
 58: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 58:   warnings.warn(msg, DeprecatedFeatureWarning)
 60: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 60:   warnings.warn(msg, DeprecatedFeatureWarning)
 88: [W929 17:56:32.294228768 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 88:   warnings.warn(msg, DeprecatedFeatureWarning)
 42: [W929 17:56:32.172613132 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [W929 17:56:32.639980949 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W929 17:56:32.639990532 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:56:32.639978780 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 42:   warnings.warn(msg, DeprecatedFeatureWarning)
104: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
104:   warnings.warn(msg, DeprecatedFeatureWarning)
108: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
108:   warnings.warn(msg, DeprecatedFeatureWarning)
111: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
111:   warnings.warn(msg, DeprecatedFeatureWarning)
121: [W929 17:56:32.420865100 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:56:32.420862534 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:56:32.420878970 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W929 17:56:32.420877533 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:56:32.420872169 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W929 17:56:32.597964315 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W929 17:56:32.598234228 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
121:   warnings.warn(msg, DeprecatedFeatureWarning)
123: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
123:   warnings.warn(msg, DeprecatedFeatureWarning)
124: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
124:   warnings.warn(msg, DeprecatedFeatureWarning)
126: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
126:   warnings.warn(msg, DeprecatedFeatureWarning)
127: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
127:   warnings.warn(msg, DeprecatedFeatureWarning)
 59: [W929 17:56:32.221598190 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 82:   warnings.warn(msg, DeprecatedFeatureWarning)
 84: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 84:   warnings.warn(msg, DeprecatedFeatureWarning)
 59: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 59:   warnings.warn(msg, DeprecatedFeatureWarning)
 18: [W929 17:56:32.150022159 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 18:   warnings.warn(msg, DeprecatedFeatureWarning)
114: [W929 17:56:32.987542264 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W929 17:56:32.987539794 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W929 17:56:32.987538491 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
114:   warnings.warn(msg, DeprecatedFeatureWarning)
115: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
115:   warnings.warn(msg, DeprecatedFeatureWarning)
117: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
117:   warnings.warn(msg, DeprecatedFeatureWarning)
 45: [W929 17:56:32.227212986 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 45:   warnings.warn(msg, DeprecatedFeatureWarning)
 23: [W929 17:56:32.174793531 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 23:   warnings.warn(msg, DeprecatedFeatureWarning)
106: [W929 17:56:32.714956368 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
106:   warnings.warn(msg, DeprecatedFeatureWarning)
  9: [W929 17:56:32.253537292 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W929 17:56:32.723900187 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  9:   warnings.warn(msg, DeprecatedFeatureWarning)
107: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
107:   warnings.warn(msg, DeprecatedFeatureWarning)
 41: [W929 17:56:32.277351995 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W929 17:56:32.764760604 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 41:   warnings.warn(msg, DeprecatedFeatureWarning)
 70: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 70:   warnings.warn(msg, DeprecatedFeatureWarning)
 96: [W929 17:56:32.965209669 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W929 17:56:32.965209513 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W929 17:56:32.965219478 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W929 17:56:32.965209581 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W929 17:56:32.052147393 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
118:   warnings.warn(msg, DeprecatedFeatureWarning)
 96: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 96:   warnings.warn(msg, DeprecatedFeatureWarning)
 98: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 98:   warnings.warn(msg, DeprecatedFeatureWarning)
100: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
100:   warnings.warn(msg, DeprecatedFeatureWarning)
103: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
103:   warnings.warn(msg, DeprecatedFeatureWarning)
113: [W929 17:56:32.059206572 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W929 17:56:32.233718939 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
113:   warnings.warn(msg, DeprecatedFeatureWarning)
 17: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 17:   warnings.warn(msg, DeprecatedFeatureWarning)
 94: [W929 17:56:32.428823921 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [W929 17:56:32.295304803 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 94:   warnings.warn(msg, DeprecatedFeatureWarning)
 46: [W929 17:56:32.301855973 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W929 17:56:32.767754291 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  8:   warnings.warn(msg, DeprecatedFeatureWarning)
 61: [W929 17:56:32.324698858 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 46:   warnings.warn(msg, DeprecatedFeatureWarning)
105: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
105:   warnings.warn(msg, DeprecatedFeatureWarning)
 61: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 61:   warnings.warn(msg, DeprecatedFeatureWarning)
 13: [W929 17:56:32.307471153 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W929 17:56:32.254662400 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 13:   warnings.warn(msg, DeprecatedFeatureWarning)
 71: [W929 17:56:32.801985234 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 20:   warnings.warn(msg, DeprecatedFeatureWarning)
 47: [W929 17:56:32.319137268 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 71:   warnings.warn(msg, DeprecatedFeatureWarning)
 24: [W929 17:56:32.002960537 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W929 17:56:32.002969621 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W929 17:56:32.321047840 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 47:   warnings.warn(msg, DeprecatedFeatureWarning)
122: [W929 17:56:32.548760063 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 44:   warnings.warn(msg, DeprecatedFeatureWarning)
 24: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 24:   warnings.warn(msg, DeprecatedFeatureWarning)
 27: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 27:   warnings.warn(msg, DeprecatedFeatureWarning)
122: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
122:   warnings.warn(msg, DeprecatedFeatureWarning)
109: [W929 17:56:32.795520833 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W929 17:56:32.098978930 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [W929 17:56:32.557388760 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [W929 17:56:32.880116496 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
109:   warnings.warn(msg, DeprecatedFeatureWarning)
 74: [W929 17:56:32.880125254 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:56:32.880125426 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:56:32.880127073 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
119:   warnings.warn(msg, DeprecatedFeatureWarning)
120: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
120:   warnings.warn(msg, DeprecatedFeatureWarning)
 16: [W929 17:56:32.279750049 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 73:   warnings.warn(msg, DeprecatedFeatureWarning)
 74: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 74:   warnings.warn(msg, DeprecatedFeatureWarning)
 76: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 76:   warnings.warn(msg, DeprecatedFeatureWarning)
 79: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 79:   warnings.warn(msg, DeprecatedFeatureWarning)
 57: [W929 17:56:32.358480319 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W929 17:56:32.826827228 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 16:   warnings.warn(msg, DeprecatedFeatureWarning)
116: [W929 17:56:32.109951630 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 57:   warnings.warn(msg, DeprecatedFeatureWarning)
 68: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 68:   warnings.warn(msg, DeprecatedFeatureWarning)
110: [W929 17:56:32.810138138 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
116:   warnings.warn(msg, DeprecatedFeatureWarning)
110: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
110:   warnings.warn(msg, DeprecatedFeatureWarning)
 81: [W929 17:56:32.756255888 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W929 17:56:32.489664289 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 81:   warnings.warn(msg, DeprecatedFeatureWarning)
 92: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 92:   warnings.warn(msg, DeprecatedFeatureWarning)
 21: [W929 17:56:32.305230319 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [W929 17:56:32.132983526 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 21:   warnings.warn(msg, DeprecatedFeatureWarning)
112: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
112:   warnings.warn(msg, DeprecatedFeatureWarning)
 85: [W929 17:56:32.773576358 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:56:32.317583166 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W929 17:56:33.601312444 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 85:   warnings.warn(msg, DeprecatedFeatureWarning)
 22: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 22:   warnings.warn(msg, DeprecatedFeatureWarning)
125: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
125:   warnings.warn(msg, DeprecatedFeatureWarning)
 97: [W929 17:56:33.074325261 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W929 17:56:33.794766751 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 97:   warnings.warn(msg, DeprecatedFeatureWarning)
 10: [W929 17:56:33.395026040 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W929 17:56:33.945232648 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W929 17:56:33.395644548 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 83:   warnings.warn(msg, DeprecatedFeatureWarning)
 10: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 10:   warnings.warn(msg, DeprecatedFeatureWarning)
 78: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 78:   warnings.warn(msg, DeprecatedFeatureWarning)
 11: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 11:   warnings.warn(msg, DeprecatedFeatureWarning)
101: [W929 17:56:33.090609207 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
101:   warnings.warn(msg, DeprecatedFeatureWarning)
 77: [W929 17:56:33.960292206 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 77:   warnings.warn(msg, DeprecatedFeatureWarning)
 87: [W929 17:56:33.822056583 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 87:   warnings.warn(msg, DeprecatedFeatureWarning)
 86: [W929 17:56:33.826182505 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W929 17:56:33.559345291 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 86:   warnings.warn(msg, DeprecatedFeatureWarning)
 95: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 95:   warnings.warn(msg, DeprecatedFeatureWarning)
 62: [W929 17:56:33.457730769 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:56:33.119598829 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W929 17:56:33.120376869 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 62:   warnings.warn(msg, DeprecatedFeatureWarning)
102: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
102:   warnings.warn(msg, DeprecatedFeatureWarning)
 99: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 99:   warnings.warn(msg, DeprecatedFeatureWarning)
 56: [W929 17:56:33.463104715 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 56:   warnings.warn(msg, DeprecatedFeatureWarning)
 30: [W929 17:56:33.131231366 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W929 17:56:33.581487993 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 30:   warnings.warn(msg, DeprecatedFeatureWarning)
 91: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 91:   warnings.warn(msg, DeprecatedFeatureWarning)
 63: [W929 17:56:33.485644231 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 63:   warnings.warn(msg, DeprecatedFeatureWarning)
 75: [W929 17:56:33.046081435 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W929 17:56:33.184608383 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 75:   warnings.warn(msg, DeprecatedFeatureWarning)
 25: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 25:   warnings.warn(msg, DeprecatedFeatureWarning)
 69: [W929 17:56:33.993817888 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 69:   warnings.warn(msg, DeprecatedFeatureWarning)
 34: [W929 17:56:33.843824839 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W929 17:56:33.843825027 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W929 17:56:33.843826964 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:56:33.843825975 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W929 17:56:33.196455906 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [W929 17:56:33.060956449 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W929 17:56:33.847312961 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 34:   warnings.warn(msg, DeprecatedFeatureWarning)
 37: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 37:   warnings.warn(msg, DeprecatedFeatureWarning)
 38: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 38:   warnings.warn(msg, DeprecatedFeatureWarning)
 39: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 39:   warnings.warn(msg, DeprecatedFeatureWarning)
 26: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 26:   warnings.warn(msg, DeprecatedFeatureWarning)
 49: [W929 17:56:33.031063437 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:56:33.031052957 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W929 17:56:33.031051629 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W929 17:56:33.031057660 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W929 17:56:33.031053856 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 72:   warnings.warn(msg, DeprecatedFeatureWarning)
 35: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 35:   warnings.warn(msg, DeprecatedFeatureWarning)
 49: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 49:   warnings.warn(msg, DeprecatedFeatureWarning)
 50: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 50:   warnings.warn(msg, DeprecatedFeatureWarning)
 52: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 52:   warnings.warn(msg, DeprecatedFeatureWarning)
 54: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 54:   warnings.warn(msg, DeprecatedFeatureWarning)
 55: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 55:   warnings.warn(msg, DeprecatedFeatureWarning)
 66: [W929 17:56:33.023383680 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 66:   warnings.warn(msg, DeprecatedFeatureWarning)
 33: [W929 17:56:33.882658591 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 33:   warnings.warn(msg, DeprecatedFeatureWarning)
 28: [W929 17:56:33.239458039 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:56:33.240297043 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:56:33.240328624 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 28:   warnings.warn(msg, DeprecatedFeatureWarning)
 29: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 29:   warnings.warn(msg, DeprecatedFeatureWarning)
 31: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 31:   warnings.warn(msg, DeprecatedFeatureWarning)
 48: [W929 17:56:33.081209858 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 48:   warnings.warn(msg, DeprecatedFeatureWarning)
 67: [W929 17:56:33.061041533 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 67:   warnings.warn(msg, DeprecatedFeatureWarning)
 53: [W929 17:56:33.095727709 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [W929 17:56:33.097500989 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [W929 17:56:33.918107708 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 53:   warnings.warn(msg, DeprecatedFeatureWarning)
 51: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 51:   warnings.warn(msg, DeprecatedFeatureWarning)
 32: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 32:   warnings.warn(msg, DeprecatedFeatureWarning)
 36: [W929 17:56:33.923171341 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 36:   warnings.warn(msg, DeprecatedFeatureWarning)
  0: [W929 17:56:33.645880588 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:56:33.645883206 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W929 17:56:33.645878430 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:56:33.645881212 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  0:   warnings.warn(msg, DeprecatedFeatureWarning)
  4: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  4:   warnings.warn(msg, DeprecatedFeatureWarning)
  5: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  5:   warnings.warn(msg, DeprecatedFeatureWarning)
  6: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  6:   warnings.warn(msg, DeprecatedFeatureWarning)
  2: [W929 17:56:33.682632451 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  2:   warnings.warn(msg, DeprecatedFeatureWarning)
  3: [W929 17:56:33.849383050 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W929 17:56:33.853150344 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  3:   warnings.warn(msg, DeprecatedFeatureWarning)
  1: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  1:   warnings.warn(msg, DeprecatedFeatureWarning)
  7: [W929 17:56:33.899036612 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  7:   warnings.warn(msg, DeprecatedFeatureWarning)
 41: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 41:   from jax.experimental.maps import xmap
 42: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 42:   from jax.experimental.maps import xmap
106: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
106:   from jax.experimental.maps import xmap
109: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
109:   from jax.experimental.maps import xmap
 93: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 93:   from jax.experimental.maps import xmap
 94: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 94:   from jax.experimental.maps import xmap
120: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
120:   from jax.experimental.maps import xmap
124: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
124:   from jax.experimental.maps import xmap
 88: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 88:   from jax.experimental.maps import xmap
 83: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 83:   from jax.experimental.maps import xmap
 84: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 84:   from jax.experimental.maps import xmap
 85: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 85:   from jax.experimental.maps import xmap
111: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
111:   from jax.experimental.maps import xmap
 64: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 64:   from jax.experimental.maps import xmap
 70: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 70:   from jax.experimental.maps import xmap
 65: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 65:   from jax.experimental.maps import xmap
125: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
125:   from jax.experimental.maps import xmap
 44: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 44:   from jax.experimental.maps import xmap
 56: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 56:   from jax.experimental.maps import xmap
 57: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 57:   from jax.experimental.maps import xmap
 58: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 58:   from jax.experimental.maps import xmap
 71: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 71:   from jax.experimental.maps import xmap
 86: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 86:   from jax.experimental.maps import xmap
113: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
113:   from jax.experimental.maps import xmap
118: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
118:   from jax.experimental.maps import xmap
 47: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 47:   from jax.experimental.maps import xmap
 90: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 90:   from jax.experimental.maps import xmap
 89: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 89:   from jax.experimental.maps import xmap
 81: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 81:   from jax.experimental.maps import xmap
105: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
105:   from jax.experimental.maps import xmap
 82: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 82:   from jax.experimental.maps import xmap
 76: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 76:   from jax.experimental.maps import xmap
 61: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 61:   from jax.experimental.maps import xmap
 46: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 46:   from jax.experimental.maps import xmap
 80: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 80:   from jax.experimental.maps import xmap
104: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
104:   from jax.experimental.maps import xmap
116: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
116:   from jax.experimental.maps import xmap
 97: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 97:   from jax.experimental.maps import xmap
 98: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 98:   from jax.experimental.maps import xmap
102: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
102:   from jax.experimental.maps import xmap
103: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
103:   from jax.experimental.maps import xmap
 91: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 91:   from jax.experimental.maps import xmap
101: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
101:   from jax.experimental.maps import xmap
119: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
119:   from jax.experimental.maps import xmap
 45: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 45:   from jax.experimental.maps import xmap
 43: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 43:   from jax.experimental.maps import xmap
 40: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 40:   from jax.experimental.maps import xmap
 10: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 10:   from jax.experimental.maps import xmap
 87: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 87:   from jax.experimental.maps import xmap
 11: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 11:   from jax.experimental.maps import xmap
 15: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 15:   from jax.experimental.maps import xmap
 69: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 69:   from jax.experimental.maps import xmap
108: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
108:   from jax.experimental.maps import xmap
 92: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 92:   from jax.experimental.maps import xmap
 18: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 18:   from jax.experimental.maps import xmap
 21: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 21:   from jax.experimental.maps import xmap
 22: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 22:   from jax.experimental.maps import xmap
 95: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 95:   from jax.experimental.maps import xmap
 59: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 59:   from jax.experimental.maps import xmap
126: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
126:   from jax.experimental.maps import xmap
127: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
127:   from jax.experimental.maps import xmap
123: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
123:   from jax.experimental.maps import xmap
 14: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 14:   from jax.experimental.maps import xmap
 17: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 17:   from jax.experimental.maps import xmap
 68: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 68:   from jax.experimental.maps import xmap
121: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
121:   from jax.experimental.maps import xmap
114: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
114:   from jax.experimental.maps import xmap
 75: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 75:   from jax.experimental.maps import xmap
 78: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 78:   from jax.experimental.maps import xmap
 23: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 23:   from jax.experimental.maps import xmap
 66: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 66:   from jax.experimental.maps import xmap
122: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
122:   from jax.experimental.maps import xmap
 62: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 62:   from jax.experimental.maps import xmap
 67: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 67:   from jax.experimental.maps import xmap
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
100:   from jax.experimental.maps import xmap
 74: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 74:   from jax.experimental.maps import xmap
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 60:   from jax.experimental.maps import xmap
 63: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 63:   from jax.experimental.maps import xmap
107: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
107:   from jax.experimental.maps import xmap
110: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
110:   from jax.experimental.maps import xmap
 33: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 33:   from jax.experimental.maps import xmap
 35: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 35:   from jax.experimental.maps import xmap
117: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
117:   from jax.experimental.maps import xmap
 77: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 77:   from jax.experimental.maps import xmap
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
115:   from jax.experimental.maps import xmap
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 16:   from jax.experimental.maps import xmap
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 29:   from jax.experimental.maps import xmap
 30: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 30:   from jax.experimental.maps import xmap
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 20:   from jax.experimental.maps import xmap
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 13:   from jax.experimental.maps import xmap
 12: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 12:   from jax.experimental.maps import xmap
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 19:   from jax.experimental.maps import xmap
  9: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  9:   from jax.experimental.maps import xmap
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 72:   from jax.experimental.maps import xmap
 38: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 38:   from jax.experimental.maps import xmap
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 96:   from jax.experimental.maps import xmap
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 52:   from jax.experimental.maps import xmap
 55: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 55:   from jax.experimental.maps import xmap
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
112:   from jax.experimental.maps import xmap
 99: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 99:   from jax.experimental.maps import xmap
  8: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  8:   from jax.experimental.maps import xmap
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 73:   from jax.experimental.maps import xmap
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 79:   from jax.experimental.maps import xmap
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 31:   from jax.experimental.maps import xmap
 28: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 28:   from jax.experimental.maps import xmap
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 32:   from jax.experimental.maps import xmap
  2: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  2:   from jax.experimental.maps import xmap
 39: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 39:   from jax.experimental.maps import xmap
 34: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 34:   from jax.experimental.maps import xmap
 48: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 48:   from jax.experimental.maps import xmap
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 49:   from jax.experimental.maps import xmap
 26: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 26:   from jax.experimental.maps import xmap
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 27:   from jax.experimental.maps import xmap
 54: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 54:   from jax.experimental.maps import xmap
  6: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  6:   from jax.experimental.maps import xmap
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 53:   from jax.experimental.maps import xmap
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 37:   from jax.experimental.maps import xmap
 50: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 50:   from jax.experimental.maps import xmap
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 24:   from jax.experimental.maps import xmap
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 36:   from jax.experimental.maps import xmap
 51: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 51:   from jax.experimental.maps import xmap
 25: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 25:   from jax.experimental.maps import xmap
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  5:   from jax.experimental.maps import xmap
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  1:   from jax.experimental.maps import xmap
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 42: 2024-09-29 17:56:41.688777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 44: 2024-09-29 17:56:41.688773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: 2024-09-29 17:56:41.698279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 47: 2024-09-29 17:56:41.699568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 41: 2024-09-29 17:56:41.700265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  0:   from jax.experimental.maps import xmap
 45: 2024-09-29 17:56:41.714505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  4:   from jax.experimental.maps import xmap
 43: 2024-09-29 17:56:41.722517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 40: 2024-09-29 17:56:41.726991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
105: 2024-09-29 17:56:41.788551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
106: 2024-09-29 17:56:41.788551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
109: 2024-09-29 17:56:41.788554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
111: 2024-09-29 17:56:41.788554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
107: 2024-09-29 17:56:41.795533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
108: 2024-09-29 17:56:41.798409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
104: 2024-09-29 17:56:41.799534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
110: 2024-09-29 17:56:41.806171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 89: 2024-09-29 17:56:41.811161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 90: 2024-09-29 17:56:41.811161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 93: 2024-09-29 17:56:41.811163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 94: 2024-09-29 17:56:41.811162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 95: 2024-09-29 17:56:41.811160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: 2024-09-29 17:56:41.830242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 91: 2024-09-29 17:56:41.831967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 92: 2024-09-29 17:56:41.835527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  3:   from jax.experimental.maps import xmap
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  7:   from jax.experimental.maps import xmap
 81: 2024-09-29 17:56:41.867506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 82: 2024-09-29 17:56:41.867506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 84: 2024-09-29 17:56:41.867506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 85: 2024-09-29 17:56:41.867506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
120: 2024-09-29 17:56:41.882834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
121: 2024-09-29 17:56:41.882828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
123: 2024-09-29 17:56:41.882828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
124: 2024-09-29 17:56:41.882828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
125: 2024-09-29 17:56:41.882834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: 2024-09-29 17:56:41.895517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 80: 2024-09-29 17:56:41.895844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: 2024-09-29 17:56:41.898725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 87: 2024-09-29 17:56:41.900178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: 2024-09-29 17:56:41.907158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
122: 2024-09-29 17:56:41.909465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
127: 2024-09-29 17:56:41.909697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 69: 2024-09-29 17:56:41.932541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 70: 2024-09-29 17:56:41.932538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 71: 2024-09-29 17:56:41.932541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: 2024-09-29 17:56:41.940664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 57: 2024-09-29 17:56:41.940664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 59: 2024-09-29 17:56:41.940664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 61: 2024-09-29 17:56:41.940664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: 2024-09-29 17:56:41.945664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 66: 2024-09-29 17:56:41.945640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 68: 2024-09-29 17:56:41.949077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: 2024-09-29 17:56:41.952198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
116: 2024-09-29 17:56:41.952199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
117: 2024-09-29 17:56:41.952199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
118: 2024-09-29 17:56:41.952197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
119: 2024-09-29 17:56:41.952197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: 2024-09-29 17:56:41.958319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: 2024-09-29 17:56:41.959465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: 2024-09-29 17:56:41.961967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: 2024-09-29 17:56:41.973117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 65: 2024-09-29 17:56:41.974370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 67: 2024-09-29 17:56:41.974594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: 2024-09-29 17:56:41.979189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: 2024-09-29 17:56:41.985154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 75: 2024-09-29 17:56:41.985154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 76: 2024-09-29 17:56:41.985155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 77: 2024-09-29 17:56:41.985155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 74: 2024-09-29 17:56:41.986476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 78: 2024-09-29 17:56:41.986828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: 2024-09-29 17:56:41.985827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: 2024-09-29 17:56:42.017414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 40:   from jax import xla_computation as _xla_computation
 41: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 41:   from jax import xla_computation as _xla_computation
 42: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 42:   from jax import xla_computation as _xla_computation
 43: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 43:   from jax import xla_computation as _xla_computation
 44: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 44:   from jax import xla_computation as _xla_computation
 45: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 45:   from jax import xla_computation as _xla_computation
 46: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 46:   from jax import xla_computation as _xla_computation
 47: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 47:   from jax import xla_computation as _xla_computation
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: 2024-09-29 17:56:42.047617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: 2024-09-29 17:56:42.073685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: 2024-09-29 17:56:42.080841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: 2024-09-29 17:56:42.080842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: 2024-09-29 17:56:42.080841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: 2024-09-29 17:56:42.080841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: 2024-09-29 17:56:42.092285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
100: 2024-09-29 17:56:42.092945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 96: 2024-09-29 17:56:42.096313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
102: 2024-09-29 17:56:42.096725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
104:   from jax import xla_computation as _xla_computation
105: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
105:   from jax import xla_computation as _xla_computation
106: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
106:   from jax import xla_computation as _xla_computation
107: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
107:   from jax import xla_computation as _xla_computation
108: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
108:   from jax import xla_computation as _xla_computation
109: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
109:   from jax import xla_computation as _xla_computation
110: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
110:   from jax import xla_computation as _xla_computation
111: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
111:   from jax import xla_computation as _xla_computation
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 88:   from jax import xla_computation as _xla_computation
 89: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 89:   from jax import xla_computation as _xla_computation
 90: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 90:   from jax import xla_computation as _xla_computation
 91: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 91:   from jax import xla_computation as _xla_computation
 92: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 92:   from jax import xla_computation as _xla_computation
 93: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 93:   from jax import xla_computation as _xla_computation
 94: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 94:   from jax import xla_computation as _xla_computation
 95: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 95:   from jax import xla_computation as _xla_computation
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 80:   from jax import xla_computation as _xla_computation
 81: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 81:   from jax import xla_computation as _xla_computation
 82: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 82:   from jax import xla_computation as _xla_computation
 83: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 83:   from jax import xla_computation as _xla_computation
 84: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 84:   from jax import xla_computation as _xla_computation
 85: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 85:   from jax import xla_computation as _xla_computation
 86: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 86:   from jax import xla_computation as _xla_computation
 87: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 87:   from jax import xla_computation as _xla_computation
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
120: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
120:   from jax import xla_computation as _xla_computation
121: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
121:   from jax import xla_computation as _xla_computation
122: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
122:   from jax import xla_computation as _xla_computation
123: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
123:   from jax import xla_computation as _xla_computation
124: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
124:   from jax import xla_computation as _xla_computation
125: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
125:   from jax import xla_computation as _xla_computation
126: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
126:   from jax import xla_computation as _xla_computation
127: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
127:   from jax import xla_computation as _xla_computation
 33: 2024-09-29 17:56:42.243109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 34: 2024-09-29 17:56:42.243109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 38: 2024-09-29 17:56:42.243109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 39: 2024-09-29 17:56:42.243109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 35: 2024-09-29 17:56:42.250019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: 2024-09-29 17:56:42.276820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  8: 2024-09-29 17:56:42.272177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  9: 2024-09-29 17:56:42.272161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 10: 2024-09-29 17:56:42.272161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 11: 2024-09-29 17:56:42.272160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 12: 2024-09-29 17:56:42.272167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 13: 2024-09-29 17:56:42.272174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 14: 2024-09-29 17:56:42.272167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 15: 2024-09-29 17:56:42.272167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 64:   from jax import xla_computation as _xla_computation
 65: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 65:   from jax import xla_computation as _xla_computation
 66: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 66:   from jax import xla_computation as _xla_computation
 67: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 67:   from jax import xla_computation as _xla_computation
 68: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 68:   from jax import xla_computation as _xla_computation
 69: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 69:   from jax import xla_computation as _xla_computation
 70: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 70:   from jax import xla_computation as _xla_computation
 71: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 71:   from jax import xla_computation as _xla_computation
 56: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 56:   from jax import xla_computation as _xla_computation
 57: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 57:   from jax import xla_computation as _xla_computation
 58: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 58:   from jax import xla_computation as _xla_computation
 59: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 59:   from jax import xla_computation as _xla_computation
 60: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 60:   from jax import xla_computation as _xla_computation
 61: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 61:   from jax import xla_computation as _xla_computation
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 62:   from jax import xla_computation as _xla_computation
 63: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 63:   from jax import xla_computation as _xla_computation
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: 2024-09-29 17:56:42.312947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
112: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
112:   from jax import xla_computation as _xla_computation
113: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
113:   from jax import xla_computation as _xla_computation
114: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
114:   from jax import xla_computation as _xla_computation
115: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
115:   from jax import xla_computation as _xla_computation
116: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
116:   from jax import xla_computation as _xla_computation
117: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
117:   from jax import xla_computation as _xla_computation
118: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
118:   from jax import xla_computation as _xla_computation
119: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
119:   from jax import xla_computation as _xla_computation
 16: 2024-09-29 17:56:42.320883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 17: 2024-09-29 17:56:42.320884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 18: 2024-09-29 17:56:42.320884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 19: 2024-09-29 17:56:42.320884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 20: 2024-09-29 17:56:42.320876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 21: 2024-09-29 17:56:42.320877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 22: 2024-09-29 17:56:42.320877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 23: 2024-09-29 17:56:42.320877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 72:   from jax import xla_computation as _xla_computation
 73: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 73:   from jax import xla_computation as _xla_computation
 74: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 74:   from jax import xla_computation as _xla_computation
 75: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 75:   from jax import xla_computation as _xla_computation
 76: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 76:   from jax import xla_computation as _xla_computation
 77: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 77:   from jax import xla_computation as _xla_computation
 78: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 78:   from jax import xla_computation as _xla_computation
 79: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 79:   from jax import xla_computation as _xla_computation
 36: 2024-09-29 17:56:42.340405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 97:   from jax import xla_computation as _xla_computation
 98: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 98:   from jax import xla_computation as _xla_computation
100: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
100:   from jax import xla_computation as _xla_computation
101: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
101:   from jax import xla_computation as _xla_computation
102: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
102:   from jax import xla_computation as _xla_computation
103: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
103:   from jax import xla_computation as _xla_computation
 96: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 96:   from jax import xla_computation as _xla_computation
 99: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 99:   from jax import xla_computation as _xla_computation
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: 2024-09-29 17:56:42.454232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 26: 2024-09-29 17:56:42.454232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 27: 2024-09-29 17:56:42.454228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 28: 2024-09-29 17:56:42.454233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 29: 2024-09-29 17:56:42.454228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 30: 2024-09-29 17:56:42.454232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 31: 2024-09-29 17:56:42.454233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 25: 2024-09-29 17:56:42.455472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 48: 2024-09-29 17:56:42.458396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 49: 2024-09-29 17:56:42.458396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 50: 2024-09-29 17:56:42.458396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 51: 2024-09-29 17:56:42.458402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 53: 2024-09-29 17:56:42.458396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 54: 2024-09-29 17:56:42.458482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 55: 2024-09-29 17:56:42.458397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 52: 2024-09-29 17:56:42.458650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 32:   from jax import xla_computation as _xla_computation
 33: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 33:   from jax import xla_computation as _xla_computation
 34: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 34:   from jax import xla_computation as _xla_computation
 35: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 35:   from jax import xla_computation as _xla_computation
 36: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 36:   from jax import xla_computation as _xla_computation
 37: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 37:   from jax import xla_computation as _xla_computation
 38: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 38:   from jax import xla_computation as _xla_computation
 39: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 39:   from jax import xla_computation as _xla_computation
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  8:   from jax import xla_computation as _xla_computation
  9: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  9:   from jax import xla_computation as _xla_computation
 10: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 10:   from jax import xla_computation as _xla_computation
 11: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 11:   from jax import xla_computation as _xla_computation
 12: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 12:   from jax import xla_computation as _xla_computation
 13: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 13:   from jax import xla_computation as _xla_computation
 14: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 14:   from jax import xla_computation as _xla_computation
 15: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 15:   from jax import xla_computation as _xla_computation
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: 2024-09-29 17:56:42.717026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  2: 2024-09-29 17:56:42.717026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: 2024-09-29 17:56:42.717020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  5: 2024-09-29 17:56:42.717024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  6: 2024-09-29 17:56:42.717024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 16: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 16:   from jax import xla_computation as _xla_computation
 17: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 17:   from jax import xla_computation as _xla_computation
 18: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 18:   from jax import xla_computation as _xla_computation
 19: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 19:   from jax import xla_computation as _xla_computation
 20: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 20:   from jax import xla_computation as _xla_computation
 21: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 21:   from jax import xla_computation as _xla_computation
 22: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 22:   from jax import xla_computation as _xla_computation
 23: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 23:   from jax import xla_computation as _xla_computation
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: 2024-09-29 17:56:42.800161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 24:   from jax import xla_computation as _xla_computation
 25: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 25:   from jax import xla_computation as _xla_computation
 26: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 26:   from jax import xla_computation as _xla_computation
 27: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 27:   from jax import xla_computation as _xla_computation
 28: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 28:   from jax import xla_computation as _xla_computation
 29: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 29:   from jax import xla_computation as _xla_computation
 30: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 30:   from jax import xla_computation as _xla_computation
 31: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 31:   from jax import xla_computation as _xla_computation
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 48:   from jax import xla_computation as _xla_computation
 49: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 49:   from jax import xla_computation as _xla_computation
 50: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 50:   from jax import xla_computation as _xla_computation
 51: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 51:   from jax import xla_computation as _xla_computation
 52: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 52:   from jax import xla_computation as _xla_computation
 53: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 53:   from jax import xla_computation as _xla_computation
 54: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 54:   from jax import xla_computation as _xla_computation
 55: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 55:   from jax import xla_computation as _xla_computation
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: 2024-09-29 17:56:42.893531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  7: 2024-09-29 17:56:42.893827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  0:   from jax import xla_computation as _xla_computation
  1: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  1:   from jax import xla_computation as _xla_computation
  2: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  2:   from jax import xla_computation as _xla_computation
  3: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  3:   from jax import xla_computation as _xla_computation
  4: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  4:   from jax import xla_computation as _xla_computation
  5: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  5:   from jax import xla_computation as _xla_computation
  6: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  6:   from jax import xla_computation as _xla_computation
  7: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  7:   from jax import xla_computation as _xla_computation
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: [W929 17:56:43.550488553 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W929 17:56:43.550477388 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [W929 17:56:43.550453455 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W929 17:56:43.550486991 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W929 17:56:43.550492107 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W929 17:56:43.550484560 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W929 17:56:43.550485538 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W929 17:56:43.550483718 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 44: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 43: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 47: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 46: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 40: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 41: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 45: :::MLLOG {"namespace": "", "time_ms": 1727632603186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: [W929 17:56:43.119350378 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W929 17:56:43.119350464 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W929 17:56:43.119387927 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W929 17:56:43.119398105 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W929 17:56:43.119359342 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W929 17:56:43.119393911 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:56:43.119346903 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [W929 17:56:43.119583608 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
105: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
104: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
106: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
110: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
111: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
108: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
107: :::MLLOG {"namespace": "", "time_ms": 1727632603287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: [W929 17:56:43.101387924 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [W929 17:56:43.101337175 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W929 17:56:43.101333729 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W929 17:56:43.101364847 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W929 17:56:43.101327844 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [W929 17:56:43.101327251 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [W929 17:56:43.101374588 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [W929 17:56:43.101370030 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 82: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 85: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 84: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 83: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 86: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 87: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 80: :::MLLOG {"namespace": "", "time_ms": 1727632603334, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 88: [W929 17:56:43.849624040 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [W929 17:56:43.849681815 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [W929 17:56:43.849577337 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W929 17:56:43.849611540 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W929 17:56:43.849692960 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W929 17:56:43.849603964 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W929 17:56:43.849575654 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W929 17:56:43.849665898 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 89: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 90: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 88: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 94: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 91: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 92: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 95: :::MLLOG {"namespace": "", "time_ms": 1727632603349, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
120: [W929 17:56:43.959930975 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W929 17:56:43.959960290 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W929 17:56:43.959972450 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:56:43.959925748 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W929 17:56:43.959965774 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:56:43.960087969 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:56:43.960124413 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W929 17:56:43.960185554 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
124: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
121: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
122: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
123: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
125: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
126: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
127: :::MLLOG {"namespace": "", "time_ms": 1727632603370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: [W929 17:56:43.284830026 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [W929 17:56:43.284828073 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W929 17:56:43.284793157 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W929 17:56:43.284849373 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W929 17:56:43.284850640 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W929 17:56:43.284835773 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W929 17:56:43.284842376 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W929 17:56:43.284841185 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 67: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 68: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 69: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 70: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 71: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 64: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 65: :::MLLOG {"namespace": "", "time_ms": 1727632603433, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
112: [W929 17:56:43.574221202 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [W929 17:56:43.574169253 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W929 17:56:43.574220494 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W929 17:56:43.574230677 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W929 17:56:43.574196807 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W929 17:56:43.574200185 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W929 17:56:43.574175634 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W929 17:56:43.574176979 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: :::MLLOG {"namespace": "", "time_ms": 1727632603439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
116: :::MLLOG {"namespace": "", "time_ms": 1727632603439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
117: :::MLLOG {"namespace": "", "time_ms": 1727632603439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
118: :::MLLOG {"namespace": "", "time_ms": 1727632603439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
119: :::MLLOG {"namespace": "", "time_ms": 1727632603439, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
112: :::MLLOG {"namespace": "", "time_ms": 1727632603440, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
114: :::MLLOG {"namespace": "", "time_ms": 1727632603440, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
115: :::MLLOG {"namespace": "", "time_ms": 1727632603440, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 72: [W929 17:56:43.370235423 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [W929 17:56:43.370256471 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W929 17:56:43.370260507 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W929 17:56:43.370235702 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:56:43.370241402 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W929 17:56:43.370235730 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W929 17:56:43.370261727 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:56:43.370268509 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 72: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 75: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 76: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 74: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 73: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 78: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 79: :::MLLOG {"namespace": "", "time_ms": 1727632603461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: [W929 17:56:43.897643815 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W929 17:56:43.897643627 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [W929 17:56:43.897709935 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W929 17:56:43.897704639 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W929 17:56:43.897687153 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W929 17:56:43.897656399 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [W929 17:56:43.897673741 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W929 17:56:43.897672118 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 57: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 59: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 58: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 62: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 63: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 60: :::MLLOG {"namespace": "", "time_ms": 1727632603513, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: [W929 17:56:43.624377571 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W929 17:56:43.624348241 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W929 17:56:43.624348294 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W929 17:56:43.624391627 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W929 17:56:43.624378144 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W929 17:56:43.624348702 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:56:43.624367429 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W929 17:56:43.624348535 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
101: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 96: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 97: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 98: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 99: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
100: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
102: :::MLLOG {"namespace": "", "time_ms": 1727632603577, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: [W929 17:56:43.434716775 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [W929 17:56:43.434689904 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W929 17:56:43.434689864 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W929 17:56:43.434719020 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W929 17:56:43.434722729 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W929 17:56:43.434727804 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W929 17:56:43.434691855 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:56:43.434691753 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 38: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 39: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 33: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 32: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 35: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 37: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 36: :::MLLOG {"namespace": "", "time_ms": 1727632603740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: [W929 17:56:43.306938688 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W929 17:56:43.306886751 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W929 17:56:43.306886682 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W929 17:56:43.306887474 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W929 17:56:43.306892498 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [W929 17:56:43.306916727 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:56:43.306892583 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W929 17:56:43.306895288 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 10: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 11: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 12: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 13: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 14: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 15: :::MLLOG {"namespace": "", "time_ms": 1727632603943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: [W929 17:56:44.095300784 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W929 17:56:44.095294453 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:56:44.095286136 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W929 17:56:44.095267503 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W929 17:56:44.095324651 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W929 17:56:44.095350106 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W929 17:56:44.095356036 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:56:44.095341303 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: :::MLLOG {"namespace": "", "time_ms": 1727632604045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 30: :::MLLOG {"namespace": "", "time_ms": 1727632604045, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 26: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 27: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 25: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 28: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 31: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 24: :::MLLOG {"namespace": "", "time_ms": 1727632604046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 48: [W929 17:56:44.937973853 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [W929 17:56:44.937973769 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:56:44.937973962 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [W929 17:56:44.938014368 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W929 17:56:44.937979774 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [W929 17:56:44.938005242 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W929 17:56:44.937982346 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W929 17:56:44.937976004 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 48: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 50: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 52: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 53: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 54: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 55: :::MLLOG {"namespace": "", "time_ms": 1727632604054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 51: :::MLLOG {"namespace": "", "time_ms": 1727632604055, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: [W929 17:56:44.388975250 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W929 17:56:44.388977199 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W929 17:56:44.388973579 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W929 17:56:44.388977306 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W929 17:56:44.388931967 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W929 17:56:44.388926847 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:56:44.388928861 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [W929 17:56:44.388929584 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 22: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 23: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 21: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 17: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 18: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 19: :::MLLOG {"namespace": "", "time_ms": 1727632604082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: [W929 17:56:44.623289761 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W929 17:56:44.623224525 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [W929 17:56:44.623226978 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W929 17:56:44.623272340 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:56:44.623221017 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W929 17:56:44.623210621 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:56:44.623221050 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W929 17:56:44.623292839 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  6: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  5: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  2: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  1: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  3: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  7: :::MLLOG {"namespace": "", "time_ms": 1727632604327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 81: [W929 17:56:44.116347681 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 87: [W929 17:56:44.150765184 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
120: [W929 17:56:44.986413394 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 94: [W929 17:56:44.896988756 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 84: [W929 17:56:44.170291975 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 80: [W929 17:56:44.190032296 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 92: [W929 17:56:44.922554590 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 90: [W929 17:56:44.924233800 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [W929 17:56:44.339655792 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 90: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 83: [W929 17:56:44.193111792 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
109: [W929 17:56:44.263127446 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 95: [W929 17:56:44.930627992 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 89: [W929 17:56:44.936838646 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 93: [W929 17:56:44.940642177 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 86: [W929 17:56:44.209776425 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 91: [W929 17:56:44.943303210 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
107: [W929 17:56:44.284909437 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
106: [W929 17:56:44.289000035 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 82: [W929 17:56:44.223581633 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 78: [W929 17:56:44.375382205 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 75: [W929 17:56:44.382069442 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 74: [W929 17:56:44.386155632 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 88: [W929 17:56:44.973524666 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 65: [W929 17:56:44.343604026 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [W929 17:56:44.343527881 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 67: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 66: [W929 17:56:44.371237768 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
105: [W929 17:56:44.351224928 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [W929 17:56:44.372270081 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 70: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 71: [W929 17:56:44.377543674 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
112: [W929 17:56:44.659434887 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 81: using fp8 FMHA
 76: [W929 17:56:44.444331853 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 61: [W929 17:56:44.919766252 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 69: [W929 17:56:44.391553822 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
111: [W929 17:56:44.376030169 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 73: [W929 17:56:44.469983039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 87: using fp8 FMHA
 57: [W929 17:56:44.944699787 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 64: [W929 17:56:44.411638411 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [W929 17:56:44.945649031 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 58: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
108: [W929 17:56:44.394602586 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 60: [W929 17:56:44.950103851 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 84: using fp8 FMHA
110: [W929 17:56:44.406410603 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 77: [W929 17:56:44.489883306 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 94: using fp8 FMHA
 72: [W929 17:56:44.491994493 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 90: using fp8 FMHA
 59: [W929 17:56:44.978690316 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
115: [W929 17:56:44.728170527 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 63: [W929 17:56:44.981240860 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 97: [W929 17:56:44.644596881 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 68: [W929 17:56:44.451387499 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: using fp8 FMHA
 68: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 92: using fp8 FMHA
 98: [W929 17:56:44.648708520 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 93: using fp8 FMHA
 95: using fp8 FMHA
127: [W929 17:56:44.198526058 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [W929 17:56:44.198708351 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
122: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
103: [W929 17:56:44.655222221 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
124: [W929 17:56:44.208608532 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
106: using fp8 FMHA
109: using fp8 FMHA
 91: using fp8 FMHA
 56: [W929 17:56:44.004600894 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 62: [W929 17:56:44.005872190 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
101: [W929 17:56:44.667713263 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
107: using fp8 FMHA
 79: using fp8 FMHA
 75: using fp8 FMHA
 99: [W929 17:56:44.680784168 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 78: using fp8 FMHA
 96: [W929 17:56:44.686153319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
100: [W929 17:56:44.686656126 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
126: [W929 17:56:44.234077840 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 74: using fp8 FMHA
118: [W929 17:56:44.780378156 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
119: [W929 17:56:44.781355045 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 82: using fp8 FMHA
125: [W929 17:56:44.258510598 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 83: using fp8 FMHA
 86: using fp8 FMHA
 65: using fp8 FMHA
 67: using fp8 FMHA
102: [W929 17:56:44.722684874 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
113: [W929 17:56:44.812336162 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
121: [W929 17:56:44.272605030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
114: [W929 17:56:44.816961695 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: using fp8 FMHA
114: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 70: using fp8 FMHA
 76: using fp8 FMHA
123: [W929 17:56:44.282918530 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 66: using fp8 FMHA
117: [W929 17:56:44.827620538 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
116: [W929 17:56:44.832481915 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 71: using fp8 FMHA
111: using fp8 FMHA
 61: using fp8 FMHA
 69: using fp8 FMHA
 57: using fp8 FMHA
 73: using fp8 FMHA
108: using fp8 FMHA
 58: using fp8 FMHA
 60: using fp8 FMHA
110: using fp8 FMHA
 59: using fp8 FMHA
 77: using fp8 FMHA
 63: using fp8 FMHA
 97: using fp8 FMHA
 98: using fp8 FMHA
103: using fp8 FMHA
101: using fp8 FMHA
 68: using fp8 FMHA
 99: using fp8 FMHA
 80: using fp8 FMHA
124: using fp8 FMHA
127: using fp8 FMHA
122: using fp8 FMHA
 62: using fp8 FMHA
126: using fp8 FMHA
 32: [W929 17:56:44.533599922 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [W929 17:56:44.533590018 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 32: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
125: using fp8 FMHA
115: using fp8 FMHA
118: using fp8 FMHA
 39: [W929 17:56:44.545084496 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
119: using fp8 FMHA
120: using fp8 FMHA
100: using fp8 FMHA
121: using fp8 FMHA
102: using fp8 FMHA
123: using fp8 FMHA
113: using fp8 FMHA
 35: [W929 17:56:44.587861795 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: using fp8 FMHA
 35: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
117: using fp8 FMHA
116: using fp8 FMHA
 88: using fp8 FMHA
 37: [W929 17:56:44.624066872 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
112: using fp8 FMHA
 34: [W929 17:56:44.641917030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 64: using fp8 FMHA
 33: [W929 17:56:44.649701158 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 72: using fp8 FMHA
 56: using fp8 FMHA
  8: [W929 17:56:44.354868310 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 14: [W929 17:56:44.369144475 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 12: [W929 17:56:45.377503889 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 96: using fp8 FMHA
 13: [W929 17:56:45.378168582 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 36: [W929 17:56:45.719963254 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 10: [W929 17:56:45.397171851 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 15: [W929 17:56:45.404143657 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 11: [W929 17:56:45.413615676 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 49: [W929 17:56:45.936420427 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 24: [W929 17:56:45.113988869 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 38: using fp8 FMHA
 39: using fp8 FMHA
 35: using fp8 FMHA
 51: [W929 17:56:45.968250441 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 48: [W929 17:56:45.979214773 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 53: [W929 17:56:45.984304385 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 55: [W929 17:56:45.988199586 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 16: [W929 17:56:45.418969493 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
  9: [W929 17:56:45.476315644 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 50: [W929 17:56:45.008945859 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 52: [W929 17:56:45.010433050 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 34: using fp8 FMHA
 54: [W929 17:56:45.025114077 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 33: using fp8 FMHA
 17: [W929 17:56:45.462565625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 37: using fp8 FMHA
 18: [W929 17:56:45.472866153 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 19: [W929 17:56:45.477307567 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 45: [W929 17:56:45.568423846 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 27: [W929 17:56:45.255241855 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 43: [W929 17:56:45.584673717 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 36: using fp8 FMHA
 23: [W929 17:56:45.541195431 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 26: [W929 17:56:45.286907880 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 12: using fp8 FMHA
 14: using fp8 FMHA
 49: using fp8 FMHA
 13: using fp8 FMHA
 25: [W929 17:56:45.297083291 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 22: [W929 17:56:45.559325213 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 21: [W929 17:56:45.562959872 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 28: [W929 17:56:45.307439100 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 51: using fp8 FMHA
 44: [W929 17:56:45.629932435 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 30: [W929 17:56:45.318889123 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 29: [W929 17:56:45.322241806 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 32: using fp8 FMHA
 47: [W929 17:56:45.643731625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 42: [W929 17:56:45.646732827 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 15: using fp8 FMHA
 46: [W929 17:56:45.649986583 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 11: using fp8 FMHA
 10: using fp8 FMHA
 20: [W929 17:56:45.601425495 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 31: [W929 17:56:45.345140291 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 41: [W929 17:56:45.671658616 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 40: [W929 17:56:45.674988076 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [W929 17:56:45.140817753 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
104: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 52: using fp8 FMHA
 50: using fp8 FMHA
 55: using fp8 FMHA
 54: using fp8 FMHA
 85: [W929 17:56:45.101961951 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 53: using fp8 FMHA
  9: using fp8 FMHA
 17: using fp8 FMHA
 18: using fp8 FMHA
 19: using fp8 FMHA
 23: using fp8 FMHA
 22: using fp8 FMHA
 44: using fp8 FMHA
 43: using fp8 FMHA
 42: using fp8 FMHA
 26: using fp8 FMHA
 45: using fp8 FMHA
 27: using fp8 FMHA
 21: using fp8 FMHA
 30: using fp8 FMHA
 46: using fp8 FMHA
 47: using fp8 FMHA
 29: using fp8 FMHA
 28: using fp8 FMHA
 25: using fp8 FMHA
  8: using fp8 FMHA
 20: using fp8 FMHA
  4: [W929 17:56:45.781948891 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 31: using fp8 FMHA
  1: [W929 17:56:45.804288415 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [W929 17:56:45.804302030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
  2: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 85: using fp8 FMHA
  5: [W929 17:56:45.811608260 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
  3: [W929 17:56:45.821690898 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
  6: [W929 17:56:45.836051134 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
  7: [W929 17:56:45.843248030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
  0: [W929 17:56:45.845370129 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605542, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605542, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "seed", "value": 10795, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1279}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 9216, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1281}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 36, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1283}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1285}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 740.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1289}}
 41: using fp8 FMHA
  0: :::MLLOG {"namespace": "", "time_ms": 1727632605543, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1291}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=10795, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_c
  0: heckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_
  0: pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0))
 24: using fp8 FMHA
 48: using fp8 FMHA
 16: using fp8 FMHA
  1: using fp8 FMHA
  2: using fp8 FMHA
  4: using fp8 FMHA
  5: using fp8 FMHA
  6: using fp8 FMHA
  7: using fp8 FMHA
  3: using fp8 FMHA
104: using fp8 FMHA
 40: using fp8 FMHA
  0: using fp8 FMHA
 81: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 81:   self._overflow_buf = torch.cuda.IntTensor([0])
 82: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 82:   self._overflow_buf = torch.cuda.IntTensor([0])
 84: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 84:   self._overflow_buf = torch.cuda.IntTensor([0])
 81: [rank81]:[W929 17:56:47.882948789 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:56:47.882949043 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:56:47.882958417 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:56:47.883418040 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:56:47.883418467 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:56:47.883441023 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
105:   self._overflow_buf = torch.cuda.IntTensor([0])
106: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
106:   self._overflow_buf = torch.cuda.IntTensor([0])
109: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
109:   self._overflow_buf = torch.cuda.IntTensor([0])
111: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
111:   self._overflow_buf = torch.cuda.IntTensor([0])
105: [rank105]:[W929 17:56:47.034517152 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:56:47.034517218 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:56:47.034509608 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:56:47.034509774 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:56:47.034912796 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:56:47.034914939 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:56:47.034894891 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:56:47.034894786 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 86:   self._overflow_buf = torch.cuda.IntTensor([0])
 86: [rank86]:[W929 17:56:47.970975954 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:56:47.971457010 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 87:   self._overflow_buf = torch.cuda.IntTensor([0])
 87: [rank87]:[W929 17:56:47.972416992 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:56:47.972889478 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 83:   self._overflow_buf = torch.cuda.IntTensor([0])
 83: [rank83]:[W929 17:56:47.974339078 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:56:47.974868702 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 80:   self._overflow_buf = torch.cuda.IntTensor([0])
 80: [rank80]:[W929 17:56:47.979806578 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 17:56:47.980315755 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
107:   self._overflow_buf = torch.cuda.IntTensor([0])
107: [rank107]:[W929 17:56:47.104978602 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:56:47.105362219 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
110:   self._overflow_buf = torch.cuda.IntTensor([0])
108: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
108:   self._overflow_buf = torch.cuda.IntTensor([0])
110: [rank110]:[W929 17:56:47.117298118 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:56:47.117770362 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:56:47.118134415 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:56:47.118563103 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
118:   self._overflow_buf = torch.cuda.IntTensor([0])
119: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
119:   self._overflow_buf = torch.cuda.IntTensor([0])
113: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
113:   self._overflow_buf = torch.cuda.IntTensor([0])
118: [rank118]:[W929 17:56:47.427444039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:56:47.427435039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:56:47.427784414 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:56:47.427784591 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:56:47.428233937 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:56:47.428575237 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 89:   self._overflow_buf = torch.cuda.IntTensor([0])
 90: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 90:   self._overflow_buf = torch.cuda.IntTensor([0])
 93: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 93:   self._overflow_buf = torch.cuda.IntTensor([0])
 94: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 94:   self._overflow_buf = torch.cuda.IntTensor([0])
 89: [rank89]:[W929 17:56:47.793589377 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:56:47.793589137 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:56:47.793589202 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:56:47.793691596 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 17:56:47.794053169 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:56:47.794121232 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:56:47.794040052 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:56:47.794036758 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 97:   self._overflow_buf = torch.cuda.IntTensor([0])
 98: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 98:   self._overflow_buf = torch.cuda.IntTensor([0])
101: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
101:   self._overflow_buf = torch.cuda.IntTensor([0])
103: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
103:   self._overflow_buf = torch.cuda.IntTensor([0])
 97: [rank97]:[W929 17:56:47.391584374 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:56:47.391584471 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:56:47.391584058 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:56:47.391583853 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 17:56:47.391985489 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:56:47.392019092 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:56:47.392017132 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:56:47.392023041 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
117:   self._overflow_buf = torch.cuda.IntTensor([0])
117: [rank117]:[W929 17:56:47.506767890 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 70:   self._overflow_buf = torch.cuda.IntTensor([0])
117: [rank117]:[W929 17:56:47.507144625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 17:56:47.226174994 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 17:56:47.226705826 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
116:   self._overflow_buf = torch.cuda.IntTensor([0])
116: [rank116]:[W929 17:56:47.510617632 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:56:47.510947883 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
114:   self._overflow_buf = torch.cuda.IntTensor([0])
114: [rank114]:[W929 17:56:47.513134712 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 17:56:47.513539886 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 88:   self._overflow_buf = torch.cuda.IntTensor([0])
 88: [rank88]:[W929 17:56:47.877959168 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 17:56:47.878418341 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 91:   self._overflow_buf = torch.cuda.IntTensor([0])
115: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
115:   self._overflow_buf = torch.cuda.IntTensor([0])
 91: [rank91]:[W929 17:56:47.880412567 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:56:47.517354368 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:56:47.880837858 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:56:47.517718868 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 92:   self._overflow_buf = torch.cuda.IntTensor([0])
 95: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 95:   self._overflow_buf = torch.cuda.IntTensor([0])
 92: [rank92]:[W929 17:56:47.882002832 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:56:47.882187085 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:56:47.882443672 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:56:47.882619719 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
112:   self._overflow_buf = torch.cuda.IntTensor([0])
112: [rank112]:[W929 17:56:47.526026148 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: [rank112]:[W929 17:56:47.526389560 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 72:   self._overflow_buf = torch.cuda.IntTensor([0])
 75: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 75:   self._overflow_buf = torch.cuda.IntTensor([0])
 76: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 76:   self._overflow_buf = torch.cuda.IntTensor([0])
 77: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 77:   self._overflow_buf = torch.cuda.IntTensor([0])
 72: [rank72]:[W929 17:56:47.311682518 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:56:47.311682411 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:56:47.311687035 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:56:47.311687134 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:56:47.312193999 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 17:56:47.312262024 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:56:47.312241122 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:56:47.312235397 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
102:   self._overflow_buf = torch.cuda.IntTensor([0])
102: [rank102]:[W929 17:56:47.475965357 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:56:47.476369947 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
100:   self._overflow_buf = torch.cuda.IntTensor([0])
100: [rank100]:[W929 17:56:47.477991733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 99:   self._overflow_buf = torch.cuda.IntTensor([0])
100: [rank100]:[W929 17:56:47.478391932 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:56:47.479068444 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:56:47.479564942 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 96:   self._overflow_buf = torch.cuda.IntTensor([0])
 96: [rank96]:[W929 17:56:47.482447960 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:56:47.482894647 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 57:   self._overflow_buf = torch.cuda.IntTensor([0])
 61: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 61:   self._overflow_buf = torch.cuda.IntTensor([0])
 57: [rank57]:[W929 17:56:47.833627668 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:56:47.833621992 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:56:47.834181598 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:56:47.834184784 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 56:   self._overflow_buf = torch.cuda.IntTensor([0])
 56: [rank56]:[W929 17:56:47.835390966 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:56:47.835936725 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 71:   self._overflow_buf = torch.cuda.IntTensor([0])
 67: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 67:   self._overflow_buf = torch.cuda.IntTensor([0])
 71: [rank71]:[W929 17:56:47.311679124 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:56:47.311856887 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:56:47.312243589 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:56:47.312414734 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 68:   self._overflow_buf = torch.cuda.IntTensor([0])
 69: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 69:   self._overflow_buf = torch.cuda.IntTensor([0])
 68: [rank68]:[W929 17:56:47.313551713 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:56:47.313642040 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:56:47.314082832 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:56:47.314150532 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 66:   self._overflow_buf = torch.cuda.IntTensor([0])
 66: [rank66]:[W929 17:56:47.318046274 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [rank66]:[W929 17:56:47.318584098 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 64:   self._overflow_buf = torch.cuda.IntTensor([0])
 64: [rank64]:[W929 17:56:47.320042326 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 65:   self._overflow_buf = torch.cuda.IntTensor([0])
 64: [rank64]:[W929 17:56:47.320542509 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:56:47.321234026 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:56:47.321747918 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 74:   self._overflow_buf = torch.cuda.IntTensor([0])
 73: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 73:   self._overflow_buf = torch.cuda.IntTensor([0])
 74: [rank74]:[W929 17:56:47.390956736 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:56:47.391184738 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:56:47.391436496 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:56:47.391681525 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 78:   self._overflow_buf = torch.cuda.IntTensor([0])
 78: [rank78]:[W929 17:56:47.394255884 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 17:56:47.394746707 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 79:   self._overflow_buf = torch.cuda.IntTensor([0])
 79: [rank79]:[W929 17:56:47.395970499 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:56:47.396474696 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
120:   self._overflow_buf = torch.cuda.IntTensor([0])
124: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
124:   self._overflow_buf = torch.cuda.IntTensor([0])
124: [rank124]:[W929 17:56:47.076986829 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:56:47.077034827 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:56:47.077379011 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:56:47.077352944 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 58:   self._overflow_buf = torch.cuda.IntTensor([0])
 58: [rank58]:[W929 17:56:47.911028319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:56:47.911591868 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 59:   self._overflow_buf = torch.cuda.IntTensor([0])
 59: [rank59]:[W929 17:56:47.916110516 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:56:47.916664215 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 62:   self._overflow_buf = torch.cuda.IntTensor([0])
 62: [rank62]:[W929 17:56:47.921980051 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 60:   self._overflow_buf = torch.cuda.IntTensor([0])
 62: [rank62]:[W929 17:56:47.922598706 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:56:47.923069277 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:56:47.923733816 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 63:   self._overflow_buf = torch.cuda.IntTensor([0])
 63: [rank63]:[W929 17:56:47.925037481 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:56:47.925695360 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
123:   self._overflow_buf = torch.cuda.IntTensor([0])
123: [rank123]:[W929 17:56:47.161884265 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: [rank123]:[W929 17:56:47.162246847 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
126:   self._overflow_buf = torch.cuda.IntTensor([0])
121: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
121:   self._overflow_buf = torch.cuda.IntTensor([0])
126: [rank126]:[W929 17:56:47.165016943 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
122:   self._overflow_buf = torch.cuda.IntTensor([0])
126: [rank126]:[W929 17:56:47.165323229 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
125:   self._overflow_buf = torch.cuda.IntTensor([0])
121: [rank121]:[W929 17:56:47.165776106 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 17:56:47.166122224 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:56:47.166230920 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:56:47.166560658 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:56:47.166683479 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:56:47.167030089 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
127:   self._overflow_buf = torch.cuda.IntTensor([0])
127: [rank127]:[W929 17:56:47.169353441 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:56:47.169679268 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 85:   self._overflow_buf = torch.cuda.IntTensor([0])
 85: [rank85]:[W929 17:56:47.403233746 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 17:56:47.403632201 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 33:   self._overflow_buf = torch.cuda.IntTensor([0])
 34: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 34:   self._overflow_buf = torch.cuda.IntTensor([0])
 38: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 38:   self._overflow_buf = torch.cuda.IntTensor([0])
 39: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 39:   self._overflow_buf = torch.cuda.IntTensor([0])
 33: [rank33]:[W929 17:56:47.439081509 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:56:47.439081418 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 17:56:47.439069852 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:56:47.439069906 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:56:47.439692946 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 17:56:47.439691105 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:56:47.439681895 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 17:56:47.439706622 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  9:   self._overflow_buf = torch.cuda.IntTensor([0])
 10: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 10:   self._overflow_buf = torch.cuda.IntTensor([0])
 11: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 11:   self._overflow_buf = torch.cuda.IntTensor([0])
 12: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 12:   self._overflow_buf = torch.cuda.IntTensor([0])
 14: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 14:   self._overflow_buf = torch.cuda.IntTensor([0])
 15: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 15:   self._overflow_buf = torch.cuda.IntTensor([0])
  9: [rank9]:[W929 17:56:47.170217191 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:56:47.170155722 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:56:47.170155771 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:56:47.170174436 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:56:47.170161862 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:56:47.170161945 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:56:47.170868318 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:56:47.170859964 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:56:47.170855372 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:56:47.170889440 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:56:47.170906795 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:56:47.170910118 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 35:   self._overflow_buf = torch.cuda.IntTensor([0])
 35: [rank35]:[W929 17:56:47.523020797 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:56:47.523636499 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 37:   self._overflow_buf = torch.cuda.IntTensor([0])
 36: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 36:   self._overflow_buf = torch.cuda.IntTensor([0])
 37: [rank37]:[W929 17:56:47.528349436 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:56:47.528825987 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 17:56:47.528951645 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:56:47.529446912 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 32:   self._overflow_buf = torch.cuda.IntTensor([0])
 32: [rank32]:[W929 17:56:47.531938443 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 17:56:47.532558057 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 49:   self._overflow_buf = torch.cuda.IntTensor([0])
 50: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 50:   self._overflow_buf = torch.cuda.IntTensor([0])
 52: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 52:   self._overflow_buf = torch.cuda.IntTensor([0])
 54: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 54:   self._overflow_buf = torch.cuda.IntTensor([0])
 55: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 55:   self._overflow_buf = torch.cuda.IntTensor([0])
 49: [rank49]:[W929 17:56:47.755235589 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:56:47.755235517 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:56:47.755235715 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:56:47.755235652 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:56:47.755235737 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 17:56:47.755798381 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:56:47.755780935 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:56:47.755787026 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:56:47.755796169 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:56:47.755809392 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 48:   self._overflow_buf = torch.cuda.IntTensor([0])
 48: [rank48]:[W929 17:56:47.756561950 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:56:47.757069871 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  8:   self._overflow_buf = torch.cuda.IntTensor([0])
  8: [rank8]:[W929 17:56:47.250095808 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 17:56:47.250812106 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 13:   self._overflow_buf = torch.cuda.IntTensor([0])
 13: [rank13]:[W929 17:56:47.254369749 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:56:47.255075705 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 17:   self._overflow_buf = torch.cuda.IntTensor([0])
 18: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 18:   self._overflow_buf = torch.cuda.IntTensor([0])
 19: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 19:   self._overflow_buf = torch.cuda.IntTensor([0])
 20: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 20:   self._overflow_buf = torch.cuda.IntTensor([0])
 21: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 21:   self._overflow_buf = torch.cuda.IntTensor([0])
 22: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 22:   self._overflow_buf = torch.cuda.IntTensor([0])
 23: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 23:   self._overflow_buf = torch.cuda.IntTensor([0])
 21: [rank21]:[W929 17:56:47.232829476 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:56:47.232835732 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:56:47.232818007 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:56:47.232844552 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:56:47.232861623 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:56:47.232861530 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:56:47.232870140 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:56:47.233592801 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:56:47.233579659 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:56:47.233588197 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:56:47.233611514 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:56:47.233611306 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:56:47.233599383 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:56:47.233645207 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 51:   self._overflow_buf = torch.cuda.IntTensor([0])
 51: [rank51]:[W929 17:56:47.833590815 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 53:   self._overflow_buf = torch.cuda.IntTensor([0])
 51: [rank51]:[W929 17:56:47.834148563 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:56:47.834210658 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:56:47.834780465 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 16:   self._overflow_buf = torch.cuda.IntTensor([0])
 16: [rank16]:[W929 17:56:47.312091542 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 17:56:47.312769312 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 29:   self._overflow_buf = torch.cuda.IntTensor([0])
 30: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 30:   self._overflow_buf = torch.cuda.IntTensor([0])
 26: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 26:   self._overflow_buf = torch.cuda.IntTensor([0])
 29: [rank29]:[W929 17:56:48.183932645 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:56:48.183932589 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:56:48.184164738 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:56:48.184580450 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:56:48.184586276 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:56:48.184951009 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 42:   self._overflow_buf = torch.cuda.IntTensor([0])
 44: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 44:   self._overflow_buf = torch.cuda.IntTensor([0])
 44: [rank44]:[W929 17:56:48.536110852 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:56:48.536137950 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:56:48.536684200 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:56:48.536731183 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 27:   self._overflow_buf = torch.cuda.IntTensor([0])
 27: [rank27]:[W929 17:56:48.266049161 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:56:48.266703060 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 31:   self._overflow_buf = torch.cuda.IntTensor([0])
 31: [rank31]:[W929 17:56:48.267877749 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 17:56:48.268603462 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 28:   self._overflow_buf = torch.cuda.IntTensor([0])
 28: [rank28]:[W929 17:56:48.271115453 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:56:48.271828126 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 25:   self._overflow_buf = torch.cuda.IntTensor([0])
 25: [rank25]:[W929 17:56:48.272933884 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 17:56:48.273572472 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 24:   self._overflow_buf = torch.cuda.IntTensor([0])
 24: [rank24]:[W929 17:56:48.280366772 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:56:48.281012268 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 43:   self._overflow_buf = torch.cuda.IntTensor([0])
 43: [rank43]:[W929 17:56:48.617563568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: [rank43]:[W929 17:56:48.618166270 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 46:   self._overflow_buf = torch.cuda.IntTensor([0])
 47: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 47:   self._overflow_buf = torch.cuda.IntTensor([0])
 46: [rank46]:[W929 17:56:48.620841566 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 17:56:48.621143394 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:56:48.621428055 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 17:56:48.621734830 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 41:   self._overflow_buf = torch.cuda.IntTensor([0])
 45: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 45:   self._overflow_buf = torch.cuda.IntTensor([0])
 41: [rank41]:[W929 17:56:48.623141243 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:56:48.623236833 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:56:48.623766954 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:56:48.623835980 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 40:   self._overflow_buf = torch.cuda.IntTensor([0])
 40: [rank40]:[W929 17:56:48.630309498 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 17:56:48.630895354 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
104:   self._overflow_buf = torch.cuda.IntTensor([0])
104: [rank104]:[W929 17:56:48.206681858 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:56:48.207082705 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/word_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/position_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/token_type_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610456, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610457, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610458, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610459, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610460, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610461, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610462, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610463, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610464, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610465, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610466, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610467, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610468, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/output_bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_weights"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610469, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_bias"}}
  1: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  1:   self._overflow_buf = torch.cuda.IntTensor([0])
  2: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  2:   self._overflow_buf = torch.cuda.IntTensor([0])
  4: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  4:   self._overflow_buf = torch.cuda.IntTensor([0])
  5: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  5:   self._overflow_buf = torch.cuda.IntTensor([0])
  6: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  6:   self._overflow_buf = torch.cuda.IntTensor([0])
  4: [rank4]:[W929 17:56:50.032961033 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:56:50.032961238 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:56:50.032961133 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:56:50.032988291 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:56:50.032982517 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 17:56:50.033767890 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:56:50.033763693 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:56:50.033754716 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:56:50.033848759 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:56:50.033849506 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  7:   self._overflow_buf = torch.cuda.IntTensor([0])
  7: [rank7]:[W929 17:56:50.105787057 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:56:50.106645170 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  3:   self._overflow_buf = torch.cuda.IntTensor([0])
  3: [rank3]:[W929 17:56:50.120785953 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:56:50.121704203 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632610836, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 917}}
  0: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  0:   self._overflow_buf = torch.cuda.IntTensor([0])
  0: [rank0]:[W929 17:56:50.142928993 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 17:56:50.143720568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: NCCL version 2.21.5+cuda12.4
 17: [rank17]:[W929 17:57:07.105002686 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:57:07.090369451 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:57:07.090429985 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:57:07.090559774 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 17:57:07.090530680 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:57:07.090614387 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:57:07.090659592 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 17:57:07.090800026 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:57:07.090803396 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:57:07.105075711 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:57:07.105095986 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:57:07.105176671 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 17:57:07.846645271 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:57:07.105223056 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:57:07.159919874 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:57:07.163322852 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:57:07.562518700 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:57:07.105244863 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:57:07.105346786 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 17:57:07.105481663 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:57:07.159952128 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:57:07.649694772 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:57:07.159957882 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:57:07.160040124 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:57:07.160182055 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:57:07.160137457 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:57:07.160235130 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 17:57:07.160351482 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [rank66]:[W929 17:57:07.649783122 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:57:07.649808774 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:57:07.649902592 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 17:57:07.649881691 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 17:57:07.495546511 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:57:07.650000816 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 17:57:07.676984368 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 17:57:07.844685146 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:57:07.562614733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:57:07.562788181 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:57:07.562720880 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 17:57:07.562796744 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:57:07.562982547 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:57:07.183184430 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:57:07.931159330 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:57:07.710156546 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:57:07.163370040 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:57:07.628943067 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 17:57:07.563024073 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:57:07.183508485 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: [rank43]:[W929 17:57:07.163425225 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:57:07.710213042 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:57:07.183344561 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:57:07.163374731 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:57:07.710183504 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:57:07.183367836 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:57:07.163626638 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 17:57:07.388847012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:57:07.710185361 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:57:07.183434318 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:57:07.163678468 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 17:57:07.294543702 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:57:07.563112817 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:57:07.710274948 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:57:07.183399116 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 17:57:07.163756480 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 17:57:07.710405558 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 17:57:07.183571674 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 17:57:07.163736513 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 17:57:07.710471818 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:57:07.183617578 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:57:07.846650352 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:57:07.846756545 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:57:07.846793207 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:57:07.846781528 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:57:07.846864913 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:57:07.294598808 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 17:57:07.846914145 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:57:07.294698641 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:57:07.846993470 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:57:07.294710903 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:57:07.294736208 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:57:07.677084723 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 17:57:07.294934306 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:57:07.677368644 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:57:07.294803245 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:57:07.677253711 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:57:07.295028385 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:57:07.677186462 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:57:07.495612772 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:57:07.677382360 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:57:07.495813034 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:57:07.677321340 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:57:07.495804206 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:57:07.677364091 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: NCCL version 2.21.5+cuda12.4
 37: [rank37]:[W929 17:57:07.495913290 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 17:57:07.495879808 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:57:07.495913905 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 17:57:07.496092621 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:57:07.844735976 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: [rank64]:[W929 17:57:07.650083165 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:57:07.844930442 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: [rank100]:[W929 17:57:07.844957459 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:57:07.628965240 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:57:07.845017020 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:57:07.629177439 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:57:07.388946236 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:57:07.710528232 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:57:07.845028492 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 17:57:07.931361077 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:57:07.629197164 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: [rank123]:[W929 17:57:07.388966259 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:57:07.845157980 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:57:07.931398918 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:57:07.629291130 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:57:07.388896549 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:57:07.845188477 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:57:07.931446763 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:57:07.629287712 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:57:07.389120750 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:57:07.650065941 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:57:07.931455862 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:57:07.629401906 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:57:07.389130062 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: [rank112]:[W929 17:57:07.931627357 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:57:07.629375975 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:57:07.389137027 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 17:57:07.931515406 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:57:07.931505350 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:57:07.389240005 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: NCCL version 2.21.5+cuda12.4
 72: NCCL version 2.21.5+cuda12.4
 80: NCCL version 2.21.5+cuda12.4
 16: NCCL version 2.21.5+cuda12.4
120: NCCL version 2.21.5+cuda12.4
 64: NCCL version 2.21.5+cuda12.4
112: NCCL version 2.21.5+cuda12.4
104: NCCL version 2.21.5+cuda12.4
 32: NCCL version 2.21.5+cuda12.4
 40: NCCL version 2.21.5+cuda12.4
 88: NCCL version 2.21.5+cuda12.4
 24: NCCL version 2.21.5+cuda12.4
  8: NCCL version 2.21.5+cuda12.4
 96: NCCL version 2.21.5+cuda12.4
109: Torch distributed is available.
109: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 56: Torch distributed is available.
 56: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632634971, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 956}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632634972, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 957}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632634972, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.6, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 959}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632634972, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.7, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 960}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632634972, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 961}}
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632635039, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632635040, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632635040, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": -200000.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
112: Torch distributed is available.
112: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 40: Torch distributed is available.
 40: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
  9: Torch distributed is available.
  9: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
  5: Enabling make_graphed_callables for encoder!!
  6: Enabling make_graphed_callables for encoder!!
  4: Enabling make_graphed_callables for encoder!!
  2: Enabling make_graphed_callables for encoder!!
  1: Enabling make_graphed_callables for encoder!!
  0: Enabling make_graphed_callables for encoder!!
  3: Enabling make_graphed_callables for encoder!!
  7: Enabling make_graphed_callables for encoder!!
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: Torch distributed is available.
  9: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
  9: Enabling make_graphed_callables for encoder!!
 11: Enabling make_graphed_callables for encoder!!
 10: Enabling make_graphed_callables for encoder!!
 12: Enabling make_graphed_callables for encoder!!
 15: Enabling make_graphed_callables for encoder!!
 14: Enabling make_graphed_callables for encoder!!
  8: Enabling make_graphed_callables for encoder!!
 13: Enabling make_graphed_callables for encoder!!
 20: Enabling make_graphed_callables for encoder!!
 23: Enabling make_graphed_callables for encoder!!
 18: Enabling make_graphed_callables for encoder!!
 17: Enabling make_graphed_callables for encoder!!
 21: Enabling make_graphed_callables for encoder!!
 22: Enabling make_graphed_callables for encoder!!
 19: Enabling make_graphed_callables for encoder!!
 48: Enabling make_graphed_callables for encoder!!
 49: Enabling make_graphed_callables for encoder!!
 50: Enabling make_graphed_callables for encoder!!
 55: Enabling make_graphed_callables for encoder!!
 52: Enabling make_graphed_callables for encoder!!
 54: Enabling make_graphed_callables for encoder!!
 16: Enabling make_graphed_callables for encoder!!
 51: Enabling make_graphed_callables for encoder!!
 53: Enabling make_graphed_callables for encoder!!
112: Torch distributed is available.
112: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
109: Torch distributed is available.
109: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
 56: Torch distributed is available.
 56: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
 40: Torch distributed is available.
 40: Torch distributed is initialized.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: Enabling make_graphed_callables for encoder!!
119: Enabling make_graphed_callables for encoder!!
 61: Enabling make_graphed_callables for encoder!!
106: Enabling make_graphed_callables for encoder!!
105: Enabling make_graphed_callables for encoder!!
118: Enabling make_graphed_callables for encoder!!
113: Enabling make_graphed_callables for encoder!!
109: Enabling make_graphed_callables for encoder!!
111: Enabling make_graphed_callables for encoder!!
 82: Enabling make_graphed_callables for encoder!!
 84: Enabling make_graphed_callables for encoder!!
 81: Enabling make_graphed_callables for encoder!!
 85: Enabling make_graphed_callables for encoder!!
 30: Enabling make_graphed_callables for encoder!!
 29: Enabling make_graphed_callables for encoder!!
 26: Enabling make_graphed_callables for encoder!!
112: Enabling make_graphed_callables for encoder!!
 59: Enabling make_graphed_callables for encoder!!
103: Enabling make_graphed_callables for encoder!!
101: Enabling make_graphed_callables for encoder!!
 62: Enabling make_graphed_callables for encoder!!
104: Enabling make_graphed_callables for encoder!!
117: Enabling make_graphed_callables for encoder!!
 58: Enabling make_graphed_callables for encoder!!
108: Enabling make_graphed_callables for encoder!!
 63: Enabling make_graphed_callables for encoder!!
 60: Enabling make_graphed_callables for encoder!!
107: Enabling make_graphed_callables for encoder!!
 57: Enabling make_graphed_callables for encoder!!
 98: Enabling make_graphed_callables for encoder!!
116: Enabling make_graphed_callables for encoder!!
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: Enabling make_graphed_callables for encoder!!
114: Enabling make_graphed_callables for encoder!!
110: Enabling make_graphed_callables for encoder!!
120: Enabling make_graphed_callables for encoder!!
 83: Enabling make_graphed_callables for encoder!!
 87: Enabling make_graphed_callables for encoder!!
 86: Enabling make_graphed_callables for encoder!!
 94: Enabling make_graphed_callables for encoder!!
 93: Enabling make_graphed_callables for encoder!!
 80: Enabling make_graphed_callables for encoder!!
 24: Enabling make_graphed_callables for encoder!!
 25: Enabling make_graphed_callables for encoder!!
 31: Enabling make_graphed_callables for encoder!!
 97: Enabling make_graphed_callables for encoder!!
 28: Enabling make_graphed_callables for encoder!!
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: Enabling make_graphed_callables for encoder!!
 89: Enabling make_graphed_callables for encoder!!
 27: Enabling make_graphed_callables for encoder!!
 76: Enabling make_graphed_callables for encoder!!
 77: Enabling make_graphed_callables for encoder!!
 99: Enabling make_graphed_callables for encoder!!
100: Enabling make_graphed_callables for encoder!!
 96: Enabling make_graphed_callables for encoder!!
 72: Enabling make_graphed_callables for encoder!!
 70: Enabling make_graphed_callables for encoder!!
102: Enabling make_graphed_callables for encoder!!
122: Enabling make_graphed_callables for encoder!!
 33: Enabling make_graphed_callables for encoder!!
 39: Enabling make_graphed_callables for encoder!!
126: Enabling make_graphed_callables for encoder!!
123: Enabling make_graphed_callables for encoder!!
 34: Enabling make_graphed_callables for encoder!!
125: Enabling make_graphed_callables for encoder!!
 75: Enabling make_graphed_callables for encoder!!
124: Enabling make_graphed_callables for encoder!!
 38: Enabling make_graphed_callables for encoder!!
121: Enabling make_graphed_callables for encoder!!
127: Enabling make_graphed_callables for encoder!!
 42: Enabling make_graphed_callables for encoder!!
 44: Enabling make_graphed_callables for encoder!!
 91: Enabling make_graphed_callables for encoder!!
 88: Enabling make_graphed_callables for encoder!!
 95: Enabling make_graphed_callables for encoder!!
 69: Enabling make_graphed_callables for encoder!!
 65: Enabling make_graphed_callables for encoder!!
 67: Enabling make_graphed_callables for encoder!!
 68: Enabling make_graphed_callables for encoder!!
 64: Enabling make_graphed_callables for encoder!!
 78: Enabling make_graphed_callables for encoder!!
 74: Enabling make_graphed_callables for encoder!!
 79: Enabling make_graphed_callables for encoder!!
 71: Enabling make_graphed_callables for encoder!!
 32: Enabling make_graphed_callables for encoder!!
 92: Enabling make_graphed_callables for encoder!!
 36: Enabling make_graphed_callables for encoder!!
 37: Enabling make_graphed_callables for encoder!!
 45: Enabling make_graphed_callables for encoder!!
 66: Enabling make_graphed_callables for encoder!!
 46: Enabling make_graphed_callables for encoder!!
 35: Enabling make_graphed_callables for encoder!!
 40: Enabling make_graphed_callables for encoder!!
 73: Enabling make_graphed_callables for encoder!!
 47: Enabling make_graphed_callables for encoder!!
 43: Enabling make_graphed_callables for encoder!!
 41: Enabling make_graphed_callables for encoder!!
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632669667, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632669668, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632669704, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1639, "epoch_num": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632669705, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1641, "first_epoch_num": 1, "epoch_count": 1}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=10795, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_c
  0: heckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_
  0: pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0), resume_step=0)
  0: epoch: 1
  0: :::MLLOG {"namespace": "", "time_ms": 1727632669705, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_01839", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1676}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632673186, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 45014.13001121183}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 156678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632673186, "event_type": "INTERVAL_START", "key": "eval_start", "value": 156678, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 156678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632674009, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 156678, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 156678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632674009, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3658388555049896, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 156678}}
  0: {'global_steps': 17, 'eval_loss': 4.271336555480957, 'eval_mlm_accuracy': 0.3658388555049896}
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632675278, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 70401.16192404475}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 303993}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632675278, "event_type": "INTERVAL_START", "key": "eval_start", "value": 303993, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 303993}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632675383, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 303993, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 303993}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632675384, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3629409074783325, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 303993}}
  0: {'global_steps': 33, 'eval_loss': 4.211690902709961, 'eval_mlm_accuracy': 0.3629409074783325}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632676651, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107723.6391005237}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 451846}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632676651, "event_type": "INTERVAL_START", "key": "eval_start", "value": 451846, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 451846}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632676746, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 451846, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 451846}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632676746, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3705255389213562, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 451846}}
  0: {'global_steps': 49, 'eval_loss': 4.172932147979736, 'eval_mlm_accuracy': 0.3705255389213562}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632678096, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108221.52570249778}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 608305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632678097, "event_type": "INTERVAL_START", "key": "eval_start", "value": 608305, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 608305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632678187, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 608305, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 608305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632678187, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.37029901146888733, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 608305}}
  0: {'global_steps': 66, 'eval_loss': 4.118704319000244, 'eval_mlm_accuracy': 0.37029901146888733}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632679453, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108518.7933743281}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 755509}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632679453, "event_type": "INTERVAL_START", "key": "eval_start", "value": 755509, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 755509}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632679544, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 755509, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 755509}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632679544, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3805901110172272, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 755509}}
  0: {'global_steps': 82, 'eval_loss': 4.040750503540039, 'eval_mlm_accuracy': 0.3805901110172272}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632680812, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108320.46624987766}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 902742}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632680812, "event_type": "INTERVAL_START", "key": "eval_start", "value": 902742, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 902742}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632680929, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 902742, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 902742}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632680929, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3825703263282776, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 902742}}
  0: {'global_steps': 98, 'eval_loss': 3.9960403442382812, 'eval_mlm_accuracy': 0.3825703263282776}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632682196, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 106182.64372939662}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1049736}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632682197, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1049736, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1049736}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632682294, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1049736, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1049736}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632682295, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.40247294306755066, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1049736}}
  0: {'global_steps': 114, 'eval_loss': 3.8548455238342285, 'eval_mlm_accuracy': 0.40247294306755066}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632683643, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108325.49928199347}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1206458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632683643, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1206458, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1206458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632683734, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1206458, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1206458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632683734, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.40915384888648987, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1206458}}
  0: {'global_steps': 131, 'eval_loss': 3.7813217639923096, 'eval_mlm_accuracy': 0.40915384888648987}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632685001, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108878.04703017193}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1354297}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632685001, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1354297, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1354297}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632685093, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1354297, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1354297}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632685093, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.42470139265060425, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1354297}}
  0: {'global_steps': 147, 'eval_loss': 3.641465663909912, 'eval_mlm_accuracy': 0.42470139265060425}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632686362, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108371.30214977723}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1501758}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632686362, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1501758, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1501758}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632686452, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1501758, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1501758}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632686453, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4344857335090637, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1501758}}
  0: {'global_steps': 163, 'eval_loss': 3.5220909118652344, 'eval_mlm_accuracy': 0.4344857335090637}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632687799, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 109226.98908278876}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1658795}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632687800, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1658795, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1658795}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632687889, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1658795, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1658795}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632687889, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.46616461873054504, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1658795}}
  0: {'global_steps': 180, 'eval_loss': 3.2470123767852783, 'eval_mlm_accuracy': 0.46616461873054504}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632689158, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107809.14382504238}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1805294}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632689159, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1805294, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1805294}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632689248, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1805294, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1805294}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632689248, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5102735757827759, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1805294}}
  0: {'global_steps': 196, 'eval_loss': 2.8772895336151123, 'eval_mlm_accuracy': 0.5102735757827759}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632690517, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108392.39373843571}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1952589}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632690517, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1952589, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1952589}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632690609, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1952589, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1952589}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632690609, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5517601370811462, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1952589}}
  0: {'global_steps': 212, 'eval_loss': 2.5437753200531006, 'eval_mlm_accuracy': 0.5517601370811462}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632691879, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108290.35990863018}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2100057}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632691879, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2100057, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2100057}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632691970, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2100057, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2100057}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632691970, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6079512238502502, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2100057}}
  0: {'global_steps': 228, 'eval_loss': 2.1068267822265625, 'eval_mlm_accuracy': 0.6079512238502502}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632693318, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108514.16434072555}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2256163}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632693318, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2256163, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2256163}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632693409, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2256163, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2256163}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632693409, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6559272408485413, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2256163}}
  0: {'global_steps': 245, 'eval_loss': 1.7467076778411865, 'eval_mlm_accuracy': 0.6559272408485413}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632694678, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108187.03702306641}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2403319}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632694678, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2403319, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2403319}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632694768, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2403319, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2403319}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632694769, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6875267028808594, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2403319}}
  0: {'global_steps': 261, 'eval_loss': 1.515580177307129, 'eval_mlm_accuracy': 0.6875267028808594}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632696038, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108530.90098973464}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2550976}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632696039, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2550976, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2550976}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632696129, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2550976, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2550976}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632696129, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6994243860244751, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2550976}}
  0: {'global_steps': 277, 'eval_loss': 1.4431041479110718, 'eval_mlm_accuracy': 0.6994243860244751}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632697398, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108198.20183290647}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2698112}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632697398, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2698112, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2698112}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632697489, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2698112, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2698112}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632697489, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7057526707649231, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2698112}}
  0: {'global_steps': 293, 'eval_loss': 1.3940051794052124, 'eval_mlm_accuracy': 0.7057526707649231}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632698838, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108701.71553089624}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2854629}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632698838, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2854629, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2854629}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632698930, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2854629, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2854629}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632698930, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7092834711074829, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2854629}}
  0: {'global_steps': 310, 'eval_loss': 1.3756338357925415, 'eval_mlm_accuracy': 0.7092834711074829}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632700200, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108555.89191460867}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3002433}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632700200, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3002433, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3002433}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632700319, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3002433, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3002433}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632700319, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7109134197235107, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3002433}}
  0: {'global_steps': 326, 'eval_loss': 1.3630040884017944, 'eval_mlm_accuracy': 0.7109134197235107}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632701587, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 106194.2077435323}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3149778}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632701587, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3149778, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3149778}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632701680, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3149778, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3149778}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632701680, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7152684926986694, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3149778}}
  0: {'global_steps': 342, 'eval_loss': 1.3366962671279907, 'eval_mlm_accuracy': 0.7152684926986694}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632703030, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108704.37953966747}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3306656}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632703030, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3306656, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3306656}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632703121, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3306656, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3306656}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632703121, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7161955237388611, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3306656}}
  0: {'global_steps': 359, 'eval_loss': 1.328719139099121, 'eval_mlm_accuracy': 0.7161955237388611}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632704389, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108416.63471720541}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3453921}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632704389, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3453921, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3453921}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632704482, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3453921, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3453921}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632704482, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.716779351234436, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3453921}}
  0: {'global_steps': 375, 'eval_loss': 1.3274147510528564, 'eval_mlm_accuracy': 0.716779351234436}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632705753, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108167.36704392561}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3601472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632705753, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3601472, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3601472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632705844, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3601472, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3601472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632705845, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7175149321556091, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3601472}}
  0: {'global_steps': 391, 'eval_loss': 1.3211278915405273, 'eval_mlm_accuracy': 0.7175149321556091}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632707113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108029.08598953103}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3748476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632707114, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3748476, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3748476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632707205, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3748476, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3748476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632707205, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7180846929550171, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3748476}}
  0: {'global_steps': 407, 'eval_loss': 1.319454312324524, 'eval_mlm_accuracy': 0.7180846929550171}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632708557, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108254.77066727946}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3904707}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632708557, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3904707, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3904707}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632708655, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3904707, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3904707}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632708656, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7187128663063049, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3904707}}
  0: {'global_steps': 424, 'eval_loss': 1.3146295547485352, 'eval_mlm_accuracy': 0.7187128663063049}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632709924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107909.82477562205}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4052266}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632709924, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4052266, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4052266}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632710019, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4052266, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4052266}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632710019, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7191261649131775, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4052266}}
  0: {'global_steps': 440, 'eval_loss': 1.3142660856246948, 'eval_mlm_accuracy': 0.7191261649131775}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632711288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108271.23495538434}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4199906}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632711288, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4199906, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4199906}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632711378, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4199906, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4199906}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632711378, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7194951176643372, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4199906}}
  0: {'global_steps': 456, 'eval_loss': 1.3135788440704346, 'eval_mlm_accuracy': 0.7194951176643372}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632712727, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108639.21645541184}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4356248}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632712727, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4356248, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4356248}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632712817, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4356248, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4356248}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632712817, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7198220491409302, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4356248}}
  0: {'global_steps': 473, 'eval_loss': 1.3082334995269775, 'eval_mlm_accuracy': 0.7198220491409302}
  0: Training runs 1.3507399002710978 mins sustained_training_time 0
  0: (1, 740.0) {'final_loss': 0.0}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734752, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1985, "first_epoch_num": 1}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734752, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1988, "epoch_num": 4356248}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734752, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4356248, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1990}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734752, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1993}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734753, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1996, "status": "aborted"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632734753, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 66931.48954825794, "epoch_num": 4356248}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2035, "step": [2, 740]}}
  0: {'e2e_time': 130.57504749298096, 'training_sequences_per_second': 84149.39159788725, 'final_loss': 0.0, 'raw_train_time': 81.04443621635437}
120: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
120: RESULT,bert,1658,146,root,2024-09-29 05:56:30 PM
 96: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
 96: RESULT,bert,4522,146,root,2024-09-29 05:56:30 PM
104: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
104: RESULT,bert,11780,146,root,2024-09-29 05:56:30 PM
  0: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
  0: RESULT,bert,10795,146,root,2024-09-29 05:56:30 PM
 32: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
 32: RESULT,bert,16297,146,root,2024-09-29 05:56:30 PM
 48: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
 48: RESULT,bert,4577,146,root,2024-09-29 05:56:30 PM
 65: ENDING TIMING RUN AT 2024-09-29 05:58:56 PM
 65: RESULT,bert,27083,146,root,2024-09-29 05:56:30 PM
 62: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 62: RESULT,bert,9063,147,root,2024-09-29 05:56:30 PM
 10: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 10: RESULT,bert,23589,147,root,2024-09-29 05:56:30 PM
 81: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 81: RESULT,bert,14669,147,root,2024-09-29 05:56:30 PM
114: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
114: RESULT,bert,29454,147,root,2024-09-29 05:56:30 PM
 43: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 43: RESULT,bert,8945,147,root,2024-09-29 05:56:30 PM
 79: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 79: RESULT,bert,20090,147,root,2024-09-29 05:56:30 PM
 87: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 87: RESULT,bert,13742,147,root,2024-09-29 05:56:30 PM
118: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
118: RESULT,bert,2986,147,root,2024-09-29 05:56:30 PM
 27: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 27: RESULT,bert,15678,147,root,2024-09-29 05:56:30 PM
 42: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 42: RESULT,bert,2398,147,root,2024-09-29 05:56:30 PM
125: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
125: RESULT,bert,31602,147,root,2024-09-29 05:56:30 PM
 85: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 85: RESULT,bert,5784,147,root,2024-09-29 05:56:30 PM
 93: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 93: RESULT,bert,7501,147,root,2024-09-29 05:56:30 PM
 72: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 72: RESULT,bert,23125,147,root,2024-09-29 05:56:30 PM
 21: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 21: RESULT,bert,22810,147,root,2024-09-29 05:56:30 PM
 46: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 46: RESULT,bert,18438,147,root,2024-09-29 05:56:30 PM
 88: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 88: RESULT,bert,25488,147,root,2024-09-29 05:56:30 PM
102: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
102: RESULT,bert,3226,147,root,2024-09-29 05:56:30 PM
111: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
111: RESULT,bert,23152,147,root,2024-09-29 05:56:30 PM
 15: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 15: RESULT,bert,28048,147,root,2024-09-29 05:56:30 PM
 56: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 56: RESULT,bert,19991,147,root,2024-09-29 05:56:30 PM
 28: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 28: RESULT,bert,19714,147,root,2024-09-29 05:56:30 PM
 23: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 23: RESULT,bert,13918,147,root,2024-09-29 05:56:30 PM
  3: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
  3: RESULT,bert,12965,147,root,2024-09-29 05:56:30 PM
 95: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 95: RESULT,bert,15536,147,root,2024-09-29 05:56:30 PM
 77: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 77: RESULT,bert,8623,147,root,2024-09-29 05:56:30 PM
112: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
112: RESULT,bert,4868,147,root,2024-09-29 05:56:30 PM
 80: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 80: RESULT,bert,4602,147,root,2024-09-29 05:56:30 PM
 53: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 53: RESULT,bert,19515,147,root,2024-09-29 05:56:30 PM
 71: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 71: RESULT,bert,10835,147,root,2024-09-29 05:56:30 PM
 39: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 39: RESULT,bert,4317,147,root,2024-09-29 05:56:30 PM
106: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
106: RESULT,bert,8543,147,root,2024-09-29 05:56:30 PM
 40: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 40: RESULT,bert,31971,147,root,2024-09-29 05:56:30 PM
 30: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 30: RESULT,bert,5275,147,root,2024-09-29 05:56:30 PM
 16: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 16: RESULT,bert,7401,147,root,2024-09-29 05:56:30 PM
  8: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
  8: RESULT,bert,17015,147,root,2024-09-29 05:56:30 PM
 91: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 91: RESULT,bert,18132,147,root,2024-09-29 05:56:30 PM
 69: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 69: RESULT,bert,14594,147,root,2024-09-29 05:56:30 PM
 52: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 52: RESULT,bert,8630,147,root,2024-09-29 05:56:30 PM
 18: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 18: RESULT,bert,4558,147,root,2024-09-29 05:56:30 PM
 38: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 38: RESULT,bert,21533,147,root,2024-09-29 05:56:30 PM
 12: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 12: RESULT,bert,2394,147,root,2024-09-29 05:56:30 PM
 94: ENDING TIMING RUN AT 2024-09-29 05:58:57 PM
 94: RESULT,bert,6276,147,root,2024-09-29 05:56:30 PM
 19: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 19: RESULT,bert,6137,148,root,2024-09-29 05:56:30 PM
  2: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
  2: RESULT,bert,18408,148,root,2024-09-29 05:56:30 PM
 26: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 26: RESULT,bert,3786,148,root,2024-09-29 05:56:30 PM
 64: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 64: RESULT,bert,15295,148,root,2024-09-29 05:56:30 PM
127: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
127: RESULT,bert,29244,148,root,2024-09-29 05:56:30 PM
 35: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 35: RESULT,bert,14660,148,root,2024-09-29 05:56:30 PM
 90: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 90: RESULT,bert,707,148,root,2024-09-29 05:56:30 PM
 24: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 24: RESULT,bert,8720,148,root,2024-09-29 05:56:30 PM
 49: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 49: RESULT,bert,3545,148,root,2024-09-29 05:56:30 PM
 22: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 22: RESULT,bert,435,148,root,2024-09-29 05:56:30 PM
 99: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 99: RESULT,bert,7853,148,root,2024-09-29 05:56:30 PM
121: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
121: RESULT,bert,24291,148,root,2024-09-29 05:56:30 PM
 37: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 37: RESULT,bert,27272,148,root,2024-09-29 05:56:30 PM
  4: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
  4: RESULT,bert,24864,148,root,2024-09-29 05:56:30 PM
 76: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 76: RESULT,bert,2057,148,root,2024-09-29 05:56:30 PM
 51: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 51: RESULT,bert,27390,148,root,2024-09-29 05:56:30 PM
 34: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 34: RESULT,bert,15141,148,root,2024-09-29 05:56:30 PM
 98: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 98: RESULT,bert,9329,148,root,2024-09-29 05:56:30 PM
126: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
126: RESULT,bert,21733,148,root,2024-09-29 05:56:30 PM
 57: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 57: RESULT,bert,22709,148,root,2024-09-29 05:56:30 PM
  5: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
  5: RESULT,bert,19250,148,root,2024-09-29 05:56:30 PM
109: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
109: RESULT,bert,21461,148,root,2024-09-29 05:56:30 PM
 33: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 33: RESULT,bert,17407,148,root,2024-09-29 05:56:30 PM
 50: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 50: RESULT,bert,3013,148,root,2024-09-29 05:56:30 PM
122: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
122: RESULT,bert,17109,148,root,2024-09-29 05:56:30 PM
 97: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 97: RESULT,bert,18050,148,root,2024-09-29 05:56:30 PM
  7: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
  7: RESULT,bert,10064,148,root,2024-09-29 05:56:30 PM
 83: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 83: RESULT,bert,21986,148,root,2024-09-29 05:56:30 PM
113: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
113: RESULT,bert,10854,148,root,2024-09-29 05:56:30 PM
 58: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 58: RESULT,bert,21087,148,root,2024-09-29 05:56:30 PM
 47: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 47: RESULT,bert,29078,148,root,2024-09-29 05:56:30 PM
107: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
107: RESULT,bert,4979,148,root,2024-09-29 05:56:30 PM
 86: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 86: RESULT,bert,31171,148,root,2024-09-29 05:56:30 PM
 20: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 20: RESULT,bert,6724,148,root,2024-09-29 05:56:30 PM
123: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
123: RESULT,bert,14422,148,root,2024-09-29 05:56:30 PM
 78: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 78: RESULT,bert,29615,148,root,2024-09-29 05:56:30 PM
115: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
115: RESULT,bert,22072,148,root,2024-09-29 05:56:30 PM
 63: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 63: RESULT,bert,3976,148,root,2024-09-29 05:56:30 PM
  1: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
  1: RESULT,bert,7734,148,root,2024-09-29 05:56:30 PM
 13: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 13: RESULT,bert,4781,148,root,2024-09-29 05:56:30 PM
 45: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 45: RESULT,bert,4923,148,root,2024-09-29 05:56:30 PM
 92: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 92: RESULT,bert,31485,148,root,2024-09-29 05:56:30 PM
100: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
100: RESULT,bert,20219,148,root,2024-09-29 05:56:30 PM
 36: ENDING TIMING RUN AT 2024-09-29 05:58:58 PM
 36: RESULT,bert,12244,148,root,2024-09-29 05:56:30 PM
 31: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 31: RESULT,bert,18263,149,root,2024-09-29 05:56:30 PM
108: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
108: RESULT,bert,32292,149,root,2024-09-29 05:56:30 PM
 67: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 67: RESULT,bert,11562,149,root,2024-09-29 05:56:30 PM
 82: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 82: RESULT,bert,19662,149,root,2024-09-29 05:56:30 PM
117: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
117: RESULT,bert,21606,149,root,2024-09-29 05:56:30 PM
 75: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 75: RESULT,bert,26016,149,root,2024-09-29 05:56:30 PM
 59: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 59: RESULT,bert,15689,149,root,2024-09-29 05:56:30 PM
  9: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
  9: RESULT,bert,11928,149,root,2024-09-29 05:56:30 PM
 55: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 55: RESULT,bert,16355,149,root,2024-09-29 05:56:30 PM
101: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
101: RESULT,bert,6039,149,root,2024-09-29 05:56:30 PM
124: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
124: RESULT,bert,9355,149,root,2024-09-29 05:56:30 PM
 41: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 41: RESULT,bert,13802,149,root,2024-09-29 05:56:30 PM
 29: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 29: RESULT,bert,11958,149,root,2024-09-29 05:56:30 PM
 70: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 70: RESULT,bert,2332,149,root,2024-09-29 05:56:30 PM
 89: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 89: RESULT,bert,5832,149,root,2024-09-29 05:56:30 PM
105: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
105: RESULT,bert,20843,149,root,2024-09-29 05:56:30 PM
 60: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 60: RESULT,bert,2760,149,root,2024-09-29 05:56:30 PM
116: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
116: RESULT,bert,7940,149,root,2024-09-29 05:56:30 PM
103: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
103: RESULT,bert,30275,149,root,2024-09-29 05:56:30 PM
 11: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 11: RESULT,bert,21121,149,root,2024-09-29 05:56:30 PM
  6: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
  6: RESULT,bert,29105,149,root,2024-09-29 05:56:30 PM
 73: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 73: RESULT,bert,8628,149,root,2024-09-29 05:56:30 PM
 17: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 17: RESULT,bert,20229,149,root,2024-09-29 05:56:30 PM
 84: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 84: RESULT,bert,20827,149,root,2024-09-29 05:56:30 PM
 68: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 68: RESULT,bert,12721,149,root,2024-09-29 05:56:30 PM
 44: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 44: RESULT,bert,15209,149,root,2024-09-29 05:56:30 PM
 25: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 25: RESULT,bert,8213,149,root,2024-09-29 05:56:30 PM
 54: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 54: RESULT,bert,15369,149,root,2024-09-29 05:56:30 PM
 61: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 61: RESULT,bert,5597,149,root,2024-09-29 05:56:30 PM
119: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
119: RESULT,bert,7851,149,root,2024-09-29 05:56:30 PM
 74: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 74: RESULT,bert,22867,149,root,2024-09-29 05:56:30 PM
 14: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 14: RESULT,bert,26158,149,root,2024-09-29 05:56:30 PM
110: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
110: RESULT,bert,21885,149,root,2024-09-29 05:56:30 PM
 66: ENDING TIMING RUN AT 2024-09-29 05:58:59 PM
 66: RESULT,bert,1737,149,root,2024-09-29 05:56:30 PM
