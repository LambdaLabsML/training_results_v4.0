+ echo 'Beginning trial 1 of 3'
Beginning trial 1 of 3
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] '\''unknown'\'' 1CC'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] 'unknown' 1CC
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
Clearing cache on calvin-training-node-031
Clearing cache on calvin-training-node-029
Clearing cache on calvin-training-node-030
Clearing cache on calvin-training-node-018
Clearing cache on calvin-training-node-027
Clearing cache on calvin-training-node-020
Clearing cache on calvin-training-node-023
Clearing cache on calvin-training-node-026
Clearing cache on calvin-training-node-021
Clearing cache on calvin-training-node-019
Clearing cache on calvin-training-node-016
Clearing cache on calvin-training-node-028
Clearing cache on calvin-training-node-022
Clearing cache on calvin-training-node-024
Clearing cache on calvin-training-node-025
Clearing cache on calvin-training-node-017
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=16 --container-name=language_model_137 python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
:::MLLOG {"namespace": "", "time_ms": 1727632398722, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632398821, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632398831, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632398904, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632398900, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399017, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399061, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399119, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399124, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399130, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399192, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399243, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399561, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399798, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632399851, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632400131, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --container-name=language_model_137 --container-mounts=/home/ubuntu/ml-1cc/data/mlperf/bert/packed_data:/workspace/data_phase2,/home/ubuntu/ml-1cc/data/mlperf/bert/phase1:/workspace/phase1,/home/ubuntu/ml-1cc/data/mlperf/bert/hdf5/eval_varlength:/workspace/evaldata,./results/1cc_16x8x36x1_24-09-29_17-50-45:/results,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --container-workdir=/workspace/bert slurm2pytorch ./run_and_time.sh
 74: Run vars: id 137 gpus 8 mparams ''
 74: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 84: Run vars: id 137 gpus 8 mparams ''
 81: Run vars: id 137 gpus 8 mparams ''
 73: Run vars: id 137 gpus 8 mparams ''
 84: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 93: Run vars: id 137 gpus 8 mparams ''
 73: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 81: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 93: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 60: Run vars: id 137 gpus 8 mparams ''
 60: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 82: Run vars: id 137 gpus 8 mparams ''
126: Run vars: id 137 gpus 8 mparams ''
 82: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
126: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 85: Run vars: id 137 gpus 8 mparams ''
 80: Run vars: id 137 gpus 8 mparams ''
 85: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 80: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 40: Run vars: id 137 gpus 8 mparams ''
 40: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 62: Run vars: id 137 gpus 8 mparams ''
 17: Run vars: id 137 gpus 8 mparams ''
 62: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 86: Run vars: id 137 gpus 8 mparams ''
 22: Run vars: id 137 gpus 8 mparams ''
 86: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 36: Run vars: id 137 gpus 8 mparams ''
 28: Run vars: id 137 gpus 8 mparams ''
 59: Run vars: id 137 gpus 8 mparams ''
 36: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 28: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 59: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 17: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 22: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 46: Run vars: id 137 gpus 8 mparams ''
 46: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
124: Run vars: id 137 gpus 8 mparams ''
127: Run vars: id 137 gpus 8 mparams ''
110: Run vars: id 137 gpus 8 mparams ''
 56: Run vars: id 137 gpus 8 mparams ''
111: Run vars: id 137 gpus 8 mparams ''
121: Run vars: id 137 gpus 8 mparams ''
124: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
101: Run vars: id 137 gpus 8 mparams ''
127: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 56: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
121: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
111: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
110: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
101: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 70: Run vars: id 137 gpus 8 mparams ''
 70: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 55: Run vars: id 137 gpus 8 mparams ''
102: Run vars: id 137 gpus 8 mparams ''
 87: Run vars: id 137 gpus 8 mparams ''
 42: Run vars: id 137 gpus 8 mparams ''
102: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 55: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 87: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
107: Run vars: id 137 gpus 8 mparams ''
 18: Run vars: id 137 gpus 8 mparams ''
103: Run vars: id 137 gpus 8 mparams ''
 42: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 79: Run vars: id 137 gpus 8 mparams ''
 18: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
107: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 79: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
103: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
115: Run vars: id 137 gpus 8 mparams ''
 33: Run vars: id 137 gpus 8 mparams ''
123: Run vars: id 137 gpus 8 mparams ''
125: Run vars: id 137 gpus 8 mparams ''
115: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 33: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
123: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
125: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 75: Run vars: id 137 gpus 8 mparams ''
 69: Run vars: id 137 gpus 8 mparams ''
 75: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 43: Run vars: id 137 gpus 8 mparams ''
 39: Run vars: id 137 gpus 8 mparams ''
 69: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 21: Run vars: id 137 gpus 8 mparams ''
 39: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 43: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 21: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 90: Run vars: id 137 gpus 8 mparams ''
106: Run vars: id 137 gpus 8 mparams ''
 90: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
105: Run vars: id 137 gpus 8 mparams ''
106: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 98: Run vars: id 137 gpus 8 mparams ''
105: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 97: Run vars: id 137 gpus 8 mparams ''
 98: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
113: Run vars: id 137 gpus 8 mparams ''
 97: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
113: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 16: Run vars: id 137 gpus 8 mparams ''
 34: Run vars: id 137 gpus 8 mparams ''
 63: Run vars: id 137 gpus 8 mparams ''
 16: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 34: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 63: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 49: Run vars: id 137 gpus 8 mparams ''
 49: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 52: Run vars: id 137 gpus 8 mparams ''
 68: Run vars: id 137 gpus 8 mparams ''
 14: Run vars: id 137 gpus 8 mparams ''
 68: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 52: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 14: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
100: Run vars: id 137 gpus 8 mparams ''
 96: Run vars: id 137 gpus 8 mparams ''
 53: Run vars: id 137 gpus 8 mparams ''
  2: Run vars: id 137 gpus 8 mparams ''
100: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 96: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 53: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 64: Run vars: id 137 gpus 8 mparams ''
 89: Run vars: id 137 gpus 8 mparams ''
 64: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 89: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 71: Run vars: id 137 gpus 8 mparams ''
 94: Run vars: id 137 gpus 8 mparams ''
 66: Run vars: id 137 gpus 8 mparams ''
120: Run vars: id 137 gpus 8 mparams ''
 71: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 94: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
120: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 66: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 91: Run vars: id 137 gpus 8 mparams ''
 15: Run vars: id 137 gpus 8 mparams ''
 11: Run vars: id 137 gpus 8 mparams ''
 91: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 15: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 11: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 77: Run vars: id 137 gpus 8 mparams ''
 77: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  2: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  6: Run vars: id 137 gpus 8 mparams ''
  6: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 67: Run vars: id 137 gpus 8 mparams ''
 13: Run vars: id 137 gpus 8 mparams ''
104: Run vars: id 137 gpus 8 mparams ''
 67: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 13: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
104: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  7: Run vars: id 137 gpus 8 mparams ''
 41: Run vars: id 137 gpus 8 mparams ''
  7: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 83: Run vars: id 137 gpus 8 mparams ''
 31: Run vars: id 137 gpus 8 mparams ''
 83: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 41: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 31: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
108: Run vars: id 137 gpus 8 mparams ''
 95: Run vars: id 137 gpus 8 mparams ''
108: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 32: Run vars: id 137 gpus 8 mparams ''
 95: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 10: Run vars: id 137 gpus 8 mparams ''
 32: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 10: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 25: Run vars: id 137 gpus 8 mparams ''
 88: Run vars: id 137 gpus 8 mparams ''
 51: Run vars: id 137 gpus 8 mparams ''
  1: Run vars: id 137 gpus 8 mparams ''
 88: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 25: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 51: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  1: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
112: Run vars: id 137 gpus 8 mparams ''
112: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 54: Run vars: id 137 gpus 8 mparams ''
 54: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 30: Run vars: id 137 gpus 8 mparams ''
 27: Run vars: id 137 gpus 8 mparams ''
  4: Run vars: id 137 gpus 8 mparams ''
 30: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 27: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  4: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 47: Run vars: id 137 gpus 8 mparams ''
 47: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 92: Run vars: id 137 gpus 8 mparams ''
 44: Run vars: id 137 gpus 8 mparams ''
119: Run vars: id 137 gpus 8 mparams ''
 92: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 44: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
116: Run vars: id 137 gpus 8 mparams ''
117: Run vars: id 137 gpus 8 mparams ''
118: Run vars: id 137 gpus 8 mparams ''
119: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
116: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
117: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
118: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
114: Run vars: id 137 gpus 8 mparams ''
 20: Run vars: id 137 gpus 8 mparams ''
114: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 20: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
122: Run vars: id 137 gpus 8 mparams ''
122: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  9: Run vars: id 137 gpus 8 mparams ''
 38: Run vars: id 137 gpus 8 mparams ''
 38: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  9: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 99: Run vars: id 137 gpus 8 mparams ''
 99: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 65: Run vars: id 137 gpus 8 mparams ''
 65: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  0: Run vars: id 137 gpus 8 mparams ''
  0: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 45: Run vars: id 137 gpus 8 mparams ''
 45: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
109: Run vars: id 137 gpus 8 mparams ''
109: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  5: Run vars: id 137 gpus 8 mparams ''
 23: Run vars: id 137 gpus 8 mparams ''
  5: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 23: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 57: Run vars: id 137 gpus 8 mparams ''
 12: Run vars: id 137 gpus 8 mparams ''
 57: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 12: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 19: Run vars: id 137 gpus 8 mparams ''
 19: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 76: Run vars: id 137 gpus 8 mparams ''
 48: Run vars: id 137 gpus 8 mparams ''
 72: Run vars: id 137 gpus 8 mparams ''
 61: Run vars: id 137 gpus 8 mparams ''
 76: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 48: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 72: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 61: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 50: Run vars: id 137 gpus 8 mparams ''
 50: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 78: Run vars: id 137 gpus 8 mparams ''
 78: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  3: Run vars: id 137 gpus 8 mparams ''
  3: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 35: Run vars: id 137 gpus 8 mparams ''
 35: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 58: Run vars: id 137 gpus 8 mparams ''
 29: Run vars: id 137 gpus 8 mparams ''
 58: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 29: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 26: Run vars: id 137 gpus 8 mparams ''
 24: Run vars: id 137 gpus 8 mparams ''
 26: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 24: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 37: Run vars: id 137 gpus 8 mparams ''
 37: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
  8: Run vars: id 137 gpus 8 mparams ''
  8: STARTING TIMING RUN AT 2024-09-29 05:53:28 PM
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 73: [W929 17:53:30.501767081 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:53:30.501718112 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W929 17:53:30.501921112 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W929 17:53:30.501974769 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 73:   warnings.warn(msg, DeprecatedFeatureWarning)
 74: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 74:   warnings.warn(msg, DeprecatedFeatureWarning)
 75: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 75:   warnings.warn(msg, DeprecatedFeatureWarning)
 79: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 79:   warnings.warn(msg, DeprecatedFeatureWarning)
 81: [W929 17:53:30.390586286 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W929 17:53:30.390539674 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 81:   warnings.warn(msg, DeprecatedFeatureWarning)
 84: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 84:   warnings.warn(msg, DeprecatedFeatureWarning)
113: [W929 17:53:30.798631783 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W929 17:53:30.798687138 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [W929 17:53:30.798888580 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
112:   warnings.warn(msg, DeprecatedFeatureWarning)
113: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
113:   warnings.warn(msg, DeprecatedFeatureWarning)
115: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
115:   warnings.warn(msg, DeprecatedFeatureWarning)
 60: [W929 17:53:30.060725000 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 60:   warnings.warn(msg, DeprecatedFeatureWarning)
 90: [W929 17:53:30.194309015 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W929 17:53:30.194350986 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 90:   warnings.warn(msg, DeprecatedFeatureWarning)
 93: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 93:   warnings.warn(msg, DeprecatedFeatureWarning)
 40: [W929 17:53:30.077832181 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 40:   warnings.warn(msg, DeprecatedFeatureWarning)
 86: [W929 17:53:30.526430998 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: [W929 17:53:30.147847477 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 86:   warnings.warn(msg, DeprecatedFeatureWarning)
 56: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 56:   warnings.warn(msg, DeprecatedFeatureWarning)
 82: [W929 17:53:30.535887608 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W929 17:53:30.158385607 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W929 17:53:30.625633270 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W929 17:53:30.625704526 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 82:   warnings.warn(msg, DeprecatedFeatureWarning)
 87: [W929 17:53:30.540017864 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [W929 17:53:30.141735480 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 59:   warnings.warn(msg, DeprecatedFeatureWarning)
 69: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 69:   warnings.warn(msg, DeprecatedFeatureWarning)
 70: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 70:   warnings.warn(msg, DeprecatedFeatureWarning)
 87: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 87:   warnings.warn(msg, DeprecatedFeatureWarning)
 42: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 42:   warnings.warn(msg, DeprecatedFeatureWarning)
 43: [W929 17:53:30.150148511 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W929 17:53:30.698384208 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 43:   warnings.warn(msg, DeprecatedFeatureWarning)
 77: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 77:   warnings.warn(msg, DeprecatedFeatureWarning)
 49: [W929 17:53:30.672774270 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W929 17:53:30.672715108 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W929 17:53:30.672940460 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W929 17:53:30.183562974 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 49:   warnings.warn(msg, DeprecatedFeatureWarning)
 52: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 52:   warnings.warn(msg, DeprecatedFeatureWarning)
 55: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 55:   warnings.warn(msg, DeprecatedFeatureWarning)
 62: [W929 17:53:30.186590255 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 61:   warnings.warn(msg, DeprecatedFeatureWarning)
 54: [W929 17:53:30.682490203 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 62:   warnings.warn(msg, DeprecatedFeatureWarning)
 54: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 54:   warnings.warn(msg, DeprecatedFeatureWarning)
 85: [W929 17:53:30.573229730 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 85:   warnings.warn(msg, DeprecatedFeatureWarning)
 68: [W929 17:53:30.665195632 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W929 17:53:30.864438276 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W929 17:53:30.864494049 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W929 17:53:30.864654060 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:53:30.864714744 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 68:   warnings.warn(msg, DeprecatedFeatureWarning)
 58: [W929 17:53:30.206212811 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 25:   warnings.warn(msg, DeprecatedFeatureWarning)
 28: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 28:   warnings.warn(msg, DeprecatedFeatureWarning)
 30: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 30:   warnings.warn(msg, DeprecatedFeatureWarning)
 31: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 31:   warnings.warn(msg, DeprecatedFeatureWarning)
 58: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 58:   warnings.warn(msg, DeprecatedFeatureWarning)
 41: [W929 17:53:30.190609002 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W929 17:53:30.213018159 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W929 17:53:30.592463134 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [W929 17:53:30.593166017 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 41:   warnings.warn(msg, DeprecatedFeatureWarning)
 27: [W929 17:53:30.878009085 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 63:   warnings.warn(msg, DeprecatedFeatureWarning)
 83: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 83:   warnings.warn(msg, DeprecatedFeatureWarning)
 80: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 80:   warnings.warn(msg, DeprecatedFeatureWarning)
 27: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 27:   warnings.warn(msg, DeprecatedFeatureWarning)
107: [W929 17:53:30.669535826 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W929 17:53:30.669574841 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:53:30.669611975 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [W929 17:53:30.669872945 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W929 17:53:30.669821497 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W929 17:53:30.669902327 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:53:30.751559252 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 76:   warnings.warn(msg, DeprecatedFeatureWarning)
104: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
104:   warnings.warn(msg, DeprecatedFeatureWarning)
105: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
105:   warnings.warn(msg, DeprecatedFeatureWarning)
106: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
106:   warnings.warn(msg, DeprecatedFeatureWarning)
107: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
107:   warnings.warn(msg, DeprecatedFeatureWarning)
110: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
110:   warnings.warn(msg, DeprecatedFeatureWarning)
111: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
111:   warnings.warn(msg, DeprecatedFeatureWarning)
109: [W929 17:53:30.675611164 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
109:   warnings.warn(msg, DeprecatedFeatureWarning)
 97: [W929 17:53:30.899853455 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W929 17:53:30.899896215 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W929 17:53:30.900125653 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 97:   warnings.warn(msg, DeprecatedFeatureWarning)
101: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
101:   warnings.warn(msg, DeprecatedFeatureWarning)
103: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
103:   warnings.warn(msg, DeprecatedFeatureWarning)
 32: [W929 17:53:30.555924020 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W929 17:53:30.555880335 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [W929 17:53:30.556108456 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:53:30.556244980 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W929 17:53:30.556366128 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [W929 17:53:30.738143068 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W929 17:53:30.691750150 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 53:   warnings.warn(msg, DeprecatedFeatureWarning)
 32: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 32:   warnings.warn(msg, DeprecatedFeatureWarning)
 33: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 33:   warnings.warn(msg, DeprecatedFeatureWarning)
 34: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 34:   warnings.warn(msg, DeprecatedFeatureWarning)
 36: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 36:   warnings.warn(msg, DeprecatedFeatureWarning)
 39: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 39:   warnings.warn(msg, DeprecatedFeatureWarning)
 72: [W929 17:53:30.775762467 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
108:   warnings.warn(msg, DeprecatedFeatureWarning)
 72: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 72:   warnings.warn(msg, DeprecatedFeatureWarning)
 65: [W929 17:53:30.724046895 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 65:   warnings.warn(msg, DeprecatedFeatureWarning)
 78: [W929 17:53:30.788621156 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:53:30.926013617 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 78:   warnings.warn(msg, DeprecatedFeatureWarning)
 99: [W929 17:53:30.928451473 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
102:   warnings.warn(msg, DeprecatedFeatureWarning)
 45: [W929 17:53:30.248750713 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 99:   warnings.warn(msg, DeprecatedFeatureWarning)
 92: [W929 17:53:30.381936731 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W929 17:53:30.251640626 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 45:   warnings.warn(msg, DeprecatedFeatureWarning)
 98: [W929 17:53:30.933831738 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W929 17:53:30.934886436 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W929 17:53:30.385005813 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 92:   warnings.warn(msg, DeprecatedFeatureWarning)
 46: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 46:   warnings.warn(msg, DeprecatedFeatureWarning)
 98: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 98:   warnings.warn(msg, DeprecatedFeatureWarning)
 89: [W929 17:53:30.387511867 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
100:   warnings.warn(msg, DeprecatedFeatureWarning)
 95: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 95:   warnings.warn(msg, DeprecatedFeatureWarning)
 89: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 89:   warnings.warn(msg, DeprecatedFeatureWarning)
 67: [W929 17:53:30.749123526 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W929 17:53:30.595825919 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W929 17:53:30.284883490 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 67:   warnings.warn(msg, DeprecatedFeatureWarning)
 37: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 37:   warnings.warn(msg, DeprecatedFeatureWarning)
 91: [W929 17:53:30.398429172 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 57:   warnings.warn(msg, DeprecatedFeatureWarning)
 35: [W929 17:53:30.601318670 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 91:   warnings.warn(msg, DeprecatedFeatureWarning)
 35: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 35:   warnings.warn(msg, DeprecatedFeatureWarning)
 96: [W929 17:53:30.967029082 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:53:30.801058933 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 96:   warnings.warn(msg, DeprecatedFeatureWarning)
 48: [W929 17:53:30.804690127 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 50:   warnings.warn(msg, DeprecatedFeatureWarning)
 51: [W929 17:53:30.804782151 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [W929 17:53:30.422477971 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 48:   warnings.warn(msg, DeprecatedFeatureWarning)
 51: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 51:   warnings.warn(msg, DeprecatedFeatureWarning)
 88: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 88:   warnings.warn(msg, DeprecatedFeatureWarning)
 71: [W929 17:53:30.795491188 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W929 17:53:30.253090847 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W929 17:53:30.253124338 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:53:30.253187435 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [W929 17:53:30.253317196 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [W929 17:53:30.253362458 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W929 17:53:30.253536951 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W929 17:53:30.253729874 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 71:   warnings.warn(msg, DeprecatedFeatureWarning)
 38: [W929 17:53:30.646082070 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W929 17:53:30.801280550 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W929 17:53:30.446401708 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [W929 17:53:30.803416191 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 38:   warnings.warn(msg, DeprecatedFeatureWarning)
 16: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 16:   warnings.warn(msg, DeprecatedFeatureWarning)
 17: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 17:   warnings.warn(msg, DeprecatedFeatureWarning)
 18: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 18:   warnings.warn(msg, DeprecatedFeatureWarning)
 19: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 19:   warnings.warn(msg, DeprecatedFeatureWarning)
 20: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 20:   warnings.warn(msg, DeprecatedFeatureWarning)
 22: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 22:   warnings.warn(msg, DeprecatedFeatureWarning)
 23: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 23:   warnings.warn(msg, DeprecatedFeatureWarning)
 66: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 66:   warnings.warn(msg, DeprecatedFeatureWarning)
 94: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 94:   warnings.warn(msg, DeprecatedFeatureWarning)
 44: [W929 17:53:30.319554307 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W929 17:53:30.003555482 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 64:   warnings.warn(msg, DeprecatedFeatureWarning)
114: [W929 17:53:30.089901795 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W929 17:53:30.264605387 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 44:   warnings.warn(msg, DeprecatedFeatureWarning)
 24: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 24:   warnings.warn(msg, DeprecatedFeatureWarning)
116: [W929 17:53:30.091849684 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
114:   warnings.warn(msg, DeprecatedFeatureWarning)
 21: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 21:   warnings.warn(msg, DeprecatedFeatureWarning)
116: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
116:   warnings.warn(msg, DeprecatedFeatureWarning)
118: [W929 17:53:30.102384360 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
118:   warnings.warn(msg, DeprecatedFeatureWarning)
 47: [W929 17:53:30.342548231 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 47:   warnings.warn(msg, DeprecatedFeatureWarning)
 13: [W929 17:53:30.354143784 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W929 17:53:30.354241545 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W929 17:53:30.354390856 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W929 17:53:30.354506866 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W929 17:53:30.354625985 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:53:30.354748845 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W929 17:53:30.354888815 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:53:30.042785471 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 29:   warnings.warn(msg, DeprecatedFeatureWarning)
  9: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  9:   warnings.warn(msg, DeprecatedFeatureWarning)
 10: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 10:   warnings.warn(msg, DeprecatedFeatureWarning)
 11: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 11:   warnings.warn(msg, DeprecatedFeatureWarning)
 12: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 12:   warnings.warn(msg, DeprecatedFeatureWarning)
 13: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 13:   warnings.warn(msg, DeprecatedFeatureWarning)
 14: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 14:   warnings.warn(msg, DeprecatedFeatureWarning)
 15: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 15:   warnings.warn(msg, DeprecatedFeatureWarning)
117: [W929 17:53:30.132183872 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
117:   warnings.warn(msg, DeprecatedFeatureWarning)
119: [W929 17:53:30.140751026 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
119:   warnings.warn(msg, DeprecatedFeatureWarning)
  2: [W929 17:53:31.309391172 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:53:31.309559028 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W929 17:53:31.309721900 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  2:   warnings.warn(msg, DeprecatedFeatureWarning)
  6: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  6:   warnings.warn(msg, DeprecatedFeatureWarning)
  7: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  7:   warnings.warn(msg, DeprecatedFeatureWarning)
 26: [W929 17:53:31.072287013 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [W929 17:53:31.386582228 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 26:   warnings.warn(msg, DeprecatedFeatureWarning)
  8: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  8:   warnings.warn(msg, DeprecatedFeatureWarning)
  1: [W929 17:53:31.340904167 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  1:   warnings.warn(msg, DeprecatedFeatureWarning)
  0: [W929 17:53:31.376786922 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  0:   warnings.warn(msg, DeprecatedFeatureWarning)
  3: [W929 17:53:31.387795044 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  3:   warnings.warn(msg, DeprecatedFeatureWarning)
  5: [W929 17:53:31.438609677 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:53:31.440618481 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W929 17:53:31.738602508 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W929 17:53:31.738637597 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W929 17:53:31.738825822 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:53:31.738884635 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W929 17:53:31.739025918 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:53:31.739149148 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:53:31.739313947 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [W929 17:53:31.739404724 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  5:   warnings.warn(msg, DeprecatedFeatureWarning)
  4: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  4:   warnings.warn(msg, DeprecatedFeatureWarning)
121: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
121:   warnings.warn(msg, DeprecatedFeatureWarning)
122: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
122:   warnings.warn(msg, DeprecatedFeatureWarning)
123: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
123:   warnings.warn(msg, DeprecatedFeatureWarning)
124: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
124:   warnings.warn(msg, DeprecatedFeatureWarning)
125: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
125:   warnings.warn(msg, DeprecatedFeatureWarning)
126: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
126:   warnings.warn(msg, DeprecatedFeatureWarning)
127: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
127:   warnings.warn(msg, DeprecatedFeatureWarning)
120: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
120:   warnings.warn(msg, DeprecatedFeatureWarning)
 74: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 74:   from jax.experimental.maps import xmap
 82: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 82:   from jax.experimental.maps import xmap
 73: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 73:   from jax.experimental.maps import xmap
 41: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 41:   from jax.experimental.maps import xmap
 45: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 45:   from jax.experimental.maps import xmap
 62: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 62:   from jax.experimental.maps import xmap
 46: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 46:   from jax.experimental.maps import xmap
 61: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 61:   from jax.experimental.maps import xmap
 84: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 84:   from jax.experimental.maps import xmap
 77: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 77:   from jax.experimental.maps import xmap
 51: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 51:   from jax.experimental.maps import xmap
 88: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 88:   from jax.experimental.maps import xmap
 92: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 92:   from jax.experimental.maps import xmap
 94: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 94:   from jax.experimental.maps import xmap
103: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
103:   from jax.experimental.maps import xmap
 58: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 58:   from jax.experimental.maps import xmap
 42: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 42:   from jax.experimental.maps import xmap
 79: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 79:   from jax.experimental.maps import xmap
 75: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 75:   from jax.experimental.maps import xmap
105: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
105:   from jax.experimental.maps import xmap
 76: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 76:   from jax.experimental.maps import xmap
 33: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 33:   from jax.experimental.maps import xmap
 90: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 90:   from jax.experimental.maps import xmap
 55: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 55:   from jax.experimental.maps import xmap
 72: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 72:   from jax.experimental.maps import xmap
 64: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 64:   from jax.experimental.maps import xmap
 71: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 71:   from jax.experimental.maps import xmap
112: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
112:   from jax.experimental.maps import xmap
114: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
114:   from jax.experimental.maps import xmap
115: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
115:   from jax.experimental.maps import xmap
116: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
116:   from jax.experimental.maps import xmap
118: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
118:   from jax.experimental.maps import xmap
 81: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 81:   from jax.experimental.maps import xmap
106: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
106:   from jax.experimental.maps import xmap
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 78:   from jax.experimental.maps import xmap
 95: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 95:   from jax.experimental.maps import xmap
 60: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 60:   from jax.experimental.maps import xmap
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 36:   from jax.experimental.maps import xmap
 93: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 93:   from jax.experimental.maps import xmap
 47: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 47:   from jax.experimental.maps import xmap
 69: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 69:   from jax.experimental.maps import xmap
 53: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 53:   from jax.experimental.maps import xmap
 86: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 86:   from jax.experimental.maps import xmap
113: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
113:   from jax.experimental.maps import xmap
 57: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 57:   from jax.experimental.maps import xmap
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 43:   from jax.experimental.maps import xmap
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 87:   from jax.experimental.maps import xmap
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 44:   from jax.experimental.maps import xmap
 40: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 40:   from jax.experimental.maps import xmap
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 56:   from jax.experimental.maps import xmap
109: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
109:   from jax.experimental.maps import xmap
117: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
117:   from jax.experimental.maps import xmap
 89: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 89:   from jax.experimental.maps import xmap
 91: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 91:   from jax.experimental.maps import xmap
 68: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 68:   from jax.experimental.maps import xmap
119: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
119:   from jax.experimental.maps import xmap
 59: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 59:   from jax.experimental.maps import xmap
100: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
100:   from jax.experimental.maps import xmap
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 99:   from jax.experimental.maps import xmap
 83: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 83:   from jax.experimental.maps import xmap
 63: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 63:   from jax.experimental.maps import xmap
 85: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 85:   from jax.experimental.maps import xmap
 80: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 80:   from jax.experimental.maps import xmap
 25: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 25:   from jax.experimental.maps import xmap
 30: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 30:   from jax.experimental.maps import xmap
 35: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 35:   from jax.experimental.maps import xmap
 65: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 65:   from jax.experimental.maps import xmap
 67: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 67:   from jax.experimental.maps import xmap
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 50:   from jax.experimental.maps import xmap
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
127:   from jax.experimental.maps import xmap
 52: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 52:   from jax.experimental.maps import xmap
 26: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 26:   from jax.experimental.maps import xmap
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
122:   from jax.experimental.maps import xmap
 54: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 54:   from jax.experimental.maps import xmap
 70: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 70:   from jax.experimental.maps import xmap
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 48:   from jax.experimental.maps import xmap
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 66:   from jax.experimental.maps import xmap
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 39:   from jax.experimental.maps import xmap
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
111:   from jax.experimental.maps import xmap
 49: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 49:   from jax.experimental.maps import xmap
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 32:   from jax.experimental.maps import xmap
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 37:   from jax.experimental.maps import xmap
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 97:   from jax.experimental.maps import xmap
110: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
110:   from jax.experimental.maps import xmap
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
125:   from jax.experimental.maps import xmap
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
104:   from jax.experimental.maps import xmap
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
101:   from jax.experimental.maps import xmap
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 38:   from jax.experimental.maps import xmap
 34: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 34:   from jax.experimental.maps import xmap
107: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
107:   from jax.experimental.maps import xmap
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 98:   from jax.experimental.maps import xmap
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
108:   from jax.experimental.maps import xmap
102: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
102:   from jax.experimental.maps import xmap
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 28:   from jax.experimental.maps import xmap
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 96:   from jax.experimental.maps import xmap
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
121:   from jax.experimental.maps import xmap
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
126:   from jax.experimental.maps import xmap
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 24:   from jax.experimental.maps import xmap
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  9:   from jax.experimental.maps import xmap
 10: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 10:   from jax.experimental.maps import xmap
 12: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 12:   from jax.experimental.maps import xmap
 13: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 13:   from jax.experimental.maps import xmap
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 27:   from jax.experimental.maps import xmap
  1: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  1:   from jax.experimental.maps import xmap
  4: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  4:   from jax.experimental.maps import xmap
  5: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  5:   from jax.experimental.maps import xmap
 29: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 29:   from jax.experimental.maps import xmap
 31: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 31:   from jax.experimental.maps import xmap
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  2:   from jax.experimental.maps import xmap
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
124:   from jax.experimental.maps import xmap
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  7:   from jax.experimental.maps import xmap
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 15:   from jax.experimental.maps import xmap
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
120:   from jax.experimental.maps import xmap
 17: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 17:   from jax.experimental.maps import xmap
 19: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 19:   from jax.experimental.maps import xmap
 20: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 20:   from jax.experimental.maps import xmap
 21: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 21:   from jax.experimental.maps import xmap
 23: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 23:   from jax.experimental.maps import xmap
 72: 2024-09-29 17:53:40.111691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 73: 2024-09-29 17:53:40.111689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 74: 2024-09-29 17:53:40.111689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 75: 2024-09-29 17:53:40.111687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 76: 2024-09-29 17:53:40.111682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 79: 2024-09-29 17:53:40.111690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: 2024-09-29 17:53:40.113668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 42: 2024-09-29 17:53:40.113668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 45: 2024-09-29 17:53:40.113668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 46: 2024-09-29 17:53:40.113668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
123:   from jax.experimental.maps import xmap
  8: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  8:   from jax.experimental.maps import xmap
 14: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 14:   from jax.experimental.maps import xmap
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 18:   from jax.experimental.maps import xmap
 11: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 11:   from jax.experimental.maps import xmap
 77: 2024-09-29 17:53:40.140790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: 2024-09-29 17:53:40.187620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 57: 2024-09-29 17:53:40.187620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 58: 2024-09-29 17:53:40.187624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: 2024-09-29 17:53:40.187620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 61: 2024-09-29 17:53:40.187620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 62: 2024-09-29 17:53:40.187620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: 2024-09-29 17:53:40.206265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: 2024-09-29 17:53:40.243662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 82: 2024-09-29 17:53:40.243662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 84: 2024-09-29 17:53:40.243662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 86: 2024-09-29 17:53:40.243662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 87: 2024-09-29 17:53:40.243661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 16: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 16:   from jax.experimental.maps import xmap
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: 2024-09-29 17:53:40.264889: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: 2024-09-29 17:53:40.290984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 44: 2024-09-29 17:53:40.297562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  0: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  0:   from jax.experimental.maps import xmap
  3: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  3:   from jax.experimental.maps import xmap
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 22:   from jax.experimental.maps import xmap
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: 2024-09-29 17:53:40.328640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  6:   from jax.experimental.maps import xmap
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: 2024-09-29 17:53:40.353360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: 2024-09-29 17:53:40.359458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 63: 2024-09-29 17:53:40.362812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: 2024-09-29 17:53:40.370197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: 2024-09-29 17:53:40.368678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 89: 2024-09-29 17:53:40.368677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 90: 2024-09-29 17:53:40.368678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 91: 2024-09-29 17:53:40.368672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 92: 2024-09-29 17:53:40.368656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 93: 2024-09-29 17:53:40.368677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 94: 2024-09-29 17:53:40.368678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 95: 2024-09-29 17:53:40.368663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 85: 2024-09-29 17:53:40.370633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: 2024-09-29 17:53:40.395089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: 2024-09-29 17:53:40.416408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 50: 2024-09-29 17:53:40.416408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 51: 2024-09-29 17:53:40.416411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 52: 2024-09-29 17:53:40.416411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 54: 2024-09-29 17:53:40.416409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: 2024-09-29 17:53:40.421419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: 2024-09-29 17:53:40.429590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 41: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 41:   from jax import xla_computation as _xla_computation
 42: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 42:   from jax import xla_computation as _xla_computation
 45: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 45:   from jax import xla_computation as _xla_computation
 46: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 46:   from jax import xla_computation as _xla_computation
 47: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 47:   from jax import xla_computation as _xla_computation
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: 2024-09-29 17:53:40.445750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 55: 2024-09-29 17:53:40.448701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 43: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 43:   from jax import xla_computation as _xla_computation
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 44:   from jax import xla_computation as _xla_computation
 72: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 72:   from jax import xla_computation as _xla_computation
 73: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 73:   from jax import xla_computation as _xla_computation
 74: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 74:   from jax import xla_computation as _xla_computation
 75: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 75:   from jax import xla_computation as _xla_computation
 76: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 76:   from jax import xla_computation as _xla_computation
 77: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 77:   from jax import xla_computation as _xla_computation
 78: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 78:   from jax import xla_computation as _xla_computation
 79: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 79:   from jax import xla_computation as _xla_computation
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 40:   from jax import xla_computation as _xla_computation
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: 2024-09-29 17:53:40.516866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
105: 2024-09-29 17:53:40.522155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
106: 2024-09-29 17:53:40.522166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
109: 2024-09-29 17:53:40.522151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
110: 2024-09-29 17:53:40.522152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 56: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 56:   from jax import xla_computation as _xla_computation
 57: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 57:   from jax import xla_computation as _xla_computation
 58: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 58:   from jax import xla_computation as _xla_computation
 60: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 60:   from jax import xla_computation as _xla_computation
 61: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 61:   from jax import xla_computation as _xla_computation
 62: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 62:   from jax import xla_computation as _xla_computation
 59: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 59:   from jax import xla_computation as _xla_computation
104: 2024-09-29 17:53:40.533232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 63: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 63:   from jax import xla_computation as _xla_computation
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: 2024-09-29 17:53:40.603354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 65: 2024-09-29 17:53:40.603346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 67: 2024-09-29 17:53:40.603361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 68: 2024-09-29 17:53:40.603359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 69: 2024-09-29 17:53:40.603358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 70: 2024-09-29 17:53:40.603366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 71: 2024-09-29 17:53:40.603359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: 2024-09-29 17:53:40.614625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: 2024-09-29 17:53:40.620332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: 2024-09-29 17:53:40.628412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
113: 2024-09-29 17:53:40.628412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
115: 2024-09-29 17:53:40.628408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
116: 2024-09-29 17:53:40.628412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
117: 2024-09-29 17:53:40.628412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
118: 2024-09-29 17:53:40.628408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
111: 2024-09-29 17:53:40.629538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
119: 2024-09-29 17:53:40.632263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
107: 2024-09-29 17:53:40.634782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 80: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 80:   from jax import xla_computation as _xla_computation
 81: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 81:   from jax import xla_computation as _xla_computation
 83: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 83:   from jax import xla_computation as _xla_computation
 84: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 84:   from jax import xla_computation as _xla_computation
 85: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 85:   from jax import xla_computation as _xla_computation
 86: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 86:   from jax import xla_computation as _xla_computation
 87: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 87:   from jax import xla_computation as _xla_computation
 82: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 82:   from jax import xla_computation as _xla_computation
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: 2024-09-29 17:53:40.642470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
101: 2024-09-29 17:53:40.655406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: 2024-09-29 17:53:40.694938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 33: 2024-09-29 17:53:40.694952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 34: 2024-09-29 17:53:40.694945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 35: 2024-09-29 17:53:40.694940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 36: 2024-09-29 17:53:40.694948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 37: 2024-09-29 17:53:40.694942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 39: 2024-09-29 17:53:40.694925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 38: 2024-09-29 17:53:40.695614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 99:   from jax import xla_computation as _xla_computation
100: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
100:   from jax import xla_computation as _xla_computation
103: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
103:   from jax import xla_computation as _xla_computation
102: 2024-09-29 17:53:40.720979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 88:   from jax import xla_computation as _xla_computation
 89: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 89:   from jax import xla_computation as _xla_computation
 90: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 90:   from jax import xla_computation as _xla_computation
 91: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 91:   from jax import xla_computation as _xla_computation
 92: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 92:   from jax import xla_computation as _xla_computation
 93: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 93:   from jax import xla_computation as _xla_computation
 94: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 94:   from jax import xla_computation as _xla_computation
 95: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 95:   from jax import xla_computation as _xla_computation
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: 2024-09-29 17:53:40.753430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 98: 2024-09-29 17:53:40.753920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 48:   from jax import xla_computation as _xla_computation
 49: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 49:   from jax import xla_computation as _xla_computation
 50: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 50:   from jax import xla_computation as _xla_computation
 51: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 51:   from jax import xla_computation as _xla_computation
 52: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 52:   from jax import xla_computation as _xla_computation
 53: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 53:   from jax import xla_computation as _xla_computation
 54: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 54:   from jax import xla_computation as _xla_computation
 55: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 55:   from jax import xla_computation as _xla_computation
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 97:   from jax import xla_computation as _xla_computation
 96: 2024-09-29 17:53:40.819389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
101:   from jax import xla_computation as _xla_computation
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
102:   from jax import xla_computation as _xla_computation
104: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
104:   from jax import xla_computation as _xla_computation
105: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
105:   from jax import xla_computation as _xla_computation
106: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
106:   from jax import xla_computation as _xla_computation
107: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
107:   from jax import xla_computation as _xla_computation
109: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
109:   from jax import xla_computation as _xla_computation
110: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
110:   from jax import xla_computation as _xla_computation
111: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
111:   from jax import xla_computation as _xla_computation
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 98: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 98:   from jax import xla_computation as _xla_computation
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
108:   from jax import xla_computation as _xla_computation
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: 2024-09-29 17:53:40.963401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
122: 2024-09-29 17:53:40.963399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
125: 2024-09-29 17:53:40.963399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
127: 2024-09-29 17:53:40.963399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 64: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 64:   from jax import xla_computation as _xla_computation
 65: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 65:   from jax import xla_computation as _xla_computation
 66: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 66:   from jax import xla_computation as _xla_computation
 67: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 67:   from jax import xla_computation as _xla_computation
 68: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 68:   from jax import xla_computation as _xla_computation
 69: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 69:   from jax import xla_computation as _xla_computation
 70: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 70:   from jax import xla_computation as _xla_computation
 71: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 71:   from jax import xla_computation as _xla_computation
126: 2024-09-29 17:53:40.967214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
112:   from jax import xla_computation as _xla_computation
113: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
113:   from jax import xla_computation as _xla_computation
114: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
114:   from jax import xla_computation as _xla_computation
115: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
115:   from jax import xla_computation as _xla_computation
116: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
116:   from jax import xla_computation as _xla_computation
117: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
117:   from jax import xla_computation as _xla_computation
118: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
118:   from jax import xla_computation as _xla_computation
119: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
119:   from jax import xla_computation as _xla_computation
 96: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 96:   from jax import xla_computation as _xla_computation
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: 2024-09-29 17:53:41.055472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 32:   from jax import xla_computation as _xla_computation
 33: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 33:   from jax import xla_computation as _xla_computation
 34: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 34:   from jax import xla_computation as _xla_computation
 35: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 35:   from jax import xla_computation as _xla_computation
 36: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 36:   from jax import xla_computation as _xla_computation
 37: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 37:   from jax import xla_computation as _xla_computation
 38: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 38:   from jax import xla_computation as _xla_computation
 39: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 39:   from jax import xla_computation as _xla_computation
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: 2024-09-29 17:53:41.200315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
120: 2024-09-29 17:53:41.202913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: 2024-09-29 17:53:41.327210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 25: 2024-09-29 17:53:41.327201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 26: 2024-09-29 17:53:41.327213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 27: 2024-09-29 17:53:41.327210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 28: 2024-09-29 17:53:41.327212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 29: 2024-09-29 17:53:41.327217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 30: 2024-09-29 17:53:41.327201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 31: 2024-09-29 17:53:41.327201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
121:   from jax import xla_computation as _xla_computation
122: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
122:   from jax import xla_computation as _xla_computation
124: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
124:   from jax import xla_computation as _xla_computation
125: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
125:   from jax import xla_computation as _xla_computation
126: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
126:   from jax import xla_computation as _xla_computation
127: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
127:   from jax import xla_computation as _xla_computation
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
123:   from jax import xla_computation as _xla_computation
120: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
120:   from jax import xla_computation as _xla_computation
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: [W929 17:53:41.969784953 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W929 17:53:41.969691723 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W929 17:53:41.969755694 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W929 17:53:41.969674506 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W929 17:53:41.969870931 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W929 17:53:41.969859050 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: [W929 17:53:41.969969167 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W929 17:53:41.969968900 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: :::MLLOG {"namespace": "", "time_ms": 1727632421608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 41: :::MLLOG {"namespace": "", "time_ms": 1727632421608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 45: :::MLLOG {"namespace": "", "time_ms": 1727632421609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 47: :::MLLOG {"namespace": "", "time_ms": 1727632421609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 46: :::MLLOG {"namespace": "", "time_ms": 1727632421609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 43: :::MLLOG {"namespace": "", "time_ms": 1727632421610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 44: :::MLLOG {"namespace": "", "time_ms": 1727632421610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 40: :::MLLOG {"namespace": "", "time_ms": 1727632421610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: [W929 17:53:41.572170519 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [W929 17:53:41.572083845 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W929 17:53:41.572092002 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W929 17:53:41.572093718 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:53:41.572117454 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W929 17:53:41.572182680 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W929 17:53:41.572139510 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:53:41.572118025 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: :::MLLOG {"namespace": "", "time_ms": 1727632421665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 75: :::MLLOG {"namespace": "", "time_ms": 1727632421665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 74: :::MLLOG {"namespace": "", "time_ms": 1727632421666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 79: :::MLLOG {"namespace": "", "time_ms": 1727632421666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 76: :::MLLOG {"namespace": "", "time_ms": 1727632421666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 78: :::MLLOG {"namespace": "", "time_ms": 1727632421666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 72: :::MLLOG {"namespace": "", "time_ms": 1727632421666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 77: :::MLLOG {"namespace": "", "time_ms": 1727632421667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: [W929 17:53:41.072786505 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W929 17:53:41.072787508 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [W929 17:53:41.072789974 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W929 17:53:41.072837313 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W929 17:53:41.072788037 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W929 17:53:41.072773536 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [W929 17:53:41.072781253 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W929 17:53:41.072837239 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: :::MLLOG {"namespace": "", "time_ms": 1727632421691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 61: :::MLLOG {"namespace": "", "time_ms": 1727632421691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 62: :::MLLOG {"namespace": "", "time_ms": 1727632421691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: :::MLLOG {"namespace": "", "time_ms": 1727632421691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 60: :::MLLOG {"namespace": "", "time_ms": 1727632421691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 59: :::MLLOG {"namespace": "", "time_ms": 1727632421692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 58: :::MLLOG {"namespace": "", "time_ms": 1727632421692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 63: :::MLLOG {"namespace": "", "time_ms": 1727632421692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 24:   from jax import xla_computation as _xla_computation
 25: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 25:   from jax import xla_computation as _xla_computation
 26: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 26:   from jax import xla_computation as _xla_computation
 27: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 27:   from jax import xla_computation as _xla_computation
 28: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 28:   from jax import xla_computation as _xla_computation
 29: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 29:   from jax import xla_computation as _xla_computation
 30: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 30:   from jax import xla_computation as _xla_computation
 31: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 31:   from jax import xla_computation as _xla_computation
  8: 2024-09-29 17:53:41.823220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  9: 2024-09-29 17:53:41.823228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 10: 2024-09-29 17:53:41.823228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 11: 2024-09-29 17:53:41.823222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 12: 2024-09-29 17:53:41.823221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 13: 2024-09-29 17:53:41.823224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 14: 2024-09-29 17:53:41.823229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 15: 2024-09-29 17:53:41.823228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: [W929 17:53:41.914803043 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W929 17:53:41.914815496 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W929 17:53:41.914802971 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W929 17:53:41.914824459 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W929 17:53:41.914823896 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W929 17:53:41.914823913 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:53:41.914817851 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W929 17:53:41.914780649 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
102: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
100: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
103: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 97: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 99: :::MLLOG {"namespace": "", "time_ms": 1727632421871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 96: :::MLLOG {"namespace": "", "time_ms": 1727632421872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 98: :::MLLOG {"namespace": "", "time_ms": 1727632421872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 88: [W929 17:53:41.408228822 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [W929 17:53:41.408190844 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [W929 17:53:41.408168452 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W929 17:53:41.408190924 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W929 17:53:41.408190850 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W929 17:53:41.408238584 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W929 17:53:41.408190373 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W929 17:53:41.408191388 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: :::MLLOG {"namespace": "", "time_ms": 1727632421911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 92: :::MLLOG {"namespace": "", "time_ms": 1727632421911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 95: :::MLLOG {"namespace": "", "time_ms": 1727632421911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 94: :::MLLOG {"namespace": "", "time_ms": 1727632421912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 93: :::MLLOG {"namespace": "", "time_ms": 1727632421912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 91: :::MLLOG {"namespace": "", "time_ms": 1727632421912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 90: :::MLLOG {"namespace": "", "time_ms": 1727632421913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 89: :::MLLOG {"namespace": "", "time_ms": 1727632421913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 48: [W929 17:53:41.845367465 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [W929 17:53:41.845406329 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:53:41.845367350 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [W929 17:53:41.845399163 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W929 17:53:41.845351001 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [W929 17:53:41.845404220 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W929 17:53:41.845359834 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W929 17:53:41.845405499 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [W929 17:53:41.735913214 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [W929 17:53:41.735856907 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W929 17:53:41.735840504 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W929 17:53:41.735901855 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W929 17:53:41.735865563 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [W929 17:53:41.735888792 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [W929 17:53:41.735865420 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [W929 17:53:41.735865547 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: :::MLLOG {"namespace": "", "time_ms": 1727632421965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 50: :::MLLOG {"namespace": "", "time_ms": 1727632421965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 52: :::MLLOG {"namespace": "", "time_ms": 1727632421965, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 54: :::MLLOG {"namespace": "", "time_ms": 1727632421966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 51: :::MLLOG {"namespace": "", "time_ms": 1727632421966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 49: :::MLLOG {"namespace": "", "time_ms": 1727632421966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 53: :::MLLOG {"namespace": "", "time_ms": 1727632421966, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 55: :::MLLOG {"namespace": "", "time_ms": 1727632421967, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 84: :::MLLOG {"namespace": "", "time_ms": 1727632421972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 87: :::MLLOG {"namespace": "", "time_ms": 1727632421972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 86: :::MLLOG {"namespace": "", "time_ms": 1727632421972, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 83: :::MLLOG {"namespace": "", "time_ms": 1727632421973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 81: :::MLLOG {"namespace": "", "time_ms": 1727632421973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 80: :::MLLOG {"namespace": "", "time_ms": 1727632421973, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 85: :::MLLOG {"namespace": "", "time_ms": 1727632421974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 82: :::MLLOG {"namespace": "", "time_ms": 1727632421974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: [W929 17:53:42.987595744 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W929 17:53:42.987595959 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [W929 17:53:42.987597012 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W929 17:53:42.987597384 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W929 17:53:42.987595452 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W929 17:53:42.987635936 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W929 17:53:42.987591231 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W929 17:53:42.987639158 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: 2024-09-29 17:53:42.126881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  1: 2024-09-29 17:53:42.126882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  2: 2024-09-29 17:53:42.126875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  3: 2024-09-29 17:53:42.126881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: 2024-09-29 17:53:42.126882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  5: 2024-09-29 17:53:42.126884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  6: 2024-09-29 17:53:42.126882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  7: 2024-09-29 17:53:42.126881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 69: :::MLLOG {"namespace": "", "time_ms": 1727632422139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 65: :::MLLOG {"namespace": "", "time_ms": 1727632422139, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 71: :::MLLOG {"namespace": "", "time_ms": 1727632422140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 67: :::MLLOG {"namespace": "", "time_ms": 1727632422140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 64: :::MLLOG {"namespace": "", "time_ms": 1727632422140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 66: :::MLLOG {"namespace": "", "time_ms": 1727632422141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 68: :::MLLOG {"namespace": "", "time_ms": 1727632422141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 70: :::MLLOG {"namespace": "", "time_ms": 1727632422141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
112: [W929 17:53:42.286434211 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [W929 17:53:42.286434391 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W929 17:53:42.286459981 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W929 17:53:42.286459717 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W929 17:53:42.286440366 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W929 17:53:42.286476604 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W929 17:53:42.286435358 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W929 17:53:42.286494029 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: :::MLLOG {"namespace": "", "time_ms": 1727632422155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
116: :::MLLOG {"namespace": "", "time_ms": 1727632422155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
113: :::MLLOG {"namespace": "", "time_ms": 1727632422155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
114: :::MLLOG {"namespace": "", "time_ms": 1727632422156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
115: :::MLLOG {"namespace": "", "time_ms": 1727632422155, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
119: :::MLLOG {"namespace": "", "time_ms": 1727632422156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
117: :::MLLOG {"namespace": "", "time_ms": 1727632422156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
118: :::MLLOG {"namespace": "", "time_ms": 1727632422156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
104: [W929 17:53:42.046268816 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W929 17:53:42.046267377 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W929 17:53:42.046273943 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W929 17:53:42.046286891 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W929 17:53:42.046325958 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W929 17:53:42.046266794 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W929 17:53:42.046266929 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:53:42.046304105 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
110: :::MLLOG {"namespace": "", "time_ms": 1727632422218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
107: :::MLLOG {"namespace": "", "time_ms": 1727632422218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
109: :::MLLOG {"namespace": "", "time_ms": 1727632422218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
105: :::MLLOG {"namespace": "", "time_ms": 1727632422218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
106: :::MLLOG {"namespace": "", "time_ms": 1727632422218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
108: :::MLLOG {"namespace": "", "time_ms": 1727632422219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: :::MLLOG {"namespace": "", "time_ms": 1727632422219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
111: :::MLLOG {"namespace": "", "time_ms": 1727632422219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: 2024-09-29 17:53:42.279964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 17: 2024-09-29 17:53:42.279963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 18: 2024-09-29 17:53:42.279973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 19: 2024-09-29 17:53:42.279968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 20: 2024-09-29 17:53:42.279963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 21: 2024-09-29 17:53:42.279964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 22: 2024-09-29 17:53:42.279963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 23: 2024-09-29 17:53:42.279968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  8: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  8:   from jax import xla_computation as _xla_computation
  9: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  9:   from jax import xla_computation as _xla_computation
 10: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 10:   from jax import xla_computation as _xla_computation
 11: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 11:   from jax import xla_computation as _xla_computation
 12: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 12:   from jax import xla_computation as _xla_computation
 13: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 13:   from jax import xla_computation as _xla_computation
 14: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 14:   from jax import xla_computation as _xla_computation
 15: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 15:   from jax import xla_computation as _xla_computation
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: [W929 17:53:42.219538731 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [W929 17:53:42.219538505 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W929 17:53:42.219549627 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W929 17:53:42.219552177 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W929 17:53:42.219552323 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W929 17:53:42.219578843 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:53:42.219551063 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W929 17:53:42.219599994 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: :::MLLOG {"namespace": "", "time_ms": 1727632422529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 35: :::MLLOG {"namespace": "", "time_ms": 1727632422529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 38: :::MLLOG {"namespace": "", "time_ms": 1727632422529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 32: :::MLLOG {"namespace": "", "time_ms": 1727632422529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 39: :::MLLOG {"namespace": "", "time_ms": 1727632422529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 36: :::MLLOG {"namespace": "", "time_ms": 1727632422530, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 37: :::MLLOG {"namespace": "", "time_ms": 1727632422530, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 34: :::MLLOG {"namespace": "", "time_ms": 1727632422531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  0:   from jax import xla_computation as _xla_computation
  1: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  1:   from jax import xla_computation as _xla_computation
  2: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  2:   from jax import xla_computation as _xla_computation
  3: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  3:   from jax import xla_computation as _xla_computation
  4: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  4:   from jax import xla_computation as _xla_computation
  5: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  5:   from jax import xla_computation as _xla_computation
  6: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  6:   from jax import xla_computation as _xla_computation
  7: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  7:   from jax import xla_computation as _xla_computation
120: [W929 17:53:42.318788421 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W929 17:53:42.318742986 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W929 17:53:42.318743075 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:53:42.318806809 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:53:42.318771739 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W929 17:53:42.318750516 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W929 17:53:42.318776900 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:53:42.318760272 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: :::MLLOG {"namespace": "", "time_ms": 1727632422732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
125: :::MLLOG {"namespace": "", "time_ms": 1727632422732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
123: :::MLLOG {"namespace": "", "time_ms": 1727632422733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
121: :::MLLOG {"namespace": "", "time_ms": 1727632422733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
127: :::MLLOG {"namespace": "", "time_ms": 1727632422733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
122: :::MLLOG {"namespace": "", "time_ms": 1727632422733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
126: :::MLLOG {"namespace": "", "time_ms": 1727632422734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
124: :::MLLOG {"namespace": "", "time_ms": 1727632422734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 16:   from jax import xla_computation as _xla_computation
 17: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 17:   from jax import xla_computation as _xla_computation
 18: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 18:   from jax import xla_computation as _xla_computation
 19: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 19:   from jax import xla_computation as _xla_computation
 20: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 20:   from jax import xla_computation as _xla_computation
 21: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 21:   from jax import xla_computation as _xla_computation
 22: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 22:   from jax import xla_computation as _xla_computation
 23: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 23:   from jax import xla_computation as _xla_computation
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: [W929 17:53:43.488023677 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W929 17:53:43.488035496 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W929 17:53:43.488052785 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W929 17:53:43.488037471 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W929 17:53:43.488032770 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:53:43.488032662 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W929 17:53:43.487986917 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:53:43.488033043 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: :::MLLOG {"namespace": "", "time_ms": 1727632423441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 30: :::MLLOG {"namespace": "", "time_ms": 1727632423442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 31: :::MLLOG {"namespace": "", "time_ms": 1727632423442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 25: :::MLLOG {"namespace": "", "time_ms": 1727632423442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 27: :::MLLOG {"namespace": "", "time_ms": 1727632423442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 28: :::MLLOG {"namespace": "", "time_ms": 1727632423442, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 24: :::MLLOG {"namespace": "", "time_ms": 1727632423443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 26: :::MLLOG {"namespace": "", "time_ms": 1727632423443, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: [W929 17:53:43.225922177 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W929 17:53:43.225905065 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W929 17:53:43.225890688 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W929 17:53:43.225923253 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W929 17:53:43.225878019 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [W929 17:53:43.225878648 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:53:43.225931239 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W929 17:53:43.225943200 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: :::MLLOG {"namespace": "", "time_ms": 1727632423865, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  9: :::MLLOG {"namespace": "", "time_ms": 1727632423866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 12: :::MLLOG {"namespace": "", "time_ms": 1727632423866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 15: :::MLLOG {"namespace": "", "time_ms": 1727632423866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 14: :::MLLOG {"namespace": "", "time_ms": 1727632423866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 11: :::MLLOG {"namespace": "", "time_ms": 1727632423867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 13: :::MLLOG {"namespace": "", "time_ms": 1727632423867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: :::MLLOG {"namespace": "", "time_ms": 1727632423867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: [W929 17:53:44.545854884 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W929 17:53:44.545850097 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [W929 17:53:44.545853677 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W929 17:53:44.545859749 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:53:44.545896906 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W929 17:53:44.545895219 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:53:44.545889608 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W929 17:53:44.545824857 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: :::MLLOG {"namespace": "", "time_ms": 1727632424252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  2: :::MLLOG {"namespace": "", "time_ms": 1727632424252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  3: :::MLLOG {"namespace": "", "time_ms": 1727632424252, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632424253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  1: :::MLLOG {"namespace": "", "time_ms": 1727632424253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  6: :::MLLOG {"namespace": "", "time_ms": 1727632424253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  4: :::MLLOG {"namespace": "", "time_ms": 1727632424254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  5: :::MLLOG {"namespace": "", "time_ms": 1727632424254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
104: [W929 17:53:44.189063845 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 20: [W929 17:53:44.711276812 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W929 17:53:44.711275729 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [W929 17:53:44.711358488 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W929 17:53:44.711317699 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W929 17:53:44.711329717 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W929 17:53:44.711317288 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:53:44.711365422 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [W929 17:53:44.711350258 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: :::MLLOG {"namespace": "", "time_ms": 1727632424406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 21: :::MLLOG {"namespace": "", "time_ms": 1727632424406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: :::MLLOG {"namespace": "", "time_ms": 1727632424407, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 19: :::MLLOG {"namespace": "", "time_ms": 1727632424407, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 23: :::MLLOG {"namespace": "", "time_ms": 1727632424407, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 17: :::MLLOG {"namespace": "", "time_ms": 1727632424407, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 18: :::MLLOG {"namespace": "", "time_ms": 1727632424408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 22: :::MLLOG {"namespace": "", "time_ms": 1727632424408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: [W929 17:53:44.770788654 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 24: [W929 17:53:44.512788481 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 27: [W929 17:53:44.597798356 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 28: [W929 17:53:44.600108063 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 26: [W929 17:53:44.626212792 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
114: [W929 17:53:44.711682097 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 25: [W929 17:53:44.642394170 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
115: [W929 17:53:44.729117478 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [W929 17:53:44.729569935 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
117: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
108: [W929 17:53:44.429934180 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
109: [W929 17:53:44.430958562 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
118: [W929 17:53:44.739077455 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
110: [W929 17:53:44.438816197 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
105: [W929 17:53:44.442374205 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
116: [W929 17:53:44.754603367 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
111: [W929 17:53:44.457439967 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 29: [W929 17:53:44.676978120 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
113: [W929 17:53:44.761964507 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 41: [W929 17:53:44.994813757 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [W929 17:53:44.460519619 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
107: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 57: [W929 17:53:44.024143172 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 31: [W929 17:53:44.689967198 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 46: [W929 17:53:44.007227625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
119: [W929 17:53:44.779563649 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 32: [W929 17:53:44.347690788 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 45: [W929 17:53:44.019857170 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 30: [W929 17:53:44.716139876 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 43: [W929 17:53:44.035102075 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 40: [W929 17:53:44.045061857 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 42: [W929 17:53:44.053204979 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 44: [W929 17:53:44.065110356 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 72: [W929 17:53:44.618664602 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 47: [W929 17:53:44.072930506 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
104: using fp8 FMHA
 60: [W929 17:53:44.097530701 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 59: [W929 17:53:44.099165178 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 58: [W929 17:53:44.101279021 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
106: [W929 17:53:44.552072585 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 78: [W929 17:53:44.638574010 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 74: [W929 17:53:44.643211532 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 63: [W929 17:53:44.119006873 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
109: using fp8 FMHA
 36: [W929 17:53:44.461692812 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
118: using fp8 FMHA
114: using fp8 FMHA
 56: [W929 17:53:44.151094853 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: using fp8 FMHA
 56: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
105: using fp8 FMHA
110: using fp8 FMHA
 79: [W929 17:53:44.684393581 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 61: [W929 17:53:44.160869832 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
117: using fp8 FMHA
108: using fp8 FMHA
116: using fp8 FMHA
113: using fp8 FMHA
 35: [W929 17:53:44.493128781 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
111: using fp8 FMHA
107: using fp8 FMHA
 28: using fp8 FMHA
 27: using fp8 FMHA
 41: using fp8 FMHA
 46: using fp8 FMHA
 73: [W929 17:53:44.723369184 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [W929 17:53:44.509178671 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 34: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 26: using fp8 FMHA
 57: using fp8 FMHA
 45: using fp8 FMHA
 62: [W929 17:53:44.199942356 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 25: using fp8 FMHA
119: using fp8 FMHA
 76: [W929 17:53:44.733875151 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 38: [W929 17:53:44.520415481 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [W929 17:53:44.735021109 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 77: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 75: [W929 17:53:44.738668136 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
120: [W929 17:53:44.418352502 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 30: using fp8 FMHA
 29: using fp8 FMHA
 37: [W929 17:53:44.537330571 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 31: using fp8 FMHA
 43: using fp8 FMHA
 24: using fp8 FMHA
 39: [W929 17:53:44.570112111 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 33: [W929 17:53:44.572082681 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 60: using fp8 FMHA
 58: using fp8 FMHA
106: using fp8 FMHA
 59: using fp8 FMHA
 42: using fp8 FMHA
  8: [W929 17:53:44.261216196 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 63: using fp8 FMHA
 47: using fp8 FMHA
 44: using fp8 FMHA
 90: [W929 17:53:44.425649433 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 94: [W929 17:53:44.439475321 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 85: [W929 17:53:44.708728801 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 82: [W929 17:53:44.714164485 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 96: [W929 17:53:44.999399583 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 74: using fp8 FMHA
 92: [W929 17:53:44.450649488 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 95: [W929 17:53:44.452405872 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 81: [W929 17:53:44.729118676 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 78: using fp8 FMHA
 87: [W929 17:53:44.734860814 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 50: [W929 17:53:44.850542466 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: using fp8 FMHA
 50: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 35: using fp8 FMHA
 36: using fp8 FMHA
 34: using fp8 FMHA
 89: [W929 17:53:44.482033648 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 53: [W929 17:53:44.866394075 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 54: [W929 17:53:44.869351946 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 79: using fp8 FMHA
 83: [W929 17:53:44.756204385 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 12: [W929 17:53:44.358396584 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 91: [W929 17:53:44.493476000 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
  9: [W929 17:53:44.361990878 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 93: [W929 17:53:44.498774786 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 48: [W929 17:53:44.882716224 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: using fp8 FMHA
 48: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 73: using fp8 FMHA
 98: [W929 17:53:44.053341127 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 32: using fp8 FMHA
 38: using fp8 FMHA
 99: [W929 17:53:45.056416193 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 97: [W929 17:53:45.059302058 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 86: [W929 17:53:45.778041495 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
102: [W929 17:53:45.060608198 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
100: [W929 17:53:45.061995999 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 75: using fp8 FMHA
 88: [W929 17:53:45.518883157 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 62: using fp8 FMHA
 76: using fp8 FMHA
 49: [W929 17:53:45.910229749 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 39: using fp8 FMHA
 84: [W929 17:53:45.796961096 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 80: [W929 17:53:45.801167541 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 77: using fp8 FMHA
 33: using fp8 FMHA
103: [W929 17:53:45.090898118 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 10: [W929 17:53:45.409229114 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
101: [W929 17:53:45.095206439 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 51: [W929 17:53:45.930680126 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 15: [W929 17:53:45.418131554 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [W929 17:53:45.935292635 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 52: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 40: using fp8 FMHA
 14: [W929 17:53:45.434660240 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 55: [W929 17:53:45.951975241 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
126: [W929 17:53:45.670827170 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 13: [W929 17:53:45.443897839 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 72: using fp8 FMHA
 11: [W929 17:53:45.457644398 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
124: [W929 17:53:45.689591510 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
122: [W929 17:53:45.692961871 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 90: using fp8 FMHA
 94: using fp8 FMHA
 92: using fp8 FMHA
 95: using fp8 FMHA
 82: using fp8 FMHA
 56: using fp8 FMHA
 81: using fp8 FMHA
 85: using fp8 FMHA
121: [W929 17:53:45.718065542 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 87: using fp8 FMHA
 70: [W929 17:53:45.983538000 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
125: [W929 17:53:45.732851383 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
127: [W929 17:53:45.734480607 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 89: using fp8 FMHA
 66: [W929 17:53:45.003217154 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 50: using fp8 FMHA
 54: using fp8 FMHA
 91: using fp8 FMHA
 53: using fp8 FMHA
 83: using fp8 FMHA
 93: using fp8 FMHA
 67: [W929 17:53:45.041302693 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 69: [W929 17:53:45.043635001 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 68: [W929 17:53:45.044890646 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
123: [W929 17:53:45.784632963 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
112: [W929 17:53:45.342832295 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 71: [W929 17:53:45.086781889 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
100: using fp8 FMHA
 99: using fp8 FMHA
 98: using fp8 FMHA
102: using fp8 FMHA
 64: [W929 17:53:45.103735094 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 86: using fp8 FMHA
 65: [W929 17:53:45.111893212 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
122: using fp8 FMHA
126: using fp8 FMHA
 52: using fp8 FMHA
120: using fp8 FMHA
124: using fp8 FMHA
 12: using fp8 FMHA
 13: using fp8 FMHA
  9: using fp8 FMHA
 10: using fp8 FMHA
 84: using fp8 FMHA
 49: using fp8 FMHA
 51: using fp8 FMHA
 14: using fp8 FMHA
 55: using fp8 FMHA
 15: using fp8 FMHA
 97: using fp8 FMHA
101: using fp8 FMHA
121: using fp8 FMHA
103: using fp8 FMHA
125: using fp8 FMHA
127: using fp8 FMHA
 11: using fp8 FMHA
 70: using fp8 FMHA
 66: using fp8 FMHA
  5: [W929 17:53:45.636980839 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
  2: [W929 17:53:45.672354012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
  8: using fp8 FMHA
  6: [W929 17:53:45.688034620 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 96: using fp8 FMHA
  1: [W929 17:53:45.695621272 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
  3: [W929 17:53:45.711872862 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
123: using fp8 FMHA
 48: using fp8 FMHA
  4: [W929 17:53:45.727377675 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 68: using fp8 FMHA
  7: [W929 17:53:45.752041208 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: using fp8 FMHA
  7: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 67: using fp8 FMHA
 65: using fp8 FMHA
 71: using fp8 FMHA
 88: using fp8 FMHA
 80: using fp8 FMHA
  2: using fp8 FMHA
  5: using fp8 FMHA
  6: using fp8 FMHA
  1: using fp8 FMHA
  3: using fp8 FMHA
  4: using fp8 FMHA
  7: using fp8 FMHA
 20: [W929 17:53:45.986099758 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 64: using fp8 FMHA
112: using fp8 FMHA
 17: [W929 17:53:45.078814670 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [W929 17:53:45.078833361 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [W929 17:53:45.078877179 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [W929 17:53:45.078938779 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 19: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 18: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 21: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 23: [W929 17:53:45.085166682 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 22: [W929 17:53:45.085925761 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
  0: [W929 17:53:45.074604814 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2346, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1279}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 9216, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1281}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 36, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1283}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1285}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 740.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1289}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632425772, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1291}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=2346, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
  0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_p
  0: g=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0))
 20: using fp8 FMHA
 16: using fp8 FMHA
 19: using fp8 FMHA
 17: using fp8 FMHA
 21: using fp8 FMHA
 23: using fp8 FMHA
 22: using fp8 FMHA
 18: using fp8 FMHA
  0: using fp8 FMHA
 56: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 56:   self._overflow_buf = torch.cuda.IntTensor([0])
 57: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 57:   self._overflow_buf = torch.cuda.IntTensor([0])
 58: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 58:   self._overflow_buf = torch.cuda.IntTensor([0])
 60: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 60:   self._overflow_buf = torch.cuda.IntTensor([0])
 61: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 61:   self._overflow_buf = torch.cuda.IntTensor([0])
 62: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 62:   self._overflow_buf = torch.cuda.IntTensor([0])
 61: [rank61]:[W929 17:53:47.802164394 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:53:47.802191226 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 17:53:47.802191249 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:53:47.802211837 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:53:47.802211712 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:53:47.802211612 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:53:47.802703030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:53:47.802703402 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 17:53:47.802694619 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:53:47.802721310 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:53:47.802737642 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:53:47.802738733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 41:   self._overflow_buf = torch.cuda.IntTensor([0])
 42: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 42:   self._overflow_buf = torch.cuda.IntTensor([0])
 45: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 45:   self._overflow_buf = torch.cuda.IntTensor([0])
 46: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 46:   self._overflow_buf = torch.cuda.IntTensor([0])
 45: [rank45]:[W929 17:53:47.784677221 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:53:47.784677276 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:53:47.784704494 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:53:47.784704414 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:53:47.785244787 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:53:47.785244508 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:53:47.785264974 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:53:47.785281955 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 73:   self._overflow_buf = torch.cuda.IntTensor([0])
 74: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 74:   self._overflow_buf = torch.cuda.IntTensor([0])
 75: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 75:   self._overflow_buf = torch.cuda.IntTensor([0])
 76: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 76:   self._overflow_buf = torch.cuda.IntTensor([0])
 79: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 79:   self._overflow_buf = torch.cuda.IntTensor([0])
 73: [rank73]:[W929 17:53:47.391758944 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:53:47.391758840 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:53:47.391785988 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:53:47.391787953 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:53:47.391801039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:53:47.392258214 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:53:47.392257938 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:53:47.392288174 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:53:47.392291119 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:53:47.392303298 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 47:   self._overflow_buf = torch.cuda.IntTensor([0])
 47: [rank47]:[W929 17:53:47.863612337 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 17:53:47.864250507 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 44:   self._overflow_buf = torch.cuda.IntTensor([0])
 44: [rank44]:[W929 17:53:47.865037981 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:53:47.865609013 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 59:   self._overflow_buf = torch.cuda.IntTensor([0])
 59: [rank59]:[W929 17:53:47.886135940 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:53:47.886653719 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 63:   self._overflow_buf = torch.cuda.IntTensor([0])
 63: [rank63]:[W929 17:53:47.888960861 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:53:47.889500785 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 43:   self._overflow_buf = torch.cuda.IntTensor([0])
 43: [rank43]:[W929 17:53:47.870810332 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: [rank43]:[W929 17:53:47.871390381 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 40:   self._overflow_buf = torch.cuda.IntTensor([0])
 40: [rank40]:[W929 17:53:47.879211571 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 17:53:47.879833585 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 72:   self._overflow_buf = torch.cuda.IntTensor([0])
 72: [rank72]:[W929 17:53:47.474299894 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 77:   self._overflow_buf = torch.cuda.IntTensor([0])
 78: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 78:   self._overflow_buf = torch.cuda.IntTensor([0])
 72: [rank72]:[W929 17:53:47.474772926 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 17:53:47.475245021 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:53:47.475273650 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 17:53:47.475749719 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:53:47.475773921 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 81:   self._overflow_buf = torch.cuda.IntTensor([0])
 82: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 82:   self._overflow_buf = torch.cuda.IntTensor([0])
 84: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 84:   self._overflow_buf = torch.cuda.IntTensor([0])
 86: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 86:   self._overflow_buf = torch.cuda.IntTensor([0])
 87: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 87:   self._overflow_buf = torch.cuda.IntTensor([0])
 84: [rank84]:[W929 17:53:47.509999207 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:53:47.509999061 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:53:47.509999160 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:53:47.510024107 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:53:47.510024512 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:53:47.510460017 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:53:47.510478492 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:53:47.510494357 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:53:47.510499568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:53:47.510504345 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 50:   self._overflow_buf = torch.cuda.IntTensor([0])
 52: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 52:   self._overflow_buf = torch.cuda.IntTensor([0])
 54: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 54:   self._overflow_buf = torch.cuda.IntTensor([0])
 48: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 48:   self._overflow_buf = torch.cuda.IntTensor([0])
 52: [rank52]:[W929 17:53:47.650994975 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:53:47.650994924 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:53:47.651014413 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:53:47.651302050 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:53:47.651567885 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:53:47.651544961 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:53:47.651544587 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:53:47.651822958 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 89:   self._overflow_buf = torch.cuda.IntTensor([0])
 90: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 90:   self._overflow_buf = torch.cuda.IntTensor([0])
 91: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 91:   self._overflow_buf = torch.cuda.IntTensor([0])
 92: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 92:   self._overflow_buf = torch.cuda.IntTensor([0])
 94: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 94:   self._overflow_buf = torch.cuda.IntTensor([0])
 95: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 95:   self._overflow_buf = torch.cuda.IntTensor([0])
 89: [rank89]:[W929 17:53:47.310097039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:53:47.310097123 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:53:47.310097238 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:53:47.310103426 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:53:47.310090022 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 17:53:47.310550641 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:53:47.310553359 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:53:47.310550613 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:53:47.310542997 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:53:47.310551203 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:53:47.310546084 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:53:47.310937348 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 85:   self._overflow_buf = torch.cuda.IntTensor([0])
 85: [rank85]:[W929 17:53:47.583906115 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 17:53:47.584400030 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 83:   self._overflow_buf = torch.cuda.IntTensor([0])
 83: [rank83]:[W929 17:53:47.592504918 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:53:47.593011489 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 80:   self._overflow_buf = torch.cuda.IntTensor([0])
 80: [rank80]:[W929 17:53:47.597621906 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 17:53:47.598098782 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 49:   self._overflow_buf = torch.cuda.IntTensor([0])
 51: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 51:   self._overflow_buf = torch.cuda.IntTensor([0])
 49: [rank49]:[W929 17:53:47.726297690 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:53:47.726818705 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 17:53:47.726876567 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:53:47.727394451 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 55:   self._overflow_buf = torch.cuda.IntTensor([0])
 55: [rank55]:[W929 17:53:47.732490381 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 53:   self._overflow_buf = torch.cuda.IntTensor([0])
 55: [rank55]:[W929 17:53:47.733061230 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:53:47.733797723 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:53:47.734359073 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 93:   self._overflow_buf = torch.cuda.IntTensor([0])
 93: [rank93]:[W929 17:53:47.392310935 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:53:47.392732716 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 88:   self._overflow_buf = torch.cuda.IntTensor([0])
 88: [rank88]:[W929 17:53:47.395077576 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 17:53:47.395530901 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
103:   self._overflow_buf = torch.cuda.IntTensor([0])
103: [rank103]:[W929 17:53:47.988201720 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:53:47.988627166 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
102:   self._overflow_buf = torch.cuda.IntTensor([0])
102: [rank102]:[W929 17:53:48.066879677 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:53:48.067323872 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
101:   self._overflow_buf = torch.cuda.IntTensor([0])
100: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
100:   self._overflow_buf = torch.cuda.IntTensor([0])
101: [rank101]:[W929 17:53:48.070042969 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:53:48.070446337 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: [rank100]:[W929 17:53:48.070476885 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: [rank100]:[W929 17:53:48.070861736 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 98:   self._overflow_buf = torch.cuda.IntTensor([0])
 98: [rank98]:[W929 17:53:48.078602239 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 97:   self._overflow_buf = torch.cuda.IntTensor([0])
 98: [rank98]:[W929 17:53:48.079026354 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 96:   self._overflow_buf = torch.cuda.IntTensor([0])
 97: [rank97]:[W929 17:53:48.079747545 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 17:53:48.080214067 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:53:48.080347053 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:53:48.080755376 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 99:   self._overflow_buf = torch.cuda.IntTensor([0])
 99: [rank99]:[W929 17:53:48.083655625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:53:48.084073442 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 64:   self._overflow_buf = torch.cuda.IntTensor([0])
 65: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 65:   self._overflow_buf = torch.cuda.IntTensor([0])
 67: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 67:   self._overflow_buf = torch.cuda.IntTensor([0])
 68: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 68:   self._overflow_buf = torch.cuda.IntTensor([0])
 69: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 69:   self._overflow_buf = torch.cuda.IntTensor([0])
 71: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 71:   self._overflow_buf = torch.cuda.IntTensor([0])
 64: [rank64]:[W929 17:53:48.937482275 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:53:48.937482173 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:53:48.937482292 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:53:48.937499329 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:53:48.937492240 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:53:48.937488470 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: [rank64]:[W929 17:53:48.937982040 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:53:48.937987535 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:53:48.937980192 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:53:48.937992446 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:53:48.938000647 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:53:48.938001411 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 70:   self._overflow_buf = torch.cuda.IntTensor([0])
 70: [rank70]:[W929 17:53:48.018987903 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 17:53:48.019483252 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 66:   self._overflow_buf = torch.cuda.IntTensor([0])
 66: [rank66]:[W929 17:53:48.023629973 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [rank66]:[W929 17:53:48.024193946 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
104:   self._overflow_buf = torch.cuda.IntTensor([0])
105: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
105:   self._overflow_buf = torch.cuda.IntTensor([0])
106: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
106:   self._overflow_buf = torch.cuda.IntTensor([0])
109: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
109:   self._overflow_buf = torch.cuda.IntTensor([0])
110: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
110:   self._overflow_buf = torch.cuda.IntTensor([0])
109: [rank109]:[W929 17:53:49.512801679 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:53:49.512801568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:53:49.512837177 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:53:49.512837158 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:53:49.512837058 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:53:49.513173294 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:53:49.513197063 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:53:49.513219189 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:53:49.513217419 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:53:49.513215297 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
113:   self._overflow_buf = torch.cuda.IntTensor([0])
116: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
116:   self._overflow_buf = torch.cuda.IntTensor([0])
118: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
118:   self._overflow_buf = torch.cuda.IntTensor([0])
113: [rank113]:[W929 17:53:49.857470962 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:53:49.857526153 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:53:49.857544496 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:53:49.857842403 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:53:49.857895079 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:53:49.857935354 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
112:   self._overflow_buf = torch.cuda.IntTensor([0])
112: [rank112]:[W929 17:53:49.862712038 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: [rank112]:[W929 17:53:49.863068784 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
107:   self._overflow_buf = torch.cuda.IntTensor([0])
107: [rank107]:[W929 17:53:49.593501052 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:53:49.593921204 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
108:   self._overflow_buf = torch.cuda.IntTensor([0])
108: [rank108]:[W929 17:53:49.598169073 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:53:49.598541504 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
111:   self._overflow_buf = torch.cuda.IntTensor([0])
111: [rank111]:[W929 17:53:49.599641533 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:53:49.600039881 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
114:   self._overflow_buf = torch.cuda.IntTensor([0])
115: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
115:   self._overflow_buf = torch.cuda.IntTensor([0])
114: [rank114]:[W929 17:53:49.939495384 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 17:53:49.939861014 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:53:49.940036523 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:53:49.940380179 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
117:   self._overflow_buf = torch.cuda.IntTensor([0])
117: [rank117]:[W929 17:53:49.943281480 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 17:53:49.943632098 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
119:   self._overflow_buf = torch.cuda.IntTensor([0])
119: [rank119]:[W929 17:53:49.945690686 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:53:49.946070382 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 33:   self._overflow_buf = torch.cuda.IntTensor([0])
 35: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 35:   self._overflow_buf = torch.cuda.IntTensor([0])
 36: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 36:   self._overflow_buf = torch.cuda.IntTensor([0])
 37: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 37:   self._overflow_buf = torch.cuda.IntTensor([0])
 39: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 39:   self._overflow_buf = torch.cuda.IntTensor([0])
 33: [rank33]:[W929 17:53:50.735313538 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:53:50.735313444 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:53:50.735350922 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 17:53:50.735351082 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:53:50.735350982 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 17:53:50.735947641 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:53:50.735955759 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:53:50.736062859 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 17:53:50.736074176 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:53:50.736050218 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 32:   self._overflow_buf = torch.cuda.IntTensor([0])
 32: [rank32]:[W929 17:53:50.748729624 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 17:53:50.749280998 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 24:   self._overflow_buf = torch.cuda.IntTensor([0])
 30: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 30:   self._overflow_buf = torch.cuda.IntTensor([0])
 30: [rank30]:[W929 17:53:50.158579803 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:53:50.158674950 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:53:50.159259204 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:53:50.159333022 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 38:   self._overflow_buf = torch.cuda.IntTensor([0])
 38: [rank38]:[W929 17:53:50.814013106 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 34:   self._overflow_buf = torch.cuda.IntTensor([0])
 38: [rank38]:[W929 17:53:50.814590398 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:53:50.815243534 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:53:50.815891599 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 31:   self._overflow_buf = torch.cuda.IntTensor([0])
 31: [rank31]:[W929 17:53:50.238404002 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 17:53:50.239035004 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 28:   self._overflow_buf = torch.cuda.IntTensor([0])
 28: [rank28]:[W929 17:53:50.239941566 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:53:50.240590980 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 29:   self._overflow_buf = torch.cuda.IntTensor([0])
 27: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 27:   self._overflow_buf = torch.cuda.IntTensor([0])
 29: [rank29]:[W929 17:53:50.241717674 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:53:50.241959366 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:53:50.242352978 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:53:50.242628753 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 26:   self._overflow_buf = torch.cuda.IntTensor([0])
 26: [rank26]:[W929 17:53:50.245756877 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:53:50.246404775 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 25:   self._overflow_buf = torch.cuda.IntTensor([0])
 25: [rank25]:[W929 17:53:50.248625388 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 17:53:50.249269458 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/word_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/position_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/token_type_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/output_bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430243, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430244, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_weights"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430244, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_bias"}}
121: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
121:   self._overflow_buf = torch.cuda.IntTensor([0])
122: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
122:   self._overflow_buf = torch.cuda.IntTensor([0])
125: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
125:   self._overflow_buf = torch.cuda.IntTensor([0])
127: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
127:   self._overflow_buf = torch.cuda.IntTensor([0])
121: [rank121]:[W929 17:53:50.104439581 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:53:50.104439668 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:53:50.104467546 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:53:50.104469645 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 17:53:50.104801978 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:53:50.104798533 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:53:50.104806785 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:53:50.104827492 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  9:   self._overflow_buf = torch.cuda.IntTensor([0])
 10: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 10:   self._overflow_buf = torch.cuda.IntTensor([0])
 12: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 12:   self._overflow_buf = torch.cuda.IntTensor([0])
 13: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 13:   self._overflow_buf = torch.cuda.IntTensor([0])
 10: [rank10]:[W929 17:53:50.881097701 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:53:50.881084761 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:53:50.881084988 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:53:50.881127641 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:53:50.881815988 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:53:50.881801383 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:53:50.881800501 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:53:50.881827855 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  1:   self._overflow_buf = torch.cuda.IntTensor([0])
  7: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  7:   self._overflow_buf = torch.cuda.IntTensor([0])
  7: [rank7]:[W929 17:53:50.852872029 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:53:50.852915065 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:53:50.853815797 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:53:50.853703430 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 15:   self._overflow_buf = torch.cuda.IntTensor([0])
 15: [rank15]:[W929 17:53:50.959717879 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:53:50.960480136 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 11:   self._overflow_buf = torch.cuda.IntTensor([0])
 11: [rank11]:[W929 17:53:50.962022705 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:53:50.962789367 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
123:   self._overflow_buf = torch.cuda.IntTensor([0])
124: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
124:   self._overflow_buf = torch.cuda.IntTensor([0])
126: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
126:   self._overflow_buf = torch.cuda.IntTensor([0])
120: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
120:   self._overflow_buf = torch.cuda.IntTensor([0])
123: [rank123]:[W929 17:53:50.193727275 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:53:50.193954452 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:53:50.194011428 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: [rank123]:[W929 17:53:50.194078373 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:53:50.194305065 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:53:50.194338060 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:53:50.194344646 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:53:50.194668714 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  4:   self._overflow_buf = torch.cuda.IntTensor([0])
  6: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  6:   self._overflow_buf = torch.cuda.IntTensor([0])
  2: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  2:   self._overflow_buf = torch.cuda.IntTensor([0])
  4: [rank4]:[W929 17:53:50.899756617 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  3:   self._overflow_buf = torch.cuda.IntTensor([0])
  5: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  5:   self._overflow_buf = torch.cuda.IntTensor([0])
  6: [rank6]:[W929 17:53:50.900145977 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:53:50.900256721 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:53:50.900462955 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 17:53:50.900615485 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:53:50.900607887 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:53:50.900942568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:53:50.901137885 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:53:50.901360942 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:53:50.901458472 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 19:   self._overflow_buf = torch.cuda.IntTensor([0])
 20: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 20:   self._overflow_buf = torch.cuda.IntTensor([0])
 21: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 21:   self._overflow_buf = torch.cuda.IntTensor([0])
 20: [rank20]:[W929 17:53:50.919333261 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:53:50.919338775 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:53:50.919501048 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 17:   self._overflow_buf = torch.cuda.IntTensor([0])
 20: [rank20]:[W929 17:53:50.919965844 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:53:50.919970801 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:53:50.920289951 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:53:50.920316415 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  8:   self._overflow_buf = torch.cuda.IntTensor([0])
 17: [rank17]:[W929 17:53:50.921063588 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 14:   self._overflow_buf = torch.cuda.IntTensor([0])
  8: [rank8]:[W929 17:53:50.976152092 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:53:50.976495527 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 17:53:50.976869994 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:53:50.977195930 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632430616, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 917}}
  0: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  0:   self._overflow_buf = torch.cuda.IntTensor([0])
  0: [rank0]:[W929 17:53:50.922359218 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 17:53:50.923180839 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 22:   self._overflow_buf = torch.cuda.IntTensor([0])
 22: [rank22]:[W929 17:53:50.003373682 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:53:50.004095524 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 18:   self._overflow_buf = torch.cuda.IntTensor([0])
 18: [rank18]:[W929 17:53:50.006429950 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:53:50.007094055 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 23:   self._overflow_buf = torch.cuda.IntTensor([0])
 23: [rank23]:[W929 17:53:50.012606977 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:53:50.013353891 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: NCCL version 2.21.5+cuda12.4
 16: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 16:   self._overflow_buf = torch.cuda.IntTensor([0])
 16: [rank16]:[W929 17:53:50.020605005 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 17:53:50.021283539 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 17:54:07.309216103 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:54:07.904137822 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:54:07.973734366 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:54:07.904304901 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:54:07.918800709 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:54:07.904331445 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:54:07.376327720 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 17:54:07.904374277 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:54:07.904482642 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 17:54:07.904584875 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:54:07.904527321 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:54:07.904517782 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:54:07.309452138 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:54:07.309362572 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:54:07.309414054 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 17:54:07.309503409 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:54:07.973935468 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 17:54:07.309620257 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:54:07.973957389 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:54:07.309654195 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:54:07.973888879 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 17:54:07.309667243 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:54:07.523721199 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:54:07.973997078 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 17:54:07.660425212 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 17:54:07.974149705 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 17:54:07.658540578 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:54:07.974178875 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:54:07.974226836 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:54:07.523838939 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 17:54:07.523838779 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:54:07.523878890 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:54:07.524043559 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:54:07.996905031 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:54:07.442797012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:54:07.977202662 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:54:07.376363718 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 17:54:07.490935929 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:54:07.918892164 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:54:07.376544591 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:54:07.918978350 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:54:07.376494979 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:54:07.918956095 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:54:07.442797101 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:54:07.376590286 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:54:07.919022809 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:54:07.442950814 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:54:07.376585348 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:54:07.919205098 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:54:07.443019810 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 17:54:07.376695769 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 17:54:07.919272393 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:54:07.442962289 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 17:54:07.376717846 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:54:07.919279498 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:54:07.443022459 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:54:07.443149975 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:54:07.463381371 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 17:54:07.108226287 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:54:07.660518571 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:54:07.443244172 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 17:54:07.202493192 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:54:07.660520594 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:54:07.658621332 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:54:07.660652523 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:54:07.658654642 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:54:07.660625242 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: [rank100]:[W929 17:54:07.658758603 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:54:07.660619976 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:54:07.658849486 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:54:07.660737344 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:54:07.658868782 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 17:54:07.660738166 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:54:07.658868280 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:54:07.658909143 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 17:54:07.524146731 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:54:07.524090052 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 17:54:07.524297190 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:54:07.202530291 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:54:07.108329807 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: [rank123]:[W929 17:54:07.202733007 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:54:07.108369255 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:54:07.202758854 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:54:07.108402756 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:54:07.202829319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: NCCL version 2.21.5+cuda12.4
 67: [rank67]:[W929 17:54:07.463476847 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:54:07.108579039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:54:07.202865724 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:54:07.463537121 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:54:07.108500941 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:54:07.202937247 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: [rank64]:[W929 17:54:07.463705175 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:54:07.108568341 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:54:07.202905151 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: NCCL version 2.21.5+cuda12.4
 66: [rank66]:[W929 17:54:07.463595102 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:54:07.463596806 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:54:07.463699564 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 17:54:07.463799157 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 17:54:07.108755440 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:54:07.996953129 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:54:07.745091239 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:54:07.490903748 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:54:07.997067788 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:54:07.490994166 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:54:07.997133012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:54:07.977252870 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: NCCL version 2.21.5+cuda12.4
 52: [rank52]:[W929 17:54:07.490921948 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:54:07.997131731 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: NCCL version 2.21.5+cuda12.4
 43: [rank43]:[W929 17:54:07.977357637 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:54:07.491062929 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: NCCL version 2.21.5+cuda12.4
 62: [rank62]:[W929 17:54:07.997195444 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:54:07.977348671 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:54:07.491211148 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:54:07.997372413 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:54:07.977359384 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:54:07.491246668 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:54:07.997372733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:54:07.977438463 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:54:07.491320519 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: NCCL version 2.21.5+cuda12.4
 47: [rank47]:[W929 17:54:07.977557435 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 17:54:07.977659793 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:54:07.745096139 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: [rank112]:[W929 17:54:07.745353610 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 17:54:07.745399211 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:54:07.745201343 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:54:07.745203574 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 17:54:07.745403024 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:54:07.745588061 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: NCCL version 2.21.5+cuda12.4
 72: NCCL version 2.21.5+cuda12.4
  8: NCCL version 2.21.5+cuda12.4
 16: NCCL version 2.21.5+cuda12.4
 88: NCCL version 2.21.5+cuda12.4
 80: NCCL version 2.21.5+cuda12.4
 56: NCCL version 2.21.5+cuda12.4
 40: NCCL version 2.21.5+cuda12.4
120: NCCL version 2.21.5+cuda12.4
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
109: Torch distributed is available.
109: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454845, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 956}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454846, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 957}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454846, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.6, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 959}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454846, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.7, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 960}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454846, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 961}}
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 40: Torch distributed is available.
 40: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
112: Torch distributed is available.
112: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454947, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454947, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632454947, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": -200000.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 50: Torch distributed is available.
 50: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 56: Torch distributed is available.
 56: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
  9: Torch distributed is available.
  9: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
 48: Enabling make_graphed_callables for encoder!!
 52: Enabling make_graphed_callables for encoder!!
 50: Enabling make_graphed_callables for encoder!!
 54: Enabling make_graphed_callables for encoder!!
 53: Enabling make_graphed_callables for encoder!!
 55: Enabling make_graphed_callables for encoder!!
 49: Enabling make_graphed_callables for encoder!!
 51: Enabling make_graphed_callables for encoder!!
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: Torch distributed is available.
 56: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
 40: Torch distributed is available.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 40: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
112: Torch distributed is available.
112: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
  9: Torch distributed is available.
  9: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
109: Torch distributed is available.
109: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
 81: Enabling make_graphed_callables for encoder!!
 84: Enabling make_graphed_callables for encoder!!
 82: Enabling make_graphed_callables for encoder!!
 87: Enabling make_graphed_callables for encoder!!
 86: Enabling make_graphed_callables for encoder!!
 83: Enabling make_graphed_callables for encoder!!
 85: Enabling make_graphed_callables for encoder!!
 80: Enabling make_graphed_callables for encoder!!
103: Enabling make_graphed_callables for encoder!!
 58: Enabling make_graphed_callables for encoder!!
 60: Enabling make_graphed_callables for encoder!!
 96: Enabling make_graphed_callables for encoder!!
100: Enabling make_graphed_callables for encoder!!
101: Enabling make_graphed_callables for encoder!!
 97: Enabling make_graphed_callables for encoder!!
 99: Enabling make_graphed_callables for encoder!!
102: Enabling make_graphed_callables for encoder!!
 98: Enabling make_graphed_callables for encoder!!
 57: Enabling make_graphed_callables for encoder!!
 62: Enabling make_graphed_callables for encoder!!
 61: Enabling make_graphed_callables for encoder!!
 91: Enabling make_graphed_callables for encoder!!
 92: Enabling make_graphed_callables for encoder!!
 94: Enabling make_graphed_callables for encoder!!
 95: Enabling make_graphed_callables for encoder!!
 59: Enabling make_graphed_callables for encoder!!
 63: Enabling make_graphed_callables for encoder!!
 56: Enabling make_graphed_callables for encoder!!
 89: Enabling make_graphed_callables for encoder!!
 93: Enabling make_graphed_callables for encoder!!
 90: Enabling make_graphed_callables for encoder!!
 88: Enabling make_graphed_callables for encoder!!
 79: Enabling make_graphed_callables for encoder!!
 73: Enabling make_graphed_callables for encoder!!
 76: Enabling make_graphed_callables for encoder!!
 74: Enabling make_graphed_callables for encoder!!
 75: Enabling make_graphed_callables for encoder!!
 68: Enabling make_graphed_callables for encoder!!
 69: Enabling make_graphed_callables for encoder!!
 65: Enabling make_graphed_callables for encoder!!
 67: Enabling make_graphed_callables for encoder!!
 71: Enabling make_graphed_callables for encoder!!
 72: Enabling make_graphed_callables for encoder!!
 77: Enabling make_graphed_callables for encoder!!
 78: Enabling make_graphed_callables for encoder!!
 64: Enabling make_graphed_callables for encoder!!
 70: Enabling make_graphed_callables for encoder!!
 66: Enabling make_graphed_callables for encoder!!
 42: Enabling make_graphed_callables for encoder!!
 46: Enabling make_graphed_callables for encoder!!
 45: Enabling make_graphed_callables for encoder!!
 44: Enabling make_graphed_callables for encoder!!
 40: Enabling make_graphed_callables for encoder!!
 43: Enabling make_graphed_callables for encoder!!
 47: Enabling make_graphed_callables for encoder!!
113: Enabling make_graphed_callables for encoder!!
116: Enabling make_graphed_callables for encoder!!
 41: Enabling make_graphed_callables for encoder!!
112: Enabling make_graphed_callables for encoder!!
121: Enabling make_graphed_callables for encoder!!
127: Enabling make_graphed_callables for encoder!!
125: Enabling make_graphed_callables for encoder!!
122: Enabling make_graphed_callables for encoder!!
117: Enabling make_graphed_callables for encoder!!
115: Enabling make_graphed_callables for encoder!!
119: Enabling make_graphed_callables for encoder!!
  7: Enabling make_graphed_callables for encoder!!
118: Enabling make_graphed_callables for encoder!!
  1: Enabling make_graphed_callables for encoder!!
  8: Enabling make_graphed_callables for encoder!!
114: Enabling make_graphed_callables for encoder!!
 12: Enabling make_graphed_callables for encoder!!
 13: Enabling make_graphed_callables for encoder!!
 20: Enabling make_graphed_callables for encoder!!
 17: Enabling make_graphed_callables for encoder!!
 21: Enabling make_graphed_callables for encoder!!
 19: Enabling make_graphed_callables for encoder!!
  9: Enabling make_graphed_callables for encoder!!
120: Enabling make_graphed_callables for encoder!!
123: Enabling make_graphed_callables for encoder!!
124: Enabling make_graphed_callables for encoder!!
126: Enabling make_graphed_callables for encoder!!
  2: Enabling make_graphed_callables for encoder!!
  3: Enabling make_graphed_callables for encoder!!
  4: Enabling make_graphed_callables for encoder!!
  6: Enabling make_graphed_callables for encoder!!
  0: Enabling make_graphed_callables for encoder!!
 35: Enabling make_graphed_callables for encoder!!
 36: Enabling make_graphed_callables for encoder!!
 10: Enabling make_graphed_callables for encoder!!
 37: Enabling make_graphed_callables for encoder!!
 39: Enabling make_graphed_callables for encoder!!
 11: Enabling make_graphed_callables for encoder!!
  5: Enabling make_graphed_callables for encoder!!
109: Enabling make_graphed_callables for encoder!!
104: Enabling make_graphed_callables for encoder!!
 15: Enabling make_graphed_callables for encoder!!
 14: Enabling make_graphed_callables for encoder!!
106: Enabling make_graphed_callables for encoder!!
105: Enabling make_graphed_callables for encoder!!
110: Enabling make_graphed_callables for encoder!!
 18: Enabling make_graphed_callables for encoder!!
 22: Enabling make_graphed_callables for encoder!!
 23: Enabling make_graphed_callables for encoder!!
 24: Enabling make_graphed_callables for encoder!!
 16: Enabling make_graphed_callables for encoder!!
 30: Enabling make_graphed_callables for encoder!!
 34: Enabling make_graphed_callables for encoder!!
 38: Enabling make_graphed_callables for encoder!!
108: Enabling make_graphed_callables for encoder!!
107: Enabling make_graphed_callables for encoder!!
111: Enabling make_graphed_callables for encoder!!
 25: Enabling make_graphed_callables for encoder!!
 27: Enabling make_graphed_callables for encoder!!
 26: Enabling make_graphed_callables for encoder!!
 29: Enabling make_graphed_callables for encoder!!
 32: Enabling make_graphed_callables for encoder!!
 31: Enabling make_graphed_callables for encoder!!
 28: Enabling make_graphed_callables for encoder!!
 33: Enabling make_graphed_callables for encoder!!
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: INFO:root:running build_ext
 50: INFO:root:running build_ext
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: INFO:root:running build_ext
 49: INFO:root:running build_ext
 55: INFO:root:running build_ext
 52: INFO:root:running build_ext
 48: INFO:root:running build_ext
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: INFO:root:running build_ext
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: INFO:root:running build_ext
 87: INFO:root:running build_ext
 62: INFO:root:running build_ext
 58: INFO:root:running build_ext
 59: INFO:root:running build_ext
 57: INFO:root:running build_ext
 89: INFO:root:running build_ext
 91: INFO:root:running build_ext
 94: INFO:root:running build_ext
 90: INFO:root:running build_ext
 93: INFO:root:running build_ext
 56: INFO:root:running build_ext
 81: INFO:root:running build_ext
 80: INFO:root:running build_ext
 82: INFO:root:running build_ext
 97: INFO:root:running build_ext
103: INFO:root:running build_ext
102: INFO:root:running build_ext
 86: INFO:root:running build_ext
 85: INFO:root:running build_ext
 84: INFO:root:running build_ext
100: INFO:root:running build_ext
 67: INFO:root:running build_ext
 61: INFO:root:running build_ext
 60: INFO:root:running build_ext
 63: INFO:root:running build_ext
 70: INFO:root:running build_ext
 71: INFO:root:running build_ext
 77: INFO:root:running build_ext
 78: INFO:root:running build_ext
 79: INFO:root:running build_ext
 72: INFO:root:running build_ext
 75: INFO:root:running build_ext
 74: INFO:root:running build_ext
 88: INFO:root:running build_ext
101: INFO:root:running build_ext
 92: INFO:root:running build_ext
 95: INFO:root:running build_ext
 96: INFO:root:running build_ext
 98: INFO:root:running build_ext
 99: INFO:root:running build_ext
 69: INFO:root:running build_ext
 73: INFO:root:running build_ext
 64: INFO:root:running build_ext
 65: INFO:root:running build_ext
 76: INFO:root:running build_ext
 68: INFO:root:running build_ext
 41: INFO:root:running build_ext
 66: INFO:root:running build_ext
 45: INFO:root:running build_ext
 47: INFO:root:running build_ext
122: INFO:root:running build_ext
127: INFO:root:running build_ext
125: INFO:root:running build_ext
123: INFO:root:running build_ext
114: INFO:root:running build_ext
121: INFO:root:running build_ext
 44: INFO:root:running build_ext
 46: INFO:root:running build_ext
 43: INFO:root:running build_ext
 42: INFO:root:running build_ext
 40: INFO:root:running build_ext
 35: INFO:root:running build_ext
 36: INFO:root:running build_ext
 39: INFO:root:running build_ext
120: INFO:root:running build_ext
112: INFO:root:running build_ext
113: INFO:root:running build_ext
124: INFO:root:running build_ext
126: INFO:root:running build_ext
116: INFO:root:running build_ext
118: INFO:root:running build_ext
117: INFO:root:running build_ext
119: INFO:root:running build_ext
115: INFO:root:running build_ext
 37: INFO:root:running build_ext
 38: INFO:root:running build_ext
 33: INFO:root:running build_ext
 32: INFO:root:running build_ext
 34: INFO:root:running build_ext
104: INFO:root:running build_ext
109: INFO:root:running build_ext
110: INFO:root:running build_ext
108: INFO:root:running build_ext
111: INFO:root:running build_ext
105: INFO:root:running build_ext
107: INFO:root:running build_ext
 12: INFO:root:running build_ext
106: INFO:root:running build_ext
 13: INFO:root:running build_ext
 14: INFO:root:running build_ext
 15: INFO:root:running build_ext
  9: INFO:root:running build_ext
 10: INFO:root:running build_ext
 11: INFO:root:running build_ext
  8: INFO:root:running build_ext
  3: INFO:root:running build_ext
  5: INFO:root:running build_ext
  1: INFO:root:running build_ext
  0: INFO:root:running build_ext
 21: INFO:root:running build_ext
 22: INFO:root:running build_ext
 18: INFO:root:running build_ext
 17: INFO:root:running build_ext
 16: INFO:root:running build_ext
 29: INFO:root:running build_ext
 31: INFO:root:running build_ext
 25: INFO:root:running build_ext
 27: INFO:root:running build_ext
 24: INFO:root:running build_ext
  2: INFO:root:running build_ext
 19: INFO:root:running build_ext
 26: INFO:root:running build_ext
  4: INFO:root:running build_ext
  6: INFO:root:running build_ext
  7: INFO:root:running build_ext
 20: INFO:root:running build_ext
 30: INFO:root:running build_ext
 28: INFO:root:running build_ext
 23: INFO:root:running build_ext
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632490845, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632490845, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632490890, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1639, "epoch_num": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632490891, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1641, "first_epoch_num": 1, "epoch_count": 1}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=2346, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
  0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_p
  0: g=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0), resume_step=0)
  0: epoch: 1
  0: :::MLLOG {"namespace": "", "time_ms": 1727632490894, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_02452", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1676}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632494728, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 40814.701806988065}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 156497}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632494729, "event_type": "INTERVAL_START", "key": "eval_start", "value": 156497, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 156497}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632495549, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 156497, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 156497}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632495550, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3616308867931366, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 156497}}
  0: {'global_steps': 17, 'eval_loss': 4.238453388214111, 'eval_mlm_accuracy': 0.3616308867931366}
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632496833, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 70206.22559705364}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 304268}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632496833, "event_type": "INTERVAL_START", "key": "eval_start", "value": 304268, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 304268}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632496924, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 304268, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 304268}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632496924, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3652060329914093, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 304268}}
  0: {'global_steps': 33, 'eval_loss': 4.175991058349609, 'eval_mlm_accuracy': 0.3652060329914093}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632498190, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108296.62309798028}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 451287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632498191, "event_type": "INTERVAL_START", "key": "eval_start", "value": 451287, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 451287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632498285, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 451287, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 451287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632498286, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3697969615459442, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 451287}}
  0: {'global_steps': 49, 'eval_loss': 4.169295310974121, 'eval_mlm_accuracy': 0.3697969615459442}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632499631, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108863.68376662195}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 608153}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632499632, "event_type": "INTERVAL_START", "key": "eval_start", "value": 608153, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 608153}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632499726, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 608153, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 608153}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632499726, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.371618390083313, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 608153}}
  0: {'global_steps': 66, 'eval_loss': 4.134327411651611, 'eval_mlm_accuracy': 0.371618390083313}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632500996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107534.36108493013}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 754880}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632500996, "event_type": "INTERVAL_START", "key": "eval_start", "value": 754880, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 754880}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632501089, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 754880, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 754880}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632501089, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.381764680147171, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 754880}}
  0: {'global_steps': 82, 'eval_loss': 4.037147045135498, 'eval_mlm_accuracy': 0.381764680147171}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632502369, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107535.24253669944}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 902586}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632502370, "event_type": "INTERVAL_START", "key": "eval_start", "value": 902586, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 902586}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632502459, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 902586, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 902586}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632502460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3855780065059662, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 902586}}
  0: {'global_steps': 98, 'eval_loss': 4.006439685821533, 'eval_mlm_accuracy': 0.3855780065059662}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632503730, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108509.47568828391}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1050222}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632503730, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1050222, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1050222}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632503831, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1050222, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1050222}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632503831, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.392779678106308, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1050222}}
  0: {'global_steps': 114, 'eval_loss': 3.9294116497039795, 'eval_mlm_accuracy': 0.392779678106308}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632505177, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107779.01604937011}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1206210}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632505177, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1206210, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1206210}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632505275, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1206210, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1206210}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632505276, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.40358680486679077, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1206210}}
  0: {'global_steps': 131, 'eval_loss': 3.8284926414489746, 'eval_mlm_accuracy': 0.40358680486679077}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632506543, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107980.01125573175}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1353718}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632506544, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1353718, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1353718}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632506634, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1353718, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1353718}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632506634, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.409655898809433, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1353718}}
  0: {'global_steps': 147, 'eval_loss': 3.743891716003418, 'eval_mlm_accuracy': 0.409655898809433}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632507903, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108523.86785204944}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1501235}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632507903, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1501235, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1501235}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632507994, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1501235, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1501235}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632507994, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.43272969126701355, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1501235}}
  0: {'global_steps': 163, 'eval_loss': 3.5513882637023926, 'eval_mlm_accuracy': 0.43272969126701355}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632509341, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108987.8801816354}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1658000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632509341, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1658000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1658000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632509433, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1658000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1658000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632509433, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4575665295124054, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1658000}}
  0: {'global_steps': 180, 'eval_loss': 3.3495750427246094, 'eval_mlm_accuracy': 0.4575665295124054}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632510704, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107934.674400867}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1805171}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632510705, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1805171, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1805171}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632510805, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1805171, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1805171}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632510805, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4932665526866913, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1805171}}
  0: {'global_steps': 196, 'eval_loss': 3.0558066368103027, 'eval_mlm_accuracy': 0.4932665526866913}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632512073, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107847.72639652716}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1952789}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632512073, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1952789, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1952789}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632512163, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1952789, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1952789}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632512164, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5203544497489929, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1952789}}
  0: {'global_steps': 212, 'eval_loss': 2.8187222480773926, 'eval_mlm_accuracy': 0.5203544497489929}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632513433, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107740.48357547258}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2099333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632513434, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2099333, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2099333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632513527, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2099333, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2099333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632513527, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5580931305885315, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2099333}}
  0: {'global_steps': 228, 'eval_loss': 2.491250991821289, 'eval_mlm_accuracy': 0.5580931305885315}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632514876, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108266.24950445056}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2255561}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632514877, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2255561, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2255561}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632514968, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2255561, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2255561}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632514968, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.625070333480835, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2255561}}
  0: {'global_steps': 245, 'eval_loss': 1.9777166843414307, 'eval_mlm_accuracy': 0.625070333480835}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632516234, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108602.91865688478}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2403036}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632516235, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2403036, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2403036}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632516326, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2403036, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2403036}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632516326, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6697373986244202, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2403036}}
  0: {'global_steps': 261, 'eval_loss': 1.6352348327636719, 'eval_mlm_accuracy': 0.6697373986244202}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632517593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108517.24920813245}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2550472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632517593, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2550472, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2550472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632517683, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2550472, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2550472}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632517684, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6934697031974792, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2550472}}
  0: {'global_steps': 277, 'eval_loss': 1.4699536561965942, 'eval_mlm_accuracy': 0.6934697031974792}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632518953, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108483.49845059654}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2697973}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632518953, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2697973, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2697973}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632519044, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2697973, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2697973}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632519044, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7030812501907349, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2697973}}
  0: {'global_steps': 293, 'eval_loss': 1.4166339635849, 'eval_mlm_accuracy': 0.7030812501907349}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632520391, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108969.53697274248}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2854690}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632520391, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2854690, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2854690}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632520486, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2854690, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2854690}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632520486, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7066867351531982, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2854690}}
  0: {'global_steps': 310, 'eval_loss': 1.3879448175430298, 'eval_mlm_accuracy': 0.7066867351531982}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632521755, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108232.0193807061}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3002311}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632521755, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3002311, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3002311}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632521844, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3002311, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3002311}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632521845, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7096570730209351, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3002311}}
  0: {'global_steps': 326, 'eval_loss': 1.3810005187988281, 'eval_mlm_accuracy': 0.7096570730209351}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632523114, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108516.63294380798}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3149857}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632523115, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3149857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3149857}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632523205, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3149857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3149857}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632523205, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7143273949623108, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3149857}}
  0: {'global_steps': 342, 'eval_loss': 1.3426610231399536, 'eval_mlm_accuracy': 0.7143273949623108}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632524553, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 109132.89345952534}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3306824}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632524553, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3306824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3306824}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632524646, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3306824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3306824}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632524646, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7154856324195862, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3306824}}
  0: {'global_steps': 359, 'eval_loss': 1.3328492641448975, 'eval_mlm_accuracy': 0.7154856324195862}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632525915, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108171.87185654946}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3454161}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632525915, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3454161, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3454161}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632526013, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3454161, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3454161}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632526014, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7165668606758118, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3454161}}
  0: {'global_steps': 375, 'eval_loss': 1.3315519094467163, 'eval_mlm_accuracy': 0.7165668606758118}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632527282, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107822.11972786412}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3601633}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632527283, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3601633, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3601633}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632527372, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3601633, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3601633}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632527373, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7171366214752197, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3601633}}
  0: {'global_steps': 391, 'eval_loss': 1.3256287574768066, 'eval_mlm_accuracy': 0.7171366214752197}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632528643, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108543.32872750524}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3749356}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632528644, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3749356, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3749356}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632528742, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3749356, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3749356}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632528742, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7177017331123352, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3749356}}
  0: {'global_steps': 407, 'eval_loss': 1.3227481842041016, 'eval_mlm_accuracy': 0.7177017331123352}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632530090, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108517.61039334291}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3906354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632530090, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3906354, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3906354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632530181, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3906354, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3906354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632530181, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7177087068557739, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3906354}}
  0: {'global_steps': 424, 'eval_loss': 1.320529580116272, 'eval_mlm_accuracy': 0.7177087068557739}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632531450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108415.60186391276}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4053735}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632531450, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4053735, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4053735}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632531540, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4053735, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4053735}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632531540, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7180800437927246, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4053735}}
  0: {'global_steps': 440, 'eval_loss': 1.3172132968902588, 'eval_mlm_accuracy': 0.7180800437927246}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632532808, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108445.84651446779}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4201100}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632532809, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4201100, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4201100}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632532899, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4201100, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4201100}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632532899, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7186077833175659, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4201100}}
  0: {'global_steps': 456, 'eval_loss': 1.3135393857955933, 'eval_mlm_accuracy': 0.7186077833175659}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632534248, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108774.6225197424}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4357640}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632534248, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4357640, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4357640}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632534338, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4357640, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4357640}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632534338, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7192429304122925, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4357640}}
  0: {'global_steps': 473, 'eval_loss': 1.3105865716934204, 'eval_mlm_accuracy': 0.7192429304122925}
  0: Training runs 1.388703501224518 mins sustained_training_time 0
  0: (1, 740.0) {'final_loss': 0.0}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556662, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1985, "first_epoch_num": 1}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556663, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1988, "epoch_num": 4357640}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556663, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4357640, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1990}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556663, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1993}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556663, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1996, "status": "aborted"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632556663, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 66207.84392492133, "epoch_num": 4357640}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2035, "step": [2, 740]}}
  0: {'e2e_time': 132.60532069206238, 'training_sequences_per_second': 81848.95431099221, 'final_loss': 0.0, 'raw_train_time': 83.32226181030273}
 40: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 40: RESULT,bert,11193,150,root,2024-09-29 05:53:28 PM
 96: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 96: RESULT,bert,3343,150,root,2024-09-29 05:53:28 PM
112: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
112: RESULT,bert,24722,150,root,2024-09-29 05:53:28 PM
 64: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 64: RESULT,bert,27312,150,root,2024-09-29 05:53:28 PM
 80: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 80: RESULT,bert,28610,150,root,2024-09-29 05:53:28 PM
 56: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 56: RESULT,bert,20707,150,root,2024-09-29 05:53:28 PM
 88: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 88: RESULT,bert,12203,150,root,2024-09-29 05:53:28 PM
121: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
121: RESULT,bert,29596,150,root,2024-09-29 05:53:28 PM
104: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
104: RESULT,bert,27182,150,root,2024-09-29 05:53:28 PM
 16: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
 16: RESULT,bert,24363,150,root,2024-09-29 05:53:28 PM
  1: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
  1: RESULT,bert,789,150,root,2024-09-29 05:53:28 PM
  9: ENDING TIMING RUN AT 2024-09-29 05:55:58 PM
  9: RESULT,bert,8767,150,root,2024-09-29 05:53:28 PM
124: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
124: RESULT,bert,7650,151,root,2024-09-29 05:53:28 PM
 24: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 24: RESULT,bert,2466,151,root,2024-09-29 05:53:28 PM
 33: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 33: RESULT,bert,11564,151,root,2024-09-29 05:53:28 PM
109: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
109: RESULT,bert,8575,151,root,2024-09-29 05:53:28 PM
 75: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 75: RESULT,bert,26947,151,root,2024-09-29 05:53:28 PM
  3: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
  3: RESULT,bert,4792,151,root,2024-09-29 05:53:28 PM
 37: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 37: RESULT,bert,9671,151,root,2024-09-29 05:53:28 PM
122: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
122: RESULT,bert,16361,151,root,2024-09-29 05:53:28 PM
 49: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 49: RESULT,bert,18453,151,root,2024-09-29 05:53:28 PM
 94: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 94: RESULT,bert,4558,151,root,2024-09-29 05:53:28 PM
  6: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
  6: RESULT,bert,3824,151,root,2024-09-29 05:53:28 PM
 52: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 52: RESULT,bert,16344,151,root,2024-09-29 05:53:28 PM
119: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
119: RESULT,bert,9793,151,root,2024-09-29 05:53:28 PM
 83: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 83: RESULT,bert,5007,151,root,2024-09-29 05:53:28 PM
 78: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 78: RESULT,bert,17154,151,root,2024-09-29 05:53:28 PM
 47: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 47: RESULT,bert,3370,151,root,2024-09-29 05:53:28 PM
 92: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 92: RESULT,bert,1701,151,root,2024-09-29 05:53:28 PM
 35: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 35: RESULT,bert,14442,151,root,2024-09-29 05:53:28 PM
 50: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 50: RESULT,bert,7271,151,root,2024-09-29 05:53:28 PM
120: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
120: RESULT,bert,21793,151,root,2024-09-29 05:53:28 PM
 12: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 12: RESULT,bert,9174,151,root,2024-09-29 05:53:28 PM
107: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
107: RESULT,bert,15849,151,root,2024-09-29 05:53:28 PM
 30: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 30: RESULT,bert,6139,151,root,2024-09-29 05:53:28 PM
 99: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 99: RESULT,bert,11823,151,root,2024-09-29 05:53:28 PM
  0: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
  0: RESULT,bert,2346,151,root,2024-09-29 05:53:28 PM
 68: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 68: RESULT,bert,26704,151,root,2024-09-29 05:53:28 PM
 61: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 61: RESULT,bert,1361,151,root,2024-09-29 05:53:28 PM
 76: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 76: RESULT,bert,32526,151,root,2024-09-29 05:53:28 PM
 48: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 48: RESULT,bert,15776,151,root,2024-09-29 05:53:28 PM
114: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
114: RESULT,bert,13388,151,root,2024-09-29 05:53:28 PM
106: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
106: RESULT,bert,23063,151,root,2024-09-29 05:53:28 PM
  8: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
  8: RESULT,bert,27922,151,root,2024-09-29 05:53:28 PM
 32: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 32: RESULT,bert,22777,151,root,2024-09-29 05:53:28 PM
 74: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 74: RESULT,bert,28867,151,root,2024-09-29 05:53:28 PM
 29: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 29: RESULT,bert,31419,151,root,2024-09-29 05:53:28 PM
 54: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 54: RESULT,bert,5854,151,root,2024-09-29 05:53:28 PM
 19: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 19: RESULT,bert,23948,151,root,2024-09-29 05:53:28 PM
 57: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 57: RESULT,bert,25223,151,root,2024-09-29 05:53:28 PM
 89: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 89: RESULT,bert,15976,151,root,2024-09-29 05:53:28 PM
 81: ENDING TIMING RUN AT 2024-09-29 05:55:59 PM
 81: RESULT,bert,3537,151,root,2024-09-29 05:53:28 PM
 38: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 38: RESULT,bert,10835,152,root,2024-09-29 05:53:28 PM
 17: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 17: RESULT,bert,11485,152,root,2024-09-29 05:53:28 PM
 72: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 72: RESULT,bert,732,152,root,2024-09-29 05:53:28 PM
101: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
101: RESULT,bert,31934,152,root,2024-09-29 05:53:28 PM
 66: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 66: RESULT,bert,11012,152,root,2024-09-29 05:53:28 PM
 59: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 59: RESULT,bert,12944,152,root,2024-09-29 05:53:28 PM
 82: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 82: RESULT,bert,10379,152,root,2024-09-29 05:53:28 PM
 91: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 91: RESULT,bert,5817,152,root,2024-09-29 05:53:28 PM
125: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
125: RESULT,bert,30923,152,root,2024-09-29 05:53:28 PM
 18: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 18: RESULT,bert,10456,152,root,2024-09-29 05:53:28 PM
 41: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 41: RESULT,bert,23523,152,root,2024-09-29 05:53:28 PM
100: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
100: RESULT,bert,28926,152,root,2024-09-29 05:53:28 PM
 58: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 58: RESULT,bert,8923,152,root,2024-09-29 05:53:28 PM
 67: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 67: RESULT,bert,11300,152,root,2024-09-29 05:53:28 PM
113: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
113: RESULT,bert,1246,152,root,2024-09-29 05:53:28 PM
 63: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 63: RESULT,bert,19757,152,root,2024-09-29 05:53:28 PM
127: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
127: RESULT,bert,31217,152,root,2024-09-29 05:53:28 PM
 90: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 90: RESULT,bert,19871,152,root,2024-09-29 05:53:28 PM
 42: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 42: RESULT,bert,17273,152,root,2024-09-29 05:53:28 PM
103: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
103: RESULT,bert,15974,152,root,2024-09-29 05:53:28 PM
115: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
115: RESULT,bert,9402,152,root,2024-09-29 05:53:28 PM
 28: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 28: RESULT,bert,8565,152,root,2024-09-29 05:53:28 PM
  7: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
  7: RESULT,bert,31332,152,root,2024-09-29 05:53:28 PM
 43: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 43: RESULT,bert,11987,152,root,2024-09-29 05:53:28 PM
 85: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 85: RESULT,bert,5174,152,root,2024-09-29 05:53:28 PM
 60: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 60: RESULT,bert,16406,152,root,2024-09-29 05:53:28 PM
126: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
126: RESULT,bert,2642,152,root,2024-09-29 05:53:28 PM
 97: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 97: RESULT,bert,12686,152,root,2024-09-29 05:53:28 PM
111: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
111: RESULT,bert,11535,152,root,2024-09-29 05:53:28 PM
 15: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 15: RESULT,bert,1324,152,root,2024-09-29 05:53:28 PM
 65: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 65: RESULT,bert,22280,152,root,2024-09-29 05:53:28 PM
  5: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
  5: RESULT,bert,10760,152,root,2024-09-29 05:53:28 PM
 73: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 73: RESULT,bert,8292,152,root,2024-09-29 05:53:28 PM
 51: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 51: RESULT,bert,3600,152,root,2024-09-29 05:53:28 PM
 45: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 45: RESULT,bert,29153,152,root,2024-09-29 05:53:28 PM
 21: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 21: RESULT,bert,5396,152,root,2024-09-29 05:53:28 PM
 39: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 39: RESULT,bert,22062,152,root,2024-09-29 05:53:28 PM
 13: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 13: RESULT,bert,6764,152,root,2024-09-29 05:53:28 PM
 31: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 31: RESULT,bert,17790,152,root,2024-09-29 05:53:28 PM
123: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
123: RESULT,bert,16047,152,root,2024-09-29 05:53:28 PM
 70: ENDING TIMING RUN AT 2024-09-29 05:56:00 PM
 70: RESULT,bert,32489,152,root,2024-09-29 05:53:28 PM
116: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
116: RESULT,bert,31584,153,root,2024-09-29 05:53:28 PM
 98: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 98: RESULT,bert,2319,153,root,2024-09-29 05:53:28 PM
 95: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 95: RESULT,bert,23121,153,root,2024-09-29 05:53:28 PM
108: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
108: RESULT,bert,28712,153,root,2024-09-29 05:53:28 PM
 53: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 53: RESULT,bert,21671,153,root,2024-09-29 05:53:28 PM
  2: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
  2: RESULT,bert,17607,153,root,2024-09-29 05:53:28 PM
 86: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 86: RESULT,bert,15073,153,root,2024-09-29 05:53:28 PM
 44: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 44: RESULT,bert,13011,153,root,2024-09-29 05:53:28 PM
 79: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 79: RESULT,bert,31434,153,root,2024-09-29 05:53:28 PM
 11: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 11: RESULT,bert,9270,153,root,2024-09-29 05:53:28 PM
 23: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 23: RESULT,bert,7251,153,root,2024-09-29 05:53:28 PM
118: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
118: RESULT,bert,25751,153,root,2024-09-29 05:53:28 PM
 34: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 34: RESULT,bert,24109,153,root,2024-09-29 05:53:28 PM
 71: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 71: RESULT,bert,9528,153,root,2024-09-29 05:53:28 PM
 62: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 62: RESULT,bert,5995,153,root,2024-09-29 05:53:28 PM
 25: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 25: RESULT,bert,21132,153,root,2024-09-29 05:53:28 PM
105: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
105: RESULT,bert,13914,153,root,2024-09-29 05:53:28 PM
 10: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 10: RESULT,bert,19710,153,root,2024-09-29 05:53:28 PM
 87: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 87: RESULT,bert,18234,153,root,2024-09-29 05:53:28 PM
  4: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
  4: RESULT,bert,5303,153,root,2024-09-29 05:53:28 PM
102: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
102: RESULT,bert,464,153,root,2024-09-29 05:53:28 PM
 55: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 55: RESULT,bert,13227,153,root,2024-09-29 05:53:28 PM
 93: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 93: RESULT,bert,23945,153,root,2024-09-29 05:53:28 PM
 20: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 20: RESULT,bert,19057,153,root,2024-09-29 05:53:28 PM
 46: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 46: RESULT,bert,30297,153,root,2024-09-29 05:53:28 PM
 77: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 77: RESULT,bert,4693,153,root,2024-09-29 05:53:28 PM
117: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
117: RESULT,bert,19188,153,root,2024-09-29 05:53:28 PM
 69: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 69: RESULT,bert,28281,153,root,2024-09-29 05:53:28 PM
 36: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 36: RESULT,bert,2326,153,root,2024-09-29 05:53:28 PM
 27: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 27: RESULT,bert,29770,153,root,2024-09-29 05:53:28 PM
110: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
110: RESULT,bert,31750,153,root,2024-09-29 05:53:28 PM
 84: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 84: RESULT,bert,26701,153,root,2024-09-29 05:53:28 PM
 14: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 14: RESULT,bert,31979,153,root,2024-09-29 05:53:28 PM
 22: ENDING TIMING RUN AT 2024-09-29 05:56:01 PM
 22: RESULT,bert,28719,153,root,2024-09-29 05:53:28 PM
 26: ENDING TIMING RUN AT 2024-09-29 05:56:02 PM
 26: RESULT,bert,16099,154,root,2024-09-29 05:53:28 PM
