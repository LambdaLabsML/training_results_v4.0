+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/localdisk/sd/stable_diffusions/stable_diffusions/images/sd+mlperf-nvidia+sd.sqsh 796 16 compute-hpc-node-[25,73,96,147,157,186,194,199,219,241,246,445,450,478,555,596] BM.GPU.H100.8 Cluster DGXH100_16x08x08'
:::DLPAL /mnt/localdisk/sd/stable_diffusions/stable_diffusions/images/sd+mlperf-nvidia+sd.sqsh 796 16 compute-hpc-node-[25,73,96,147,157,186,194,199,219,241,246,445,450,478,555,596] BM.GPU.H100.8 Cluster DGXH100_16x08x08
++ srun --ntasks=1 --container-name=stable_diffusion_796 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-1018-oracle","nvidia_kernel_driver":"535.161.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-1018-oracle","nvidia_kernel_driver":"535.161.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
Clearing cache on compute-hpc-node-246
Clearing cache on compute-hpc-node-219
Clearing cache on compute-hpc-node-147
Clearing cache on compute-hpc-node-478
Clearing cache on compute-hpc-node-25
Clearing cache on compute-hpc-node-555
Clearing cache on compute-hpc-node-186
Clearing cache on compute-hpc-node-596
Clearing cache on compute-hpc-node-157
Clearing cache on compute-hpc-node-445
Clearing cache on compute-hpc-node-199
Clearing cache on compute-hpc-node-96
Clearing cache on compute-hpc-node-73
Clearing cache on compute-hpc-node-194
Clearing cache on compute-hpc-node-241
Clearing cache on compute-hpc-node-450
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=16 --mpi=pmix --container-name=stable_diffusion_796 python -c '
from mlperf_logging import mllog
mllogger = mllog.get_mllogger()
mllogger.event(key=mllog.constants.CACHE_CLEAR, value=True)'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
:::MLLOG {"namespace": "", "time_ms": 1715318677515, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677537, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677547, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677548, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677574, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677592, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677592, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677599, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677600, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677598, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677612, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677618, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677619, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677620, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677647, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715318677671, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --container-name=stable_diffusion_796 --container-mounts=/nfs/scratch/sd/stable_diffusions/logs:/results,/mnt/localdisk/sd/stable_diffusions/stable_diffusions/datasets:/datasets,/mnt/localdisk/sd/stable_diffusions/stable_diffusions/checkpoints:/checkpoints,/nfs/scratch/sd/stable_diffusions/nemologs:/nemologs,/nfs/cluster:/nfs/cluster,/opt/openmpi-4.1.4/bin:/opt/openmpi-4.1.4/bin --container-workdir=/workspace/sd --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
 30: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 25: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 64: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 26: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 31: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 87: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 27: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 28: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 29: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 24: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 97: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
101: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 70: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 68: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 65: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 69: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 67: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 99: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 82: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 98: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 85: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
103: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 66: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 81: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
102: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 86: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
100: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 96: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 84: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 83: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 71: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 80: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 74: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  3: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 75: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 72: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 76: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 77: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 78: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
118: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 73: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 79: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  1: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  4: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  7: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  5: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
110: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  6: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
119: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
112: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
117: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  0: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  2: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
116: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
113: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
114: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
115: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
107: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
122: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 48: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
104: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
106: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
111: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
108: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
105: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
109: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 54: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
127: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 52: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
126: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 51: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 53: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
120: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 49: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
121: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
125: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
124: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 16: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 55: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 50: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
123: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 37: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 59: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 14: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 46: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 95: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 21: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 20: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 22: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 23: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 19: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 18: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 17: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 39: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 35: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 38: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 34: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 32: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 36: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 33: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 58: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 61: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 60: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 57: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 56: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 62: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 63: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  8: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 10: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 13: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 12: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
  9: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 11: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 15: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 47: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 41: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 43: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 45: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 44: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 40: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 42: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 90: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 89: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 93: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 88: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 91: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 94: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 92: STARTING TIMING RUN AT 2024-05-10 05:24:39 AM
 64: :::MLLOG {"namespace": "", "time_ms": 1715318681098, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 66: :::MLLOG {"namespace": "", "time_ms": 1715318681098, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 68: :::MLLOG {"namespace": "", "time_ms": 1715318681098, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 70: :::MLLOG {"namespace": "", "time_ms": 1715318681098, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 65: :::MLLOG {"namespace": "", "time_ms": 1715318681099, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 69: :::MLLOG {"namespace": "", "time_ms": 1715318681100, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 67: :::MLLOG {"namespace": "", "time_ms": 1715318681100, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 71: :::MLLOG {"namespace": "", "time_ms": 1715318681101, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 30: :::MLLOG {"namespace": "", "time_ms": 1715318681132, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 31: :::MLLOG {"namespace": "", "time_ms": 1715318681132, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 29: :::MLLOG {"namespace": "", "time_ms": 1715318681132, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 26: :::MLLOG {"namespace": "", "time_ms": 1715318681133, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 28: :::MLLOG {"namespace": "", "time_ms": 1715318681134, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 25: :::MLLOG {"namespace": "", "time_ms": 1715318681135, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 27: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 24: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
101: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
102: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 97: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
100: :::MLLOG {"namespace": "", "time_ms": 1715318681136, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 96: :::MLLOG {"namespace": "", "time_ms": 1715318681139, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 98: :::MLLOG {"namespace": "", "time_ms": 1715318681139, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 99: :::MLLOG {"namespace": "", "time_ms": 1715318681140, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
103: :::MLLOG {"namespace": "", "time_ms": 1715318681140, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 72: :::MLLOG {"namespace": "", "time_ms": 1715318681142, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 77: :::MLLOG {"namespace": "", "time_ms": 1715318681142, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 73: :::MLLOG {"namespace": "", "time_ms": 1715318681142, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 74: :::MLLOG {"namespace": "", "time_ms": 1715318681142, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 76: :::MLLOG {"namespace": "", "time_ms": 1715318681144, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 79: :::MLLOG {"namespace": "", "time_ms": 1715318681145, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 78: :::MLLOG {"namespace": "", "time_ms": 1715318681145, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 75: :::MLLOG {"namespace": "", "time_ms": 1715318681145, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
121: :::MLLOG {"namespace": "", "time_ms": 1715318681151, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
120: :::MLLOG {"namespace": "", "time_ms": 1715318681151, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
125: :::MLLOG {"namespace": "", "time_ms": 1715318681151, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 16: :::MLLOG {"namespace": "", "time_ms": 1715318681153, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 17: :::MLLOG {"namespace": "", "time_ms": 1715318681153, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 18: :::MLLOG {"namespace": "", "time_ms": 1715318681153, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
124: :::MLLOG {"namespace": "", "time_ms": 1715318681154, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
126: :::MLLOG {"namespace": "", "time_ms": 1715318681154, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
127: :::MLLOG {"namespace": "", "time_ms": 1715318681155, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 22: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 20: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
105: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
108: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
109: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
110: :::MLLOG {"namespace": "", "time_ms": 1715318681156, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 23: :::MLLOG {"namespace": "", "time_ms": 1715318681157, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
104: :::MLLOG {"namespace": "", "time_ms": 1715318681158, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
107: :::MLLOG {"namespace": "", "time_ms": 1715318681158, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 87: :::MLLOG {"namespace": "", "time_ms": 1715318681159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 82: :::MLLOG {"namespace": "", "time_ms": 1715318681159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 85: :::MLLOG {"namespace": "", "time_ms": 1715318681159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 80: :::MLLOG {"namespace": "", "time_ms": 1715318681159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
111: :::MLLOG {"namespace": "", "time_ms": 1715318681159, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
106: :::MLLOG {"namespace": "", "time_ms": 1715318681160, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 19: :::MLLOG {"namespace": "", "time_ms": 1715318681160, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 81: :::MLLOG {"namespace": "", "time_ms": 1715318681160, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 86: :::MLLOG {"namespace": "", "time_ms": 1715318681161, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 83: :::MLLOG {"namespace": "", "time_ms": 1715318681162, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 84: :::MLLOG {"namespace": "", "time_ms": 1715318681162, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
123: :::MLLOG {"namespace": "", "time_ms": 1715318681166, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
122: :::MLLOG {"namespace": "", "time_ms": 1715318681168, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  1: :::MLLOG {"namespace": "", "time_ms": 1715318681175, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  3: :::MLLOG {"namespace": "", "time_ms": 1715318681175, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318681175, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  5: :::MLLOG {"namespace": "", "time_ms": 1715318681175, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  6: :::MLLOG {"namespace": "", "time_ms": 1715318681177, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  2: :::MLLOG {"namespace": "", "time_ms": 1715318681177, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  4: :::MLLOG {"namespace": "", "time_ms": 1715318681178, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  7: :::MLLOG {"namespace": "", "time_ms": 1715318681178, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 21: :::MLLOG {"namespace": "", "time_ms": 1715318681180, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  8: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
  9: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 13: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 44: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 47: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 43: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 41: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 46: :::MLLOG {"namespace": "", "time_ms": 1715318681185, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 14: :::MLLOG {"namespace": "", "time_ms": 1715318681188, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 12: :::MLLOG {"namespace": "", "time_ms": 1715318681188, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 42: :::MLLOG {"namespace": "", "time_ms": 1715318681188, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 40: :::MLLOG {"namespace": "", "time_ms": 1715318681188, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 45: :::MLLOG {"namespace": "", "time_ms": 1715318681189, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 11: :::MLLOG {"namespace": "", "time_ms": 1715318681189, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 57: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 60: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 61: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 58: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
112: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
116: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
118: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
119: :::MLLOG {"namespace": "", "time_ms": 1715318681193, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 63: :::MLLOG {"namespace": "", "time_ms": 1715318681194, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 59: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 62: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 15: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
115: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
114: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
117: :::MLLOG {"namespace": "", "time_ms": 1715318681195, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 10: :::MLLOG {"namespace": "", "time_ms": 1715318681196, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 56: :::MLLOG {"namespace": "", "time_ms": 1715318681196, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
113: :::MLLOG {"namespace": "", "time_ms": 1715318681196, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 54: :::MLLOG {"namespace": "", "time_ms": 1715318681203, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 48: :::MLLOG {"namespace": "", "time_ms": 1715318681203, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 55: :::MLLOG {"namespace": "", "time_ms": 1715318681203, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 49: :::MLLOG {"namespace": "", "time_ms": 1715318681203, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 51: :::MLLOG {"namespace": "", "time_ms": 1715318681204, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 50: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 89: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 95: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 93: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 53: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 52: :::MLLOG {"namespace": "", "time_ms": 1715318681205, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 94: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 35: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 38: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 34: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 39: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 92: :::MLLOG {"namespace": "", "time_ms": 1715318681208, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 88: :::MLLOG {"namespace": "", "time_ms": 1715318681210, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 90: :::MLLOG {"namespace": "", "time_ms": 1715318681210, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 37: :::MLLOG {"namespace": "", "time_ms": 1715318681210, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 33: :::MLLOG {"namespace": "", "time_ms": 1715318681211, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 36: :::MLLOG {"namespace": "", "time_ms": 1715318681211, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 32: :::MLLOG {"namespace": "", "time_ms": 1715318681212, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 91: :::MLLOG {"namespace": "", "time_ms": 1715318681215, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 66: RANDOM_SEED=24612
 70: RANDOM_SEED=24612
 64: RANDOM_SEED=24612
 68: RANDOM_SEED=24612
 69: RANDOM_SEED=24612
 67: RANDOM_SEED=24612
 65: RANDOM_SEED=24612
 71: RANDOM_SEED=24612
 97: RANDOM_SEED=24612
100: RANDOM_SEED=24612
101: RANDOM_SEED=24612
102: RANDOM_SEED=24612
 26: RANDOM_SEED=24612
 29: RANDOM_SEED=24612
 30: RANDOM_SEED=24612
 31: RANDOM_SEED=24612
 74: RANDOM_SEED=24612
 77: RANDOM_SEED=24612
 72: RANDOM_SEED=24612
 73: RANDOM_SEED=24612
 28: RANDOM_SEED=24612
120: RANDOM_SEED=24612
 24: RANDOM_SEED=24612
121: RANDOM_SEED=24612
125: RANDOM_SEED=24612
 25: RANDOM_SEED=24612
123: RANDOM_SEED=24612
 96: RANDOM_SEED=24612
 16: RANDOM_SEED=24612
 18: RANDOM_SEED=24612
 19: RANDOM_SEED=24612
 17: RANDOM_SEED=24612
 98: RANDOM_SEED=24612
 99: RANDOM_SEED=24612
105: RANDOM_SEED=24612
108: RANDOM_SEED=24612
110: RANDOM_SEED=24612
109: RANDOM_SEED=24612
 76: RANDOM_SEED=24612
 78: RANDOM_SEED=24612
 79: RANDOM_SEED=24612
103: RANDOM_SEED=24612
126: RANDOM_SEED=24612
124: RANDOM_SEED=24612
 75: RANDOM_SEED=24612
 20: RANDOM_SEED=24612
 22: RANDOM_SEED=24612
 27: RANDOM_SEED=24612
 87: RANDOM_SEED=24612
 80: RANDOM_SEED=24612
 85: RANDOM_SEED=24612
 82: RANDOM_SEED=24612
 23: RANDOM_SEED=24612
127: RANDOM_SEED=24612
104: RANDOM_SEED=24612
107: RANDOM_SEED=24612
106: RANDOM_SEED=24612
 81: RANDOM_SEED=24612
 86: RANDOM_SEED=24612
 84: RANDOM_SEED=24612
111: RANDOM_SEED=24612
  1: RANDOM_SEED=24612
122: RANDOM_SEED=24612
  5: RANDOM_SEED=24612
  0: RANDOM_SEED=24612
  3: RANDOM_SEED=24612
  9: RANDOM_SEED=24612
 83: RANDOM_SEED=24612
  8: RANDOM_SEED=24612
 10: RANDOM_SEED=24612
 13: RANDOM_SEED=24612
 43: RANDOM_SEED=24612
 44: RANDOM_SEED=24612
 41: RANDOM_SEED=24612
 46: RANDOM_SEED=24612
 47: RANDOM_SEED=24612
  6: RANDOM_SEED=24612
  2: RANDOM_SEED=24612
  4: RANDOM_SEED=24612
 58: RANDOM_SEED=24612
 57: RANDOM_SEED=24612
 61: RANDOM_SEED=24612
 60: RANDOM_SEED=24612
 12: RANDOM_SEED=24612
 14: RANDOM_SEED=24612
112: RANDOM_SEED=24612
116: RANDOM_SEED=24612
118: RANDOM_SEED=24612
119: RANDOM_SEED=24612
 90: RANDOM_SEED=24612
 42: RANDOM_SEED=24612
 93: RANDOM_SEED=24612
 95: RANDOM_SEED=24612
 89: RANDOM_SEED=24612
  7: RANDOM_SEED=24612
 45: RANDOM_SEED=24612
 11: RANDOM_SEED=24612
 21: RANDOM_SEED=24612
115: RANDOM_SEED=24612
 55: RANDOM_SEED=24612
 63: RANDOM_SEED=24612
 48: RANDOM_SEED=24612
 54: RANDOM_SEED=24612
 49: RANDOM_SEED=24612
 59: RANDOM_SEED=24612
114: RANDOM_SEED=24612
 40: RANDOM_SEED=24612
117: RANDOM_SEED=24612
 62: RANDOM_SEED=24612
 38: RANDOM_SEED=24612
 34: RANDOM_SEED=24612
 39: RANDOM_SEED=24612
 35: RANDOM_SEED=24612
 51: RANDOM_SEED=24612
 15: RANDOM_SEED=24612
 50: RANDOM_SEED=24612
 53: RANDOM_SEED=24612
113: RANDOM_SEED=24612
 94: RANDOM_SEED=24612
 56: RANDOM_SEED=24612
 92: RANDOM_SEED=24612
 33: RANDOM_SEED=24612
 88: RANDOM_SEED=24612
 37: RANDOM_SEED=24612
 52: RANDOM_SEED=24612
 32: RANDOM_SEED=24612
 36: RANDOM_SEED=24612
 91: RANDOM_SEED=24612
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
108: FlashAttention Installed
 13: FlashAttention Installed
 97: FlashAttention Installed
 99: FlashAttention Installed
 72: FlashAttention Installed
 79: FlashAttention Installed
 44: FlashAttention Installed
 46: FlashAttention Installed
 58: FlashAttention Installed
 57: FlashAttention Installed
 29: FlashAttention Installed
 31: FlashAttention Installed
 30: FlashAttention Installed
107: FlashAttention Installed
 69: FlashAttention Installed
 66: FlashAttention Installed
 71: FlashAttention Installed
 95: FlashAttention Installed
 93: FlashAttention Installed
 94: FlashAttention Installed
120: FlashAttention Installed
 59: FlashAttention Installed
 17: FlashAttention Installed
 19: FlashAttention Installed
 16: FlashAttention Installed
 18: FlashAttention Installed
 38: FlashAttention Installed
 39: FlashAttention Installed
 34: FlashAttention Installed
 35: FlashAttention Installed
121: FlashAttention Installed
 49: FlashAttention Installed
 51: FlashAttention Installed
122: FlashAttention Installed
 22: FlashAttention Installed
 21: FlashAttention Installed
  0: FlashAttention Installed
  3: FlashAttention Installed
 20: FlashAttention Installed
 36: FlashAttention Installed
 37: FlashAttention Installed
123: FlashAttention Installed
 33: FlashAttention Installed
 23: FlashAttention Installed
118: FlashAttention Installed
119: FlashAttention Installed
 28: FlashAttention Installed
 32: FlashAttention Installed
 25: FlashAttention Installed
 24: FlashAttention Installed
 26: FlashAttention Installed
 27: FlashAttention Installed
 82: FlashAttention Installed
 87: FlashAttention Installed
 48: FlashAttention Installed
 68: FlashAttention Installed
 64: FlashAttention Installed
 65: FlashAttention Installed
  1: FlashAttention Installed
 92: FlashAttention Installed
 70: FlashAttention Installed
 90: FlashAttention Installed
 67: FlashAttention Installed
113: FlashAttention Installed
115: FlashAttention Installed
 89: FlashAttention Installed
 88: FlashAttention Installed
 91: FlashAttention Installed
 43: FlashAttention Installed
114: FlashAttention Installed
 42: FlashAttention Installed
 41: FlashAttention Installed
 47: FlashAttention Installed
 45: FlashAttention Installed
117: FlashAttention Installed
116: FlashAttention Installed
 50: FlashAttention Installed
 73: FlashAttention Installed
 40: FlashAttention Installed
112: FlashAttention Installed
 77: FlashAttention Installed
 76: FlashAttention Installed
 74: FlashAttention Installed
 75: FlashAttention Installed
111: FlashAttention Installed
110: FlashAttention Installed
 61: FlashAttention Installed
  2: FlashAttention Installed
 52: FlashAttention Installed
 55: FlashAttention Installed
 54: FlashAttention Installed
106: FlashAttention Installed
 62: FlashAttention Installed
109: FlashAttention Installed
 53: FlashAttention Installed
 78: FlashAttention Installed
102: FlashAttention Installed
 85: FlashAttention Installed
125: FlashAttention Installed
 80: FlashAttention Installed
 56: FlashAttention Installed
 63: FlashAttention Installed
 60: FlashAttention Installed
127: FlashAttention Installed
124: FlashAttention Installed
104: FlashAttention Installed
 83: FlashAttention Installed
126: FlashAttention Installed
 81: FlashAttention Installed
105: FlashAttention Installed
  8: FlashAttention Installed
  9: FlashAttention Installed
 15: FlashAttention Installed
 12: FlashAttention Installed
  7: FlashAttention Installed
 84: FlashAttention Installed
 86: FlashAttention Installed
  4: FlashAttention Installed
 10: FlashAttention Installed
  5: FlashAttention Installed
101: FlashAttention Installed
 11: FlashAttention Installed
  6: FlashAttention Installed
 98: FlashAttention Installed
 14: FlashAttention Installed
103: FlashAttention Installed
 96: FlashAttention Installed
100: FlashAttention Installed
 73: :::MLLOG {"namespace": "", "time_ms": 1715318697347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 77: :::MLLOG {"namespace": "", "time_ms": 1715318697347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 79: :::MLLOG {"namespace": "", "time_ms": 1715318697347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 72: :::MLLOG {"namespace": "", "time_ms": 1715318697347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 75: :::MLLOG {"namespace": "", "time_ms": 1715318697347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 74: :::MLLOG {"namespace": "", "time_ms": 1715318697348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 76: :::MLLOG {"namespace": "", "time_ms": 1715318697348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 78: :::MLLOG {"namespace": "", "time_ms": 1715318697348, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
103: :::MLLOG {"namespace": "", "time_ms": 1715318697355, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
102: :::MLLOG {"namespace": "", "time_ms": 1715318697355, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 99: :::MLLOG {"namespace": "", "time_ms": 1715318697356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
101: :::MLLOG {"namespace": "", "time_ms": 1715318697356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 98: :::MLLOG {"namespace": "", "time_ms": 1715318697356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 97: :::MLLOG {"namespace": "", "time_ms": 1715318697356, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 41: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 47: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 42: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 46: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 40: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 44: :::MLLOG {"namespace": "", "time_ms": 1715318697357, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 43: :::MLLOG {"namespace": "", "time_ms": 1715318697358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 96: :::MLLOG {"namespace": "", "time_ms": 1715318697358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 45: :::MLLOG {"namespace": "", "time_ms": 1715318697359, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
107: :::MLLOG {"namespace": "", "time_ms": 1715318697369, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
110: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
111: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
109: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
106: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
108: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
104: :::MLLOG {"namespace": "", "time_ms": 1715318697370, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
105: :::MLLOG {"namespace": "", "time_ms": 1715318697371, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
100: :::MLLOG {"namespace": "", "time_ms": 1715318697377, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 56: :::MLLOG {"namespace": "", "time_ms": 1715318697403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 58: :::MLLOG {"namespace": "", "time_ms": 1715318697403, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 59: :::MLLOG {"namespace": "", "time_ms": 1715318697404, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 61: :::MLLOG {"namespace": "", "time_ms": 1715318697404, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 63: :::MLLOG {"namespace": "", "time_ms": 1715318697405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 62: :::MLLOG {"namespace": "", "time_ms": 1715318697405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 57: :::MLLOG {"namespace": "", "time_ms": 1715318697405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 60: :::MLLOG {"namespace": "", "time_ms": 1715318697405, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 90: :::MLLOG {"namespace": "", "time_ms": 1715318697447, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 93: :::MLLOG {"namespace": "", "time_ms": 1715318697447, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 89: :::MLLOG {"namespace": "", "time_ms": 1715318697447, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 88: :::MLLOG {"namespace": "", "time_ms": 1715318697448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 91: :::MLLOG {"namespace": "", "time_ms": 1715318697448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 94: :::MLLOG {"namespace": "", "time_ms": 1715318697448, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 95: :::MLLOG {"namespace": "", "time_ms": 1715318697449, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 92: :::MLLOG {"namespace": "", "time_ms": 1715318697449, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 31: :::MLLOG {"namespace": "", "time_ms": 1715318697502, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 24: :::MLLOG {"namespace": "", "time_ms": 1715318697502, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 28: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 30: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 27: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 25: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 29: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 26: :::MLLOG {"namespace": "", "time_ms": 1715318697503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 65: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 68: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 71: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 67: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 69: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 70: :::MLLOG {"namespace": "", "time_ms": 1715318697538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 64: :::MLLOG {"namespace": "", "time_ms": 1715318697539, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 66: :::MLLOG {"namespace": "", "time_ms": 1715318697541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
126: :::MLLOG {"namespace": "", "time_ms": 1715318697541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 52: :::MLLOG {"namespace": "", "time_ms": 1715318697541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 55: :::MLLOG {"namespace": "", "time_ms": 1715318697541, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 54: :::MLLOG {"namespace": "", "time_ms": 1715318697542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 50: :::MLLOG {"namespace": "", "time_ms": 1715318697542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
121: :::MLLOG {"namespace": "", "time_ms": 1715318697542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 49: :::MLLOG {"namespace": "", "time_ms": 1715318697542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
125: :::MLLOG {"namespace": "", "time_ms": 1715318697542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 53: :::MLLOG {"namespace": "", "time_ms": 1715318697543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
122: :::MLLOG {"namespace": "", "time_ms": 1715318697543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 51: :::MLLOG {"namespace": "", "time_ms": 1715318697543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
127: :::MLLOG {"namespace": "", "time_ms": 1715318697543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
124: :::MLLOG {"namespace": "", "time_ms": 1715318697543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
123: :::MLLOG {"namespace": "", "time_ms": 1715318697544, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
120: :::MLLOG {"namespace": "", "time_ms": 1715318697544, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 48: :::MLLOG {"namespace": "", "time_ms": 1715318697544, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 10: :::MLLOG {"namespace": "", "time_ms": 1715318697579, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 14: :::MLLOG {"namespace": "", "time_ms": 1715318697579, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  9: :::MLLOG {"namespace": "", "time_ms": 1715318697580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 11: :::MLLOG {"namespace": "", "time_ms": 1715318697580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 15: :::MLLOG {"namespace": "", "time_ms": 1715318697580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 12: :::MLLOG {"namespace": "", "time_ms": 1715318697580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  8: :::MLLOG {"namespace": "", "time_ms": 1715318697581, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 13: :::MLLOG {"namespace": "", "time_ms": 1715318697581, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 17: :::MLLOG {"namespace": "", "time_ms": 1715318697597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 18: :::MLLOG {"namespace": "", "time_ms": 1715318697597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 16: :::MLLOG {"namespace": "", "time_ms": 1715318697597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 22: :::MLLOG {"namespace": "", "time_ms": 1715318697597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 20: :::MLLOG {"namespace": "", "time_ms": 1715318697597, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 23: :::MLLOG {"namespace": "", "time_ms": 1715318697598, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 19: :::MLLOG {"namespace": "", "time_ms": 1715318697598, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 21: :::MLLOG {"namespace": "", "time_ms": 1715318697599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 33: :::MLLOG {"namespace": "", "time_ms": 1715318697631, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 37: :::MLLOG {"namespace": "", "time_ms": 1715318697631, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 39: :::MLLOG {"namespace": "", "time_ms": 1715318697632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 36: :::MLLOG {"namespace": "", "time_ms": 1715318697632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 32: :::MLLOG {"namespace": "", "time_ms": 1715318697633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 34: :::MLLOG {"namespace": "", "time_ms": 1715318697633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 35: :::MLLOG {"namespace": "", "time_ms": 1715318697633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 38: :::MLLOG {"namespace": "", "time_ms": 1715318697634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  2: :::MLLOG {"namespace": "", "time_ms": 1715318697683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  1: :::MLLOG {"namespace": "", "time_ms": 1715318697683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  4: :::MLLOG {"namespace": "", "time_ms": 1715318697683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  7: :::MLLOG {"namespace": "", "time_ms": 1715318697683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  5: :::MLLOG {"namespace": "", "time_ms": 1715318697684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  3: :::MLLOG {"namespace": "", "time_ms": 1715318697684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  6: :::MLLOG {"namespace": "", "time_ms": 1715318697685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
  0: [NeMo W 2024-05-10 05:24:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
  0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  0:       ret = run_job(
  0:     
  0: :::MLLOG {"namespace": "", "time_ms": 1715318697688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
118: :::MLLOG {"namespace": "", "time_ms": 1715318697772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
117: :::MLLOG {"namespace": "", "time_ms": 1715318697772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
114: :::MLLOG {"namespace": "", "time_ms": 1715318697772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
112: :::MLLOG {"namespace": "", "time_ms": 1715318697772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
115: :::MLLOG {"namespace": "", "time_ms": 1715318697772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
119: :::MLLOG {"namespace": "", "time_ms": 1715318697773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
116: :::MLLOG {"namespace": "", "time_ms": 1715318697773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
113: :::MLLOG {"namespace": "", "time_ms": 1715318697774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 80: :::MLLOG {"namespace": "", "time_ms": 1715318697887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 81: :::MLLOG {"namespace": "", "time_ms": 1715318697887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 85: :::MLLOG {"namespace": "", "time_ms": 1715318697887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 87: :::MLLOG {"namespace": "", "time_ms": 1715318697887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 86: :::MLLOG {"namespace": "", "time_ms": 1715318697888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 83: :::MLLOG {"namespace": "", "time_ms": 1715318697888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 84: :::MLLOG {"namespace": "", "time_ms": 1715318697888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 82: :::MLLOG {"namespace": "", "time_ms": 1715318697889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 75: :::MLLOG {"namespace": "", "time_ms": 1715318697960, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3348392821, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 75: [rank: 75] Seed set to 3348392821
 73: :::MLLOG {"namespace": "", "time_ms": 1715318697960, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3008249453, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 73: [rank: 73] Seed set to 3008249453
 72: :::MLLOG {"namespace": "", "time_ms": 1715318697964, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2859095957, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 72: [rank: 72] Seed set to 2859095957
 74: :::MLLOG {"namespace": "", "time_ms": 1715318697974, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1773616787, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 74: [rank: 74] Seed set to 1773616787
 78: :::MLLOG {"namespace": "", "time_ms": 1715318697975, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1311045648, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 78: [rank: 78] Seed set to 1311045648
103: :::MLLOG {"namespace": "", "time_ms": 1715318697975, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2578688809, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
103: [rank: 103] Seed set to 2578688809
 77: :::MLLOG {"namespace": "", "time_ms": 1715318697976, "event_type": "POINT_IN_TIME", "key": "seed", "value": 990143317, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 77: [rank: 77] Seed set to 990143317
106: :::MLLOG {"namespace": "", "time_ms": 1715318697977, "event_type": "POINT_IN_TIME", "key": "seed", "value": 884692383, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
106: [rank: 106] Seed set to 884692383
104: :::MLLOG {"namespace": "", "time_ms": 1715318697977, "event_type": "POINT_IN_TIME", "key": "seed", "value": 882194848, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
104: [rank: 104] Seed set to 882194848
 79: :::MLLOG {"namespace": "", "time_ms": 1715318697977, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2670112453, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 79: [rank: 79] Seed set to 2670112453
 76: :::MLLOG {"namespace": "", "time_ms": 1715318697978, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2974853376, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 76: [rank: 76] Seed set to 2974853376
105: :::MLLOG {"namespace": "", "time_ms": 1715318697983, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3874583575, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
105: [rank: 105] Seed set to 3874583575
107: :::MLLOG {"namespace": "", "time_ms": 1715318697983, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2394909498, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
107: [rank: 107] Seed set to 2394909498
109: :::MLLOG {"namespace": "", "time_ms": 1715318697984, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4242092397, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
109: [rank: 109] Seed set to 4242092397
 98: :::MLLOG {"namespace": "", "time_ms": 1715318697984, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1139858748, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 98: [rank: 98] Seed set to 1139858748
110: :::MLLOG {"namespace": "", "time_ms": 1715318697985, "event_type": "POINT_IN_TIME", "key": "seed", "value": 766779084, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
110: [rank: 110] Seed set to 766779084
111: :::MLLOG {"namespace": "", "time_ms": 1715318697985, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1281592110, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
111: [rank: 111] Seed set to 1281592110
 96: :::MLLOG {"namespace": "", "time_ms": 1715318697988, "event_type": "POINT_IN_TIME", "key": "seed", "value": 221845120, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 96: [rank: 96] Seed set to 221845120
 58: :::MLLOG {"namespace": "", "time_ms": 1715318697989, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4047496540, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 58: [rank: 58] Seed set to 4047496540
102: :::MLLOG {"namespace": "", "time_ms": 1715318697991, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2532378247, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
102: [rank: 102] Seed set to 2532378247
 97: :::MLLOG {"namespace": "", "time_ms": 1715318697991, "event_type": "POINT_IN_TIME", "key": "seed", "value": 687033324, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 97: [rank: 97] Seed set to 687033324
 56: :::MLLOG {"namespace": "", "time_ms": 1715318697994, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1946513531, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 56: [rank: 56] Seed set to 1946513531
101: :::MLLOG {"namespace": "", "time_ms": 1715318697994, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2703349712, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
101: [rank: 101] Seed set to 2703349712
 42: :::MLLOG {"namespace": "", "time_ms": 1715318697994, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3852113382, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 42: [rank: 42] Seed set to 3852113382
108: :::MLLOG {"namespace": "", "time_ms": 1715318697996, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1351948933, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
108: [rank: 108] Seed set to 1351948933
 63: :::MLLOG {"namespace": "", "time_ms": 1715318698003, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2235360652, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 63: [rank: 63] Seed set to 2235360652
 46: :::MLLOG {"namespace": "", "time_ms": 1715318698004, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3844830177, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 46: [rank: 46] Seed set to 3844830177
 41: :::MLLOG {"namespace": "", "time_ms": 1715318698004, "event_type": "POINT_IN_TIME", "key": "seed", "value": 315430577, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 41: [rank: 41] Seed set to 315430577
 61: :::MLLOG {"namespace": "", "time_ms": 1715318698004, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2840660371, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 61: [rank: 61] Seed set to 2840660371
 62: :::MLLOG {"namespace": "", "time_ms": 1715318698005, "event_type": "POINT_IN_TIME", "key": "seed", "value": 502901605, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 62: [rank: 62] Seed set to 502901605
 57: :::MLLOG {"namespace": "", "time_ms": 1715318698008, "event_type": "POINT_IN_TIME", "key": "seed", "value": 597309169, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 57: [rank: 57] Seed set to 597309169
100: :::MLLOG {"namespace": "", "time_ms": 1715318698008, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3984319633, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
100: [rank: 100] Seed set to 3984319633
 99: :::MLLOG {"namespace": "", "time_ms": 1715318698008, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1017464052, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 99: [rank: 99] Seed set to 1017464052
 59: :::MLLOG {"namespace": "", "time_ms": 1715318698009, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3866810313, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 59: [rank: 59] Seed set to 3866810313
 47: :::MLLOG {"namespace": "", "time_ms": 1715318698012, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3886397924, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 47: [rank: 47] Seed set to 3886397924
 43: :::MLLOG {"namespace": "", "time_ms": 1715318698013, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2173726162, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 43: [rank: 43] Seed set to 2173726162
 45: :::MLLOG {"namespace": "", "time_ms": 1715318698013, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2608055906, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 45: [rank: 45] Seed set to 2608055906
 60: :::MLLOG {"namespace": "", "time_ms": 1715318698015, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3262689613, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 60: [rank: 60] Seed set to 3262689613
 40: :::MLLOG {"namespace": "", "time_ms": 1715318698016, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2935245359, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 40: [rank: 40] Seed set to 2935245359
 44: :::MLLOG {"namespace": "", "time_ms": 1715318698024, "event_type": "POINT_IN_TIME", "key": "seed", "value": 702139936, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 44: [rank: 44] Seed set to 702139936
 95: :::MLLOG {"namespace": "", "time_ms": 1715318698033, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1981670824, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 95: [rank: 95] Seed set to 1981670824
 88: :::MLLOG {"namespace": "", "time_ms": 1715318698034, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1906270112, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 88: [rank: 88] Seed set to 1906270112
 89: :::MLLOG {"namespace": "", "time_ms": 1715318698037, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2760291046, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 89: [rank: 89] Seed set to 2760291046
 91: :::MLLOG {"namespace": "", "time_ms": 1715318698046, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4043994522, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 91: [rank: 91] Seed set to 4043994522
 94: :::MLLOG {"namespace": "", "time_ms": 1715318698047, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2837301365, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 94: [rank: 94] Seed set to 2837301365
 93: :::MLLOG {"namespace": "", "time_ms": 1715318698048, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2530844517, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 93: [rank: 93] Seed set to 2530844517
 92: :::MLLOG {"namespace": "", "time_ms": 1715318698049, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1117593148, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 92: [rank: 92] Seed set to 1117593148
 90: :::MLLOG {"namespace": "", "time_ms": 1715318698050, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1664780362, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 90: [rank: 90] Seed set to 1664780362
 67: :::MLLOG {"namespace": "", "time_ms": 1715318698078, "event_type": "POINT_IN_TIME", "key": "seed", "value": 953859159, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 67: [rank: 67] Seed set to 953859159
 64: :::MLLOG {"namespace": "", "time_ms": 1715318698108, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2729507595, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 64: [rank: 64] Seed set to 2729507595
 65: :::MLLOG {"namespace": "", "time_ms": 1715318698109, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3610900043, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 65: [rank: 65] Seed set to 3610900043
 69: :::MLLOG {"namespace": "", "time_ms": 1715318698109, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1382312270, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 69: [rank: 69] Seed set to 1382312270
 70: :::MLLOG {"namespace": "", "time_ms": 1715318698109, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4028175771, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 70: [rank: 70] Seed set to 4028175771
 68: :::MLLOG {"namespace": "", "time_ms": 1715318698114, "event_type": "POINT_IN_TIME", "key": "seed", "value": 844553083, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 68: [rank: 68] Seed set to 844553083
 66: :::MLLOG {"namespace": "", "time_ms": 1715318698115, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2049636199, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 66: [rank: 66] Seed set to 2049636199
 27: :::MLLOG {"namespace": "", "time_ms": 1715318698118, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1002908684, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 27: [rank: 27] Seed set to 1002908684
 71: :::MLLOG {"namespace": "", "time_ms": 1715318698119, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3037738097, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 71: [rank: 71] Seed set to 3037738097
 24: :::MLLOG {"namespace": "", "time_ms": 1715318698121, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4062462410, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 24: [rank: 24] Seed set to 4062462410
 25: :::MLLOG {"namespace": "", "time_ms": 1715318698121, "event_type": "POINT_IN_TIME", "key": "seed", "value": 444426714, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 25: [rank: 25] Seed set to 444426714
 29: :::MLLOG {"namespace": "", "time_ms": 1715318698140, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4233751806, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 29: [rank: 29] Seed set to 4233751806
 26: :::MLLOG {"namespace": "", "time_ms": 1715318698140, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3368430677, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 26: [rank: 26] Seed set to 3368430677
 31: :::MLLOG {"namespace": "", "time_ms": 1715318698140, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4125394782, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 31: [rank: 31] Seed set to 4125394782
 30: :::MLLOG {"namespace": "", "time_ms": 1715318698141, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2305580529, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 30: [rank: 30] Seed set to 2305580529
 28: :::MLLOG {"namespace": "", "time_ms": 1715318698141, "event_type": "POINT_IN_TIME", "key": "seed", "value": 251092894, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 28: [rank: 28] Seed set to 251092894
120: :::MLLOG {"namespace": "", "time_ms": 1715318698150, "event_type": "POINT_IN_TIME", "key": "seed", "value": 100901950, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
120: [rank: 120] Seed set to 100901950
121: :::MLLOG {"namespace": "", "time_ms": 1715318698159, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3047385234, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
121: [rank: 121] Seed set to 3047385234
 49: :::MLLOG {"namespace": "", "time_ms": 1715318698160, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2316218605, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 49: [rank: 49] Seed set to 2316218605
124: :::MLLOG {"namespace": "", "time_ms": 1715318698164, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2594302258, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
124: [rank: 124] Seed set to 2594302258
126: :::MLLOG {"namespace": "", "time_ms": 1715318698165, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3135032266, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
126: [rank: 126] Seed set to 3135032266
122: :::MLLOG {"namespace": "", "time_ms": 1715318698167, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1556370634, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
122: [rank: 122] Seed set to 1556370634
127: :::MLLOG {"namespace": "", "time_ms": 1715318698172, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3955963218, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
127: [rank: 127] Seed set to 3955963218
125: :::MLLOG {"namespace": "", "time_ms": 1715318698175, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2262112129, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
125: [rank: 125] Seed set to 2262112129
 52: :::MLLOG {"namespace": "", "time_ms": 1715318698177, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3030731942, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 52: [rank: 52] Seed set to 3030731942
 48: :::MLLOG {"namespace": "", "time_ms": 1715318698181, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2320252698, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 48: [rank: 48] Seed set to 2320252698
123: :::MLLOG {"namespace": "", "time_ms": 1715318698182, "event_type": "POINT_IN_TIME", "key": "seed", "value": 149591050, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
123: [rank: 123] Seed set to 149591050
 53: :::MLLOG {"namespace": "", "time_ms": 1715318698189, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3950649061, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 53: [rank: 53] Seed set to 3950649061
 51: :::MLLOG {"namespace": "", "time_ms": 1715318698191, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1820637060, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 51: [rank: 51] Seed set to 1820637060
 55: :::MLLOG {"namespace": "", "time_ms": 1715318698191, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2139109406, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 55: [rank: 55] Seed set to 2139109406
 54: :::MLLOG {"namespace": "", "time_ms": 1715318698194, "event_type": "POINT_IN_TIME", "key": "seed", "value": 712688923, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 54: [rank: 54] Seed set to 712688923
 50: :::MLLOG {"namespace": "", "time_ms": 1715318698195, "event_type": "POINT_IN_TIME", "key": "seed", "value": 317457671, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 50: [rank: 50] Seed set to 317457671
  9: :::MLLOG {"namespace": "", "time_ms": 1715318698212, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2634961644, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  9: [rank: 9] Seed set to 2634961644
 12: :::MLLOG {"namespace": "", "time_ms": 1715318698212, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4161492257, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 12: [rank: 12] Seed set to 4161492257
 14: :::MLLOG {"namespace": "", "time_ms": 1715318698215, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3349570028, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 14: [rank: 14] Seed set to 3349570028
 10: :::MLLOG {"namespace": "", "time_ms": 1715318698219, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3311779071, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 10: [rank: 10] Seed set to 3311779071
  8: :::MLLOG {"namespace": "", "time_ms": 1715318698219, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3348302011, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  8: [rank: 8] Seed set to 3348302011
 15: :::MLLOG {"namespace": "", "time_ms": 1715318698221, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3178770128, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 15: [rank: 15] Seed set to 3178770128
 11: :::MLLOG {"namespace": "", "time_ms": 1715318698222, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1743861090, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 11: [rank: 11] Seed set to 1743861090
 13: :::MLLOG {"namespace": "", "time_ms": 1715318698228, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2641488642, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 13: [rank: 13] Seed set to 2641488642
 18: :::MLLOG {"namespace": "", "time_ms": 1715318698235, "event_type": "POINT_IN_TIME", "key": "seed", "value": 290801586, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 18: [rank: 18] Seed set to 290801586
 17: :::MLLOG {"namespace": "", "time_ms": 1715318698237, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1157201996, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 17: [rank: 17] Seed set to 1157201996
 16: :::MLLOG {"namespace": "", "time_ms": 1715318698238, "event_type": "POINT_IN_TIME", "key": "seed", "value": 876118220, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 16: [rank: 16] Seed set to 876118220
 23: :::MLLOG {"namespace": "", "time_ms": 1715318698239, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3991113728, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 23: [rank: 23] Seed set to 3991113728
 20: :::MLLOG {"namespace": "", "time_ms": 1715318698246, "event_type": "POINT_IN_TIME", "key": "seed", "value": 636015518, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 20: [rank: 20] Seed set to 636015518
 19: :::MLLOG {"namespace": "", "time_ms": 1715318698255, "event_type": "POINT_IN_TIME", "key": "seed", "value": 442882522, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 19: [rank: 19] Seed set to 442882522
 22: :::MLLOG {"namespace": "", "time_ms": 1715318698256, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4011280408, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 22: [rank: 22] Seed set to 4011280408
 21: :::MLLOG {"namespace": "", "time_ms": 1715318698257, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1632650000, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 21: [rank: 21] Seed set to 1632650000
 34: :::MLLOG {"namespace": "", "time_ms": 1715318698259, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3598202500, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 34: [rank: 34] Seed set to 3598202500
 33: :::MLLOG {"namespace": "", "time_ms": 1715318698262, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1569378524, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 33: [rank: 33] Seed set to 1569378524
 35: :::MLLOG {"namespace": "", "time_ms": 1715318698272, "event_type": "POINT_IN_TIME", "key": "seed", "value": 152328515, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 35: [rank: 35] Seed set to 152328515
 36: :::MLLOG {"namespace": "", "time_ms": 1715318698272, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2582644590, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 36: [rank: 36] Seed set to 2582644590
 32: :::MLLOG {"namespace": "", "time_ms": 1715318698273, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2103003229, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 32: [rank: 32] Seed set to 2103003229
 39: :::MLLOG {"namespace": "", "time_ms": 1715318698277, "event_type": "POINT_IN_TIME", "key": "seed", "value": 614598288, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 39: [rank: 39] Seed set to 614598288
 37: :::MLLOG {"namespace": "", "time_ms": 1715318698278, "event_type": "POINT_IN_TIME", "key": "seed", "value": 414042907, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 37: [rank: 37] Seed set to 414042907
 38: :::MLLOG {"namespace": "", "time_ms": 1715318698278, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3341674190, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 38: [rank: 38] Seed set to 3341674190
  4: :::MLLOG {"namespace": "", "time_ms": 1715318698303, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1375175387, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  4: [rank: 4] Seed set to 1375175387
  2: :::MLLOG {"namespace": "", "time_ms": 1715318698314, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3909146449, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  2: [rank: 2] Seed set to 3909146449
  1: :::MLLOG {"namespace": "", "time_ms": 1715318698315, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2417075840, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  1: [rank: 1] Seed set to 2417075840
  5: :::MLLOG {"namespace": "", "time_ms": 1715318698319, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1315517468, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  5: [rank: 5] Seed set to 1315517468
  6: :::MLLOG {"namespace": "", "time_ms": 1715318698326, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2846866878, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  6: [rank: 6] Seed set to 2846866878
  7: :::MLLOG {"namespace": "", "time_ms": 1715318698327, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2037256848, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  7: [rank: 7] Seed set to 2037256848
  0: [NeMo I 2024-05-10 05:24:58 main:65] L2 promotion: 128 B
  0: :::MLLOG {"namespace": "", "time_ms": 1715318698330, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4089765090, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  0: [rank: 0] Seed set to 4089765090
  0: [NeMo I 2024-05-10 05:24:58 main:85] 
  0:     
  0:     ************** Experiment configuration ***********
  3: :::MLLOG {"namespace": "", "time_ms": 1715318698332, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4090577753, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
  3: [rank: 3] Seed set to 4090577753
  0: [NeMo I 2024-05-10 05:24:58 main:86] 
  0:     name: stable-diffusion2-train-240510052401216597839
  0:     trainer:
  0:       devices: 8
  0:       num_nodes: 16
  0:       accelerator: gpu
  0:       logger: false
  0:       enable_checkpointing: false
  0:       max_epochs: -1
  0:       max_steps: 3500
  0:       log_every_n_steps: 10000
  0:       accumulate_grad_batches: 1
  0:       gradient_clip_val: 1.0
  0:       benchmark: false
  0:       enable_model_summary: true
  0:     exp_manager:
  0:       exp_dir: /tmp/nemologs
  0:       name: ${name}
  0:       create_wandb_logger: false
  0:       wandb_logger_kwargs:
  0:         project: stable-diffusion
  0:         group: nemo-sd
  0:         name: ${name}
  0:         resume: true
  0:       create_checkpoint_callback: true
  0:       create_tensorboard_logger: true
  0:       checkpoint_callback_params:
  0:         every_n_train_steps: 500
  0:         every_n_epochs: 0
  0:         monitor: timestamp
  0:         filename: ${name}--{timestamp}-{step}-{consumed_samples}
  0:         save_top_k: -1
  0:         save_last: false
  0:         save_nemo_on_train_end: false
  0:         save_weights_only: true
  0:       resume_if_exists: true
  0:       resume_ignore_no_checkpoint: true
  0:       ema:
  0:         enable: false
  0:         decay: 0.9999
  0:         validate_original_weights: false
  0:         every_n_steps: 1
  0:         cpu_offload: false
  0:       create_preemption_callback: false
  0:       log_step_timing: false
  0:     model:
  0:       precision: 16
  0:       micro_batch_size: 8
  0:       global_batch_size: 1024
  0:       linear_start: 0.00085
  0:       linear_end: 0.012
  0:       num_timesteps_cond: 1
  0:       log_every_t: 200
  0:       timesteps: 1000
  0:       first_stage_key: images_moments
  0:       cond_stage_key: clip_encoded
  0:       image_size: 64
  0:       channels: 4
  0:       cond_stage_trainable: false
  0:       conditioning_key: crossattn
  0:       monitor: val/loss_simple_ema
  0:       scale_factor: 0.18215
  0:       use_ema: false
  0:       scale_by_std: false
  0:       ckpt_path: /checkpoints/sd/512-base-ema.ckpt
  0:       load_vae: true
  0:       load_unet: false
  0:       load_encoder: true
  0:       ignore_keys: []
  0:       parameterization: v
  0:       clip_denoised: true
  0:       load_only_unet: false
  0:       cosine_s: 0.008
  0:       given_betas: null
  0:       original_elbo_weight: 0
  0:       v_posterior: 0
  0:       l_simple_weight: 1
  0:       use_positional_encodings: false
  0:       learn_logvar: false
  0:       logvar_init: 0
  0:       beta_schedule: linear
  0:       loss_type: l2
  0:       channels_last: true
  0:       concat_mode: true
  0:       cond_stage_forward: null
  0:       text_embedding_dropout_rate: 0.0
  0:       fused_opt: true
  0:       inductor: true
  0:       inductor_cudagraphs: false
  0:       capture_cudagraph_iters: 15
  0:       unet_config:
  0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
  0:         from_pretrained: null
  0:         from_NeMo: null
  0:         image_size: 32
  0:         in_channels: 4
  0:         out_channels: 4
  0:         model_channels: 320
  0:         attention_resolutions:
  0:         - 4
  0:         - 2
  0:         - 1
  0:         num_res_blocks: 2
  0:         channel_mult:
  0:         - 1
  0:         - 2
  0:         - 4
  0:         - 4
  0:         num_head_channels: 64
  0:         use_spatial_transformer: true
  0:         use_linear_in_transformer: true
  0:         transformer_depth: 1
  0:         context_dim: 1024
  0:         use_checkpoint: false
  0:         legacy: false
  0:         use_flash_attention: true
  0:         resblock_gn_groups: 16
  0:         unet_precision: fp16
  0:         timesteps: ${model.timesteps}
  0:       first_stage_config:
  0:         _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
  0:         from_pretrained: null
  0:         embed_dim: 4
  0:         monitor: val/rec_loss
  0:         ddconfig:
  0:           double_z: true
  0:           z_channels: 4
  0:           resolution: 256
  0:           in_channels: 3
  0:           out_ch: 3
  0:           ch: 128
  0:           ch_mult:
  0:           - 1
  0:           - 2
  0:           - 4
  0:           - 4
  0:           num_res_blocks: 2
  0:           attn_resolutions: []
  0:           dropout: 0.0
  0:         lossconfig:
  0:           target: torch.nn.Identity
  0:       cond_stage_config:
  0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder
  0:         arch: ViT-H-14
  0:         version: laion2b_s32b_b79k
  0:         freeze: true
  0:         layer: penultimate
  0:         cache_dir: /checkpoints/clip
  0:       seed: 24612
  0:       resume_from_checkpoint: null
  0:       apex_transformer_log_level: 30
  0:       gradient_as_bucket_view: true
  0:       ddp_overlap: false
  0:       nsys_profile:
  0:         enabled: false
  0:         start_step: 10
  0:         end_step: 10
  0:         ranks:
  0:         - 0
  0:         gen_shape: false
  0:       data:
  0:         num_workers: 16
  0:         train:
  0:           dataset_path: /datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar
  0:           augmentations:
  0:             resize_smallest_side: 512
  0:             center_crop_h_w: 512, 512
  0:             horizontal_flip: false
  0:           filterings: null
  0:         webdataset:
  0:           infinite_sampler: true
  0:           local_root_path: /datasets/laion-400m/webdataset-moments-filtered-encoded
  0:       optim:
  0:         name: distributed_fused_adam
  0:         lr: 0.0001024
  0:         weight_decay: 0.0
  0:         betas:
  0:         - 0.9
  0:         - 0.999
  0:         sched:
  0:           name: WarmupHoldPolicy
  0:           warmup_steps: 1000
  0:           hold_steps: 10000000000000
  0:         bucket_cap_mb: 288
  0:         overlap_grad_sync: true
  0:         overlap_param_sync: false
  0:         contiguous_grad_buffer: true
  0:         contiguous_param_buffer: true
  0:         store_params: true
  0:         dtype: torch.float32
  0:         grad_sync_dtype: torch.float16
  0:         param_sync_dtype: torch.float16
  0:         capturable: true
  0:         distribute_within_nodes: true
  0:     
  0: [NeMo W 2024-05-10 05:24:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
  0:     
119: :::MLLOG {"namespace": "", "time_ms": 1715318698364, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4044593993, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
119: [rank: 119] Seed set to 4044593993
117: :::MLLOG {"namespace": "", "time_ms": 1715318698364, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3406208563, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
117: [rank: 117] Seed set to 3406208563
114: :::MLLOG {"namespace": "", "time_ms": 1715318698365, "event_type": "POINT_IN_TIME", "key": "seed", "value": 656032849, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
114: [rank: 114] Seed set to 656032849
112: :::MLLOG {"namespace": "", "time_ms": 1715318698365, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2923758506, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
112: [rank: 112] Seed set to 2923758506
116: :::MLLOG {"namespace": "", "time_ms": 1715318698365, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2363975741, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
116: [rank: 116] Seed set to 2363975741
113: :::MLLOG {"namespace": "", "time_ms": 1715318698366, "event_type": "POINT_IN_TIME", "key": "seed", "value": 326186579, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
113: [rank: 113] Seed set to 326186579
118: :::MLLOG {"namespace": "", "time_ms": 1715318698366, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1991396656, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
115: :::MLLOG {"namespace": "", "time_ms": 1715318698366, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3920121379, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
118: [rank: 118] Seed set to 1991396656
115: [rank: 115] Seed set to 3920121379
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: IPU available: False, using: 0 IPUs
  0: HPU available: False, using: 0 HPUs
  0: [NeMo E 2024-05-10 05:24:58 exp_manager:556] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
  0: [NeMo W 2024-05-10 05:24:58 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
  0: [NeMo W 2024-05-10 05:24:58 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints. Training from scratch.
  0: [NeMo I 2024-05-10 05:24:58 exp_manager:396] Experiments will be logged at /tmp/nemologs/stable-diffusion2-train-240510052401216597839
  0: [NeMo I 2024-05-10 05:24:58 exp_manager:856] TensorboardLogger has been set up
 80: :::MLLOG {"namespace": "", "time_ms": 1715318698485, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3862671085, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 80: [rank: 80] Seed set to 3862671085
 81: :::MLLOG {"namespace": "", "time_ms": 1715318698506, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3422979223, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 81: [rank: 81] Seed set to 3422979223
 85: :::MLLOG {"namespace": "", "time_ms": 1715318698506, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1700048119, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 85: [rank: 85] Seed set to 1700048119
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 83: :::MLLOG {"namespace": "", "time_ms": 1715318698507, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3822563035, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 83: [rank: 83] Seed set to 3822563035
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 87: :::MLLOG {"namespace": "", "time_ms": 1715318698508, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3644619695, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 87: [rank: 87] Seed set to 3644619695
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_
  0: channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filtere
  0: d-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:279] Ranks 0 has data parallel rank: 0
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:287] Rank 0 has context parallel group: [0]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:291] Ranks 0 has context parallel rank: 0
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:298] Rank 0 has model parallel group: [0]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
 84: :::MLLOG {"namespace": "", "time_ms": 1715318698511, "event_type": "POINT_IN_TIME", "key": "seed", "value": 521739848, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 84: [rank: 84] Seed set to 521739848
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:308] Rank 0 has tensor model parallel group: [0]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:313] Rank 0 has tensor model parallel rank: 0
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:345] Rank 0 has embedding group: [0]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:352] Rank 0 has pipeline model parallel rank 0
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:24:58 megatron_init:354] Rank 0 has embedding rank: 0
  0: 24-05-10 05:24:58 - PID:810158 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:24:58 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_
  0: channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filtere
  0: d-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [NeMo I 2024-05-10 05:24:58 ddpm:130] LatentDiffusion: Running in v-prediction mode
 86: :::MLLOG {"namespace": "", "time_ms": 1715318698523, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1973534660, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 86: [rank: 86] Seed set to 1973534660
 82: :::MLLOG {"namespace": "", "time_ms": 1715318698524, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3931028733, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 82: [rank: 82] Seed set to 3931028733
  0: [NeMo I 2024-05-10 05:24:58 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:24:58 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:24:58 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:24:58 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:24:58 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:24:59 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:25:00 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:25:01 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:25:01 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:25:02 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:25:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:25:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:25:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:25:03 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:25:03 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:25:03 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 73: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:25:03 utils:92] DiffusionWrapper has 865.91 M params.
  0: [NeMo I 2024-05-10 05:25:03 ddpm:168] Use system random generator since CUDA graph enabled
  0: making attention of type 'vanilla' with 512 in_channels
  0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  0: making attention of type 'vanilla' with 512 in_channels
  0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  0: Loaded ViT-H-14 model config.
 40: making attention of type 'vanilla' with 512 in_channels
 40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 40: making attention of type 'vanilla' with 512 in_channels
 56: making attention of type 'vanilla' with 512 in_channels
 72: making attention of type 'vanilla' with 512 in_channels
104: making attention of type 'vanilla' with 512 in_channels
 56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 96: making attention of type 'vanilla' with 512 in_channels
 72: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 56: making attention of type 'vanilla' with 512 in_channels
104: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 96: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 64: making attention of type 'vanilla' with 512 in_channels
 72: making attention of type 'vanilla' with 512 in_channels
104: making attention of type 'vanilla' with 512 in_channels
 96: making attention of type 'vanilla' with 512 in_channels
 64: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 88: making attention of type 'vanilla' with 512 in_channels
 64: making attention of type 'vanilla' with 512 in_channels
 88: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 24: making attention of type 'vanilla' with 512 in_channels
120: making attention of type 'vanilla' with 512 in_channels
 88: making attention of type 'vanilla' with 512 in_channels
 24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
120: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 24: making attention of type 'vanilla' with 512 in_channels
120: making attention of type 'vanilla' with 512 in_channels
 40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 40: Loaded ViT-H-14 model config.
 48: making attention of type 'vanilla' with 512 in_channels
 57: making attention of type 'vanilla' with 512 in_channels
 48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 76: making attention of type 'vanilla' with 512 in_channels
 56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 56: Loaded ViT-H-14 model config.
 77: making attention of type 'vanilla' with 512 in_channels
 79: making attention of type 'vanilla' with 512 in_channels
 97: making attention of type 'vanilla' with 512 in_channels
 73: making attention of type 'vanilla' with 512 in_channels
 74: making attention of type 'vanilla' with 512 in_channels
 99: making attention of type 'vanilla' with 512 in_channels
 58: making attention of type 'vanilla' with 512 in_channels
 57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 59: making attention of type 'vanilla' with 512 in_channels
 48: making attention of type 'vanilla' with 512 in_channels
 78: making attention of type 'vanilla' with 512 in_channels
 61: making attention of type 'vanilla' with 512 in_channels
 72: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 72: Loaded ViT-H-14 model config.
104: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
104: Loaded ViT-H-14 model config.
101: making attention of type 'vanilla' with 512 in_channels
 62: making attention of type 'vanilla' with 512 in_channels
 98: making attention of type 'vanilla' with 512 in_channels
 16: making attention of type 'vanilla' with 512 in_channels
 76: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 41: making attention of type 'vanilla' with 512 in_channels
  8: making attention of type 'vanilla' with 512 in_channels
 79: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 77: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 93: making attention of type 'vanilla' with 512 in_channels
103: making attention of type 'vanilla' with 512 in_channels
 92: making attention of type 'vanilla' with 512 in_channels
 43: making attention of type 'vanilla' with 512 in_channels
 97: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: making attention of type 'vanilla' with 512 in_channels
 75: making attention of type 'vanilla' with 512 in_channels
 73: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 57: making attention of type 'vanilla' with 512 in_channels
 60: making attention of type 'vanilla' with 512 in_channels
 94: making attention of type 'vanilla' with 512 in_channels
 74: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
100: making attention of type 'vanilla' with 512 in_channels
 96: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 96: Loaded ViT-H-14 model config.
 99: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: making attention of type 'vanilla' with 512 in_channels
 67: making attention of type 'vanilla' with 512 in_channels
 58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 70: making attention of type 'vanilla' with 512 in_channels
 59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 69: making attention of type 'vanilla' with 512 in_channels
 95: making attention of type 'vanilla' with 512 in_channels
 78: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
108: making attention of type 'vanilla' with 512 in_channels
 61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
102: making attention of type 'vanilla' with 512 in_channels
 68: making attention of type 'vanilla' with 512 in_channels
 76: making attention of type 'vanilla' with 512 in_channels
 79: making attention of type 'vanilla' with 512 in_channels
 77: making attention of type 'vanilla' with 512 in_channels
101: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 98: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: making attention of type 'vanilla' with 512 in_channels
 16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
110: making attention of type 'vanilla' with 512 in_channels
 97: making attention of type 'vanilla' with 512 in_channels
109: making attention of type 'vanilla' with 512 in_channels
 41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 93: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
105: making attention of type 'vanilla' with 512 in_channels
 92: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
103: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 32: making attention of type 'vanilla' with 512 in_channels
 73: making attention of type 'vanilla' with 512 in_channels
 71: making attention of type 'vanilla' with 512 in_channels
 63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
107: making attention of type 'vanilla' with 512 in_channels
 66: making attention of type 'vanilla' with 512 in_channels
 74: making attention of type 'vanilla' with 512 in_channels
 89: making attention of type 'vanilla' with 512 in_channels
 47: making attention of type 'vanilla' with 512 in_channels
 99: making attention of type 'vanilla' with 512 in_channels
 75: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 58: making attention of type 'vanilla' with 512 in_channels
 94: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 59: making attention of type 'vanilla' with 512 in_channels
100: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 78: making attention of type 'vanilla' with 512 in_channels
 64: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 64: Loaded ViT-H-14 model config.
 67: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 61: making attention of type 'vanilla' with 512 in_channels
 70: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 44: making attention of type 'vanilla' with 512 in_channels
 91: making attention of type 'vanilla' with 512 in_channels
 46: making attention of type 'vanilla' with 512 in_channels
 69: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 95: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
108: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 90: making attention of type 'vanilla' with 512 in_channels
111: making attention of type 'vanilla' with 512 in_channels
 45: making attention of type 'vanilla' with 512 in_channels
102: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
101: making attention of type 'vanilla' with 512 in_channels
 68: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 62: making attention of type 'vanilla' with 512 in_channels
 98: making attention of type 'vanilla' with 512 in_channels
 16: making attention of type 'vanilla' with 512 in_channels
 41: making attention of type 'vanilla' with 512 in_channels
 93: making attention of type 'vanilla' with 512 in_channels
  8: making attention of type 'vanilla' with 512 in_channels
 92: making attention of type 'vanilla' with 512 in_channels
103: making attention of type 'vanilla' with 512 in_channels
 43: making attention of type 'vanilla' with 512 in_channels
 65: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
110: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: making attention of type 'vanilla' with 512 in_channels
109: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
105: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
106: making attention of type 'vanilla' with 512 in_channels
 32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 71: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 75: making attention of type 'vanilla' with 512 in_channels
 94: making attention of type 'vanilla' with 512 in_channels
 60: making attention of type 'vanilla' with 512 in_channels
107: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 66: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 89: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
100: making attention of type 'vanilla' with 512 in_channels
 47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: making attention of type 'vanilla' with 512 in_channels
 67: making attention of type 'vanilla' with 512 in_channels
 70: making attention of type 'vanilla' with 512 in_channels
 69: making attention of type 'vanilla' with 512 in_channels
 95: making attention of type 'vanilla' with 512 in_channels
108: making attention of type 'vanilla' with 512 in_channels
102: making attention of type 'vanilla' with 512 in_channels
 44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 91: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 68: making attention of type 'vanilla' with 512 in_channels
 46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 90: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
111: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: making attention of type 'vanilla' with 512 in_channels
 88: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 88: Loaded ViT-H-14 model config.
110: making attention of type 'vanilla' with 512 in_channels
109: making attention of type 'vanilla' with 512 in_channels
105: making attention of type 'vanilla' with 512 in_channels
 32: making attention of type 'vanilla' with 512 in_channels
 71: making attention of type 'vanilla' with 512 in_channels
107: making attention of type 'vanilla' with 512 in_channels
 66: making attention of type 'vanilla' with 512 in_channels
 89: making attention of type 'vanilla' with 512 in_channels
106: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 47: making attention of type 'vanilla' with 512 in_channels
112: making attention of type 'vanilla' with 512 in_channels
 44: making attention of type 'vanilla' with 512 in_channels
 91: making attention of type 'vanilla' with 512 in_channels
 46: making attention of type 'vanilla' with 512 in_channels
 90: making attention of type 'vanilla' with 512 in_channels
111: making attention of type 'vanilla' with 512 in_channels
 45: making attention of type 'vanilla' with 512 in_channels
106: making attention of type 'vanilla' with 512 in_channels
 27: making attention of type 'vanilla' with 512 in_channels
112: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 25: making attention of type 'vanilla' with 512 in_channels
 24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 24: Loaded ViT-H-14 model config.
120: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
120: Loaded ViT-H-14 model config.
 29: making attention of type 'vanilla' with 512 in_channels
 26: making attention of type 'vanilla' with 512 in_channels
112: making attention of type 'vanilla' with 512 in_channels
 27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
121: making attention of type 'vanilla' with 512 in_channels
 25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
125: making attention of type 'vanilla' with 512 in_channels
 28: making attention of type 'vanilla' with 512 in_channels
 30: making attention of type 'vanilla' with 512 in_channels
127: making attention of type 'vanilla' with 512 in_channels
 31: making attention of type 'vanilla' with 512 in_channels
126: making attention of type 'vanilla' with 512 in_channels
124: making attention of type 'vanilla' with 512 in_channels
123: making attention of type 'vanilla' with 512 in_channels
 49: making attention of type 'vanilla' with 512 in_channels
122: making attention of type 'vanilla' with 512 in_channels
 80: making attention of type 'vanilla' with 512 in_channels
 29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 50: making attention of type 'vanilla' with 512 in_channels
 51: making attention of type 'vanilla' with 512 in_channels
 26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 27: making attention of type 'vanilla' with 512 in_channels
 25: making attention of type 'vanilla' with 512 in_channels
121: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 52: making attention of type 'vanilla' with 512 in_channels
125: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
127: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 53: making attention of type 'vanilla' with 512 in_channels
 31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
126: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
124: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
123: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
122: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 29: making attention of type 'vanilla' with 512 in_channels
 80: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 55: making attention of type 'vanilla' with 512 in_channels
 26: making attention of type 'vanilla' with 512 in_channels
 51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
121: making attention of type 'vanilla' with 512 in_channels
 54: making attention of type 'vanilla' with 512 in_channels
125: making attention of type 'vanilla' with 512 in_channels
 52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 28: making attention of type 'vanilla' with 512 in_channels
 30: making attention of type 'vanilla' with 512 in_channels
127: making attention of type 'vanilla' with 512 in_channels
 31: making attention of type 'vanilla' with 512 in_channels
 48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
126: making attention of type 'vanilla' with 512 in_channels
 48: Loaded ViT-H-14 model config.
124: making attention of type 'vanilla' with 512 in_channels
 53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
123: making attention of type 'vanilla' with 512 in_channels
 49: making attention of type 'vanilla' with 512 in_channels
122: making attention of type 'vanilla' with 512 in_channels
 80: making attention of type 'vanilla' with 512 in_channels
 51: making attention of type 'vanilla' with 512 in_channels
 50: making attention of type 'vanilla' with 512 in_channels
 55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 52: making attention of type 'vanilla' with 512 in_channels
 54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 57: Loaded ViT-H-14 model config.
 53: making attention of type 'vanilla' with 512 in_channels
 76: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 79: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 76: Loaded ViT-H-14 model config.
 79: Loaded ViT-H-14 model config.
 77: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 77: Loaded ViT-H-14 model config.
 55: making attention of type 'vanilla' with 512 in_channels
 97: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 97: Loaded ViT-H-14 model config.
 13: making attention of type 'vanilla' with 512 in_channels
 54: making attention of type 'vanilla' with 512 in_channels
 73: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 73: Loaded ViT-H-14 model config.
 74: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 74: Loaded ViT-H-14 model config.
  9: making attention of type 'vanilla' with 512 in_channels
 99: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 99: Loaded ViT-H-14 model config.
 58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 58: Loaded ViT-H-14 model config.
 59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 59: Loaded ViT-H-14 model config.
 78: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 78: Loaded ViT-H-14 model config.
 12: making attention of type 'vanilla' with 512 in_channels
 61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 61: Loaded ViT-H-14 model config.
 14: making attention of type 'vanilla' with 512 in_channels
 11: making attention of type 'vanilla' with 512 in_channels
101: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 10: making attention of type 'vanilla' with 512 in_channels
101: Loaded ViT-H-14 model config.
  4: making attention of type 'vanilla' with 512 in_channels
 16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 16: Loaded ViT-H-14 model config.
 62: Loaded ViT-H-14 model config.
 98: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 98: Loaded ViT-H-14 model config.
 15: making attention of type 'vanilla' with 512 in_channels
 41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 41: Loaded ViT-H-14 model config.
 93: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 93: Loaded ViT-H-14 model config.
  8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  8: Loaded ViT-H-14 model config.
 92: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 92: Loaded ViT-H-14 model config.
103: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
103: Loaded ViT-H-14 model config.
 43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 43: Loaded ViT-H-14 model config.
 36: making attention of type 'vanilla' with 512 in_channels
 13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 63: Loaded ViT-H-14 model config.
  2: making attention of type 'vanilla' with 512 in_channels
  7: making attention of type 'vanilla' with 512 in_channels
 18: making attention of type 'vanilla' with 512 in_channels
 94: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 94: Loaded ViT-H-14 model config.
 60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 60: Loaded ViT-H-14 model config.
 75: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 75: Loaded ViT-H-14 model config.
 37: making attention of type 'vanilla' with 512 in_channels
 38: making attention of type 'vanilla' with 512 in_channels
100: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
100: Loaded ViT-H-14 model config.
 17: making attention of type 'vanilla' with 512 in_channels
  9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 42: Loaded ViT-H-14 model config.
 39: making attention of type 'vanilla' with 512 in_channels
 33: making attention of type 'vanilla' with 512 in_channels
  5: making attention of type 'vanilla' with 512 in_channels
 67: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 67: Loaded ViT-H-14 model config.
 70: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 70: Loaded ViT-H-14 model config.
 19: making attention of type 'vanilla' with 512 in_channels
 35: making attention of type 'vanilla' with 512 in_channels
  1: making attention of type 'vanilla' with 512 in_channels
 69: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 95: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 69: Loaded ViT-H-14 model config.
108: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 95: Loaded ViT-H-14 model config.
108: Loaded ViT-H-14 model config.
 12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
102: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
102: Loaded ViT-H-14 model config.
 68: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 68: Loaded ViT-H-14 model config.
 14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  6: making attention of type 'vanilla' with 512 in_channels
 20: making attention of type 'vanilla' with 512 in_channels
 11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 65: Loaded ViT-H-14 model config.
 15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 13: making attention of type 'vanilla' with 512 in_channels
110: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
110: Loaded ViT-H-14 model config.
109: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
109: Loaded ViT-H-14 model config.
105: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
105: Loaded ViT-H-14 model config.
 32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 32: Loaded ViT-H-14 model config.
 36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 71: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 71: Loaded ViT-H-14 model config.
  3: making attention of type 'vanilla' with 512 in_channels
  2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 23: making attention of type 'vanilla' with 512 in_channels
107: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
107: Loaded ViT-H-14 model config.
 89: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 89: Loaded ViT-H-14 model config.
 66: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 66: Loaded ViT-H-14 model config.
 47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 47: Loaded ViT-H-14 model config.
  9: making attention of type 'vanilla' with 512 in_channels
  7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 22: making attention of type 'vanilla' with 512 in_channels
 39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 12: making attention of type 'vanilla' with 512 in_channels
  5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 44: Loaded ViT-H-14 model config.
 14: making attention of type 'vanilla' with 512 in_channels
 91: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 91: Loaded ViT-H-14 model config.
 46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 46: Loaded ViT-H-14 model config.
  1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 34: making attention of type 'vanilla' with 512 in_channels
 21: making attention of type 'vanilla' with 512 in_channels
 90: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 90: Loaded ViT-H-14 model config.
 45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 45: Loaded ViT-H-14 model config.
111: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
111: Loaded ViT-H-14 model config.
 11: making attention of type 'vanilla' with 512 in_channels
 10: making attention of type 'vanilla' with 512 in_channels
  4: making attention of type 'vanilla' with 512 in_channels
113: making attention of type 'vanilla' with 512 in_channels
 15: making attention of type 'vanilla' with 512 in_channels
  6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 36: making attention of type 'vanilla' with 512 in_channels
  2: making attention of type 'vanilla' with 512 in_channels
106: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
106: Loaded ViT-H-14 model config.
  7: making attention of type 'vanilla' with 512 in_channels
  3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 18: making attention of type 'vanilla' with 512 in_channels
 23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 37: making attention of type 'vanilla' with 512 in_channels
 38: making attention of type 'vanilla' with 512 in_channels
 17: making attention of type 'vanilla' with 512 in_channels
 39: making attention of type 'vanilla' with 512 in_channels
  5: making attention of type 'vanilla' with 512 in_channels
 33: making attention of type 'vanilla' with 512 in_channels
 19: making attention of type 'vanilla' with 512 in_channels
 35: making attention of type 'vanilla' with 512 in_channels
114: making attention of type 'vanilla' with 512 in_channels
  1: making attention of type 'vanilla' with 512 in_channels
 22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
115: making attention of type 'vanilla' with 512 in_channels
 34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
118: making attention of type 'vanilla' with 512 in_channels
  6: making attention of type 'vanilla' with 512 in_channels
 20: making attention of type 'vanilla' with 512 in_channels
113: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: making attention of type 'vanilla' with 512 in_channels
  3: making attention of type 'vanilla' with 512 in_channels
 23: making attention of type 'vanilla' with 512 in_channels
116: making attention of type 'vanilla' with 512 in_channels
112: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
112: Loaded ViT-H-14 model config.
 22: making attention of type 'vanilla' with 512 in_channels
114: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 34: making attention of type 'vanilla' with 512 in_channels
 21: making attention of type 'vanilla' with 512 in_channels
115: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
119: making attention of type 'vanilla' with 512 in_channels
113: making attention of type 'vanilla' with 512 in_channels
118: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
116: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 27: Loaded ViT-H-14 model config.
 25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 25: Loaded ViT-H-14 model config.
114: making attention of type 'vanilla' with 512 in_channels
115: making attention of type 'vanilla' with 512 in_channels
118: making attention of type 'vanilla' with 512 in_channels
119: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: making attention of type 'vanilla' with 512 in_channels
116: making attention of type 'vanilla' with 512 in_channels
 29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 29: Loaded ViT-H-14 model config.
 26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 26: Loaded ViT-H-14 model config.
121: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
121: Loaded ViT-H-14 model config.
125: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
125: Loaded ViT-H-14 model config.
119: making attention of type 'vanilla' with 512 in_channels
 28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 28: Loaded ViT-H-14 model config.
 30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 30: Loaded ViT-H-14 model config.
127: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
127: Loaded ViT-H-14 model config.
 31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 31: Loaded ViT-H-14 model config.
126: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
126: Loaded ViT-H-14 model config.
124: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
124: Loaded ViT-H-14 model config.
123: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
123: Loaded ViT-H-14 model config.
 81: making attention of type 'vanilla' with 512 in_channels
 49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 49: Loaded ViT-H-14 model config.
122: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
122: Loaded ViT-H-14 model config.
 80: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 80: Loaded ViT-H-14 model config.
 51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 51: Loaded ViT-H-14 model config.
 50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 50: Loaded ViT-H-14 model config.
 52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 52: Loaded ViT-H-14 model config.
 53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 83: making attention of type 'vanilla' with 512 in_channels
 53: Loaded ViT-H-14 model config.
 85: making attention of type 'vanilla' with 512 in_channels
 81: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 55: Loaded ViT-H-14 model config.
 54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 54: Loaded ViT-H-14 model config.
 86: making attention of type 'vanilla' with 512 in_channels
 84: making attention of type 'vanilla' with 512 in_channels
 82: making attention of type 'vanilla' with 512 in_channels
 83: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 85: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 81: making attention of type 'vanilla' with 512 in_channels
 87: making attention of type 'vanilla' with 512 in_channels
 86: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 84: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 83: making attention of type 'vanilla' with 512 in_channels
 85: making attention of type 'vanilla' with 512 in_channels
 82: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 87: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 13: Loaded ViT-H-14 model config.
 86: making attention of type 'vanilla' with 512 in_channels
 84: making attention of type 'vanilla' with 512 in_channels
  9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  9: Loaded ViT-H-14 model config.
 82: making attention of type 'vanilla' with 512 in_channels
 12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 12: Loaded ViT-H-14 model config.
 14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 14: Loaded ViT-H-14 model config.
 10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 10: Loaded ViT-H-14 model config.
 11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 11: Loaded ViT-H-14 model config.
  4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  4: Loaded ViT-H-14 model config.
 87: making attention of type 'vanilla' with 512 in_channels
 15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 15: Loaded ViT-H-14 model config.
 36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 36: Loaded ViT-H-14 model config.
  2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  2: Loaded ViT-H-14 model config.
  7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  7: Loaded ViT-H-14 model config.
 18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 18: Loaded ViT-H-14 model config.
 37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 37: Loaded ViT-H-14 model config.
 17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 17: Loaded ViT-H-14 model config.
 38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 38: Loaded ViT-H-14 model config.
  5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  5: Loaded ViT-H-14 model config.
 39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 39: Loaded ViT-H-14 model config.
 33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 33: Loaded ViT-H-14 model config.
 35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 35: Loaded ViT-H-14 model config.
 19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 19: Loaded ViT-H-14 model config.
  1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  1: Loaded ViT-H-14 model config.
  6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  6: Loaded ViT-H-14 model config.
 20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 20: Loaded ViT-H-14 model config.
 23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 23: Loaded ViT-H-14 model config.
  3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  3: Loaded ViT-H-14 model config.
 22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 22: Loaded ViT-H-14 model config.
 21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 21: Loaded ViT-H-14 model config.
 34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 34: Loaded ViT-H-14 model config.
113: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
113: Loaded ViT-H-14 model config.
114: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
114: Loaded ViT-H-14 model config.
115: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
115: Loaded ViT-H-14 model config.
118: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
118: Loaded ViT-H-14 model config.
117: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
117: Loaded ViT-H-14 model config.
116: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
116: Loaded ViT-H-14 model config.
119: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
119: Loaded ViT-H-14 model config.
 81: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 81: Loaded ViT-H-14 model config.
 83: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 85: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 83: Loaded ViT-H-14 model config.
 85: Loaded ViT-H-14 model config.
 86: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 86: Loaded ViT-H-14 model config.
 84: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 84: Loaded ViT-H-14 model config.
 82: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 82: Loaded ViT-H-14 model config.
 87: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 87: Loaded ViT-H-14 model config.
  0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
104: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 72: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 96: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 64: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 88: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
120: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 76: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 97: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 77: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 79: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 99: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 93: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 92: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 73: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 98: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 74: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
101: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 78: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 94: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
103: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
108: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
100: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 95: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 68: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 69: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 70: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
102: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 67: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
110: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 65: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
109: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 89: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
105: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 71: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
107: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 66: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 90: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
111: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 91: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 75: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
106: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
112: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
125: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
121: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
126: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
127: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
122: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
123: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
124: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 80: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:25:16 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
  0: [NeMo I 2024-05-10 05:25:16 ddpm:261] It has 1242 entries
 50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:25:16 ddpm:262] Existing model has 1240 entries
  0: [NeMo I 2024-05-10 05:25:16 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
118: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
114: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
117: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
115: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
113: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
116: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
119: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 81: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 85: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 83: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 84: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 86: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 82: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 87: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:25:16 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
  0: [NeMo I 2024-05-10 05:25:16 ddpm:303] Missing Keys: ['model.diffusion_model._orig_mod.time_embed.0.weight', 'model.diffusion_model._orig_mod.time_embed.0.bias', 'model.diffusion_model._orig_mod.time_embed.2.weight', 'model.diffusion_model._orig_mod.time_embed.2.bias', 'model.diffusion_model._orig_mod.input_blocks.0.0.weight', 'model.diffusion_model._orig_mod.input_blocks.0.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.weight', 'mo
  0: del.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.tr
  0: ansformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffus
  0: ion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1
  0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transf
  0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
  0: ocks.2.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.
  0: _orig_mod.input_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.tra
  0: nsformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_mo
  0: del._orig_mod.input_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_in.weight', 'model.dif
  0: fusion_model._orig_mod.input_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.trans
  0: former_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.6.0.op.weight', 'mo
  0: del.diffusion_model._orig_mod.input_blocks.6.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1
  0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transf
  0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
  0: ocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.in
  0: put_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.tran
  0: sformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.bias', 'model.diffusion_model._orig_mod.
  0: input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_la
  0: yers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.2.weight', 'model.diffusion_model._orig_mo
  0: d.middle_block.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.norm.weight', 'model.diffusion_model._orig_mod.middle_block.1.norm.bias', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_
  0: model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.bias', 'model.
  0: diffusion_model._orig_mod.middle_block.1.proj_out.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_out.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.weight', '
  0: model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion
  0: _model._orig_mod.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_m
  0: od.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layer
  0: s.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.
  0: 0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diff
  0: usion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out
  0: _layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.t
  0: o_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'mode
  0: l.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5
  0: .0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.a
  0: ttn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.bias',
  0:  'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_lay
  0: ers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.ou
  0: tput_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.outp
  0: ut_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.e
  0: mb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_
  0: mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mo
  0: d.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
  0: .8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.
  0: _orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._o
  0: rig_mod.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_l
  0: ayers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.a
  0: ttn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_
  0: out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
  0: .10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.
  0: 1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.
  0: 10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.weight', 'm
  0: odel.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model
  0: .diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'mo
  0: del.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.bias', 'model.diffusion_model._orig_mod.out.0.weight', 'model.diffusion_model._orig_mod.out.0.bias', 'model.diffusion_model._orig_mod.out.1.weight', 'model.diffusion_model._orig_mod.
  0: out.1.bias']
  0: [NeMo I 2024-05-10 05:25:16 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
  0: [NeMo W 2024-05-10 05:25:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronLatentDiffusion.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
  0:     
  0: [NeMo W 2024-05-10 05:25:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
  0:     
  0: [rank: 0] Seed set to 4089765090
  0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
  1: [rank: 1] Seed set to 2417075840
  1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/128
  4: [rank: 4] Seed set to 1375175387
  4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/128
 56: [rank: 56] Seed set to 1946513531
 56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/128
 63: [rank: 63] Seed set to 2235360652
 59: [rank: 59] Seed set to 3866810313
 61: [rank: 61] Seed set to 2840660371
101: [rank: 101] Seed set to 2703349712
 47: [rank: 47] Seed set to 3886397924
 63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/128
 59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/128
 61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/128
 45: [rank: 45] Seed set to 2608055906
101: Initializing distributed: GLOBAL_RANK: 101, MEMBER: 102/128
 47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/128
121: [rank: 121] Seed set to 3047385234
 45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/128
 65: [rank: 65] Seed set to 3610900043
111: [rank: 111] Seed set to 1281592110
121: Initializing distributed: GLOBAL_RANK: 121, MEMBER: 122/128
 65: Initializing distributed: GLOBAL_RANK: 65, MEMBER: 66/128
111: Initializing distributed: GLOBAL_RANK: 111, MEMBER: 112/128
 16: [rank: 16] Seed set to 876118220
 16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/128
 39: [rank: 39] Seed set to 614598288
 39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/128
 36: [rank: 36] Seed set to 2582644590
104: [rank: 104] Seed set to 882194848
104: Initializing distributed: GLOBAL_RANK: 104, MEMBER: 105/128
120: [rank: 120] Seed set to 100901950
120: Initializing distributed: GLOBAL_RANK: 120, MEMBER: 121/128
 36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/128
103: [rank: 103] Seed set to 2578688809
103: Initializing distributed: GLOBAL_RANK: 103, MEMBER: 104/128
 93: [rank: 93] Seed set to 2530844517
 95: [rank: 95] Seed set to 1981670824
 93: Initializing distributed: GLOBAL_RANK: 93, MEMBER: 94/128
 95: Initializing distributed: GLOBAL_RANK: 95, MEMBER: 96/128
102: [rank: 102] Seed set to 2532378247
102: Initializing distributed: GLOBAL_RANK: 102, MEMBER: 103/128
 64: [rank: 64] Seed set to 2729507595
 64: Initializing distributed: GLOBAL_RANK: 64, MEMBER: 65/128
 13: [rank: 13] Seed set to 2641488642
 98: [rank: 98] Seed set to 1139858748
 98: Initializing distributed: GLOBAL_RANK: 98, MEMBER: 99/128
 57: [rank: 57] Seed set to 597309169
 57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/128
 42: [rank: 42] Seed set to 3852113382
 42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/128
 58: [rank: 58] Seed set to 4047496540
 58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/128
 13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/128
 62: [rank: 62] Seed set to 502901605
 62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/128
 43: [rank: 43] Seed set to 2173726162
 43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/128
 89: [rank: 89] Seed set to 2760291046
 89: Initializing distributed: GLOBAL_RANK: 89, MEMBER: 90/128
 71: [rank: 71] Seed set to 3037738097
 71: Initializing distributed: GLOBAL_RANK: 71, MEMBER: 72/128
 96: [rank: 96] Seed set to 221845120
105: [rank: 105] Seed set to 3874583575
 96: Initializing distributed: GLOBAL_RANK: 96, MEMBER: 97/128
105: Initializing distributed: GLOBAL_RANK: 105, MEMBER: 106/128
 66: [rank: 66] Seed set to 2049636199
 66: Initializing distributed: GLOBAL_RANK: 66, MEMBER: 67/128
 87: [rank: 87] Seed set to 3644619695
106: [rank: 106] Seed set to 884692383
106: Initializing distributed: GLOBAL_RANK: 106, MEMBER: 107/128
114: [rank: 114] Seed set to 656032849
 87: Initializing distributed: GLOBAL_RANK: 87, MEMBER: 88/128
109: [rank: 109] Seed set to 4242092397
110: [rank: 110] Seed set to 766779084
108: [rank: 108] Seed set to 1351948933
109: Initializing distributed: GLOBAL_RANK: 109, MEMBER: 110/128
110: Initializing distributed: GLOBAL_RANK: 110, MEMBER: 111/128
108: Initializing distributed: GLOBAL_RANK: 108, MEMBER: 109/128
 88: [rank: 88] Seed set to 1906270112
 88: Initializing distributed: GLOBAL_RANK: 88, MEMBER: 89/128
114: Initializing distributed: GLOBAL_RANK: 114, MEMBER: 115/128
 99: [rank: 99] Seed set to 1017464052
 99: Initializing distributed: GLOBAL_RANK: 99, MEMBER: 100/128
 76: [rank: 76] Seed set to 2974853376
 76: Initializing distributed: GLOBAL_RANK: 76, MEMBER: 77/128
  8: [rank: 8] Seed set to 3348302011
  8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/128
123: [rank: 123] Seed set to 149591050
123: Initializing distributed: GLOBAL_RANK: 123, MEMBER: 124/128
 75: [rank: 75] Seed set to 3348392821
 75: Initializing distributed: GLOBAL_RANK: 75, MEMBER: 76/128
 73: [rank: 73] Seed set to 3008249453
 73: Initializing distributed: GLOBAL_RANK: 73, MEMBER: 74/128
 97: [rank: 97] Seed set to 687033324
 97: Initializing distributed: GLOBAL_RANK: 97, MEMBER: 98/128
 40: [rank: 40] Seed set to 2935245359
 40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/128
122: [rank: 122] Seed set to 1556370634
122: Initializing distributed: GLOBAL_RANK: 122, MEMBER: 123/128
 35: [rank: 35] Seed set to 152328515
 35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/128
100: [rank: 100] Seed set to 3984319633
 91: [rank: 91] Seed set to 4043994522
100: Initializing distributed: GLOBAL_RANK: 100, MEMBER: 101/128
 91: Initializing distributed: GLOBAL_RANK: 91, MEMBER: 92/128
 19: [rank: 19] Seed set to 442882522
 19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/128
 33: [rank: 33] Seed set to 1569378524
 37: [rank: 37] Seed set to 414042907
 33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/128
 37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/128
 72: [rank: 72] Seed set to 2859095957
 72: Initializing distributed: GLOBAL_RANK: 72, MEMBER: 73/128
 34: [rank: 34] Seed set to 3598202500
 34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/128
 17: [rank: 17] Seed set to 1157201996
 18: [rank: 18] Seed set to 290801586
 17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/128
 18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/128
 90: [rank: 90] Seed set to 1664780362
 90: Initializing distributed: GLOBAL_RANK: 90, MEMBER: 91/128
 22: [rank: 22] Seed set to 4011280408
 92: [rank: 92] Seed set to 1117593148
 22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/128
 94: [rank: 94] Seed set to 2837301365
 92: Initializing distributed: GLOBAL_RANK: 92, MEMBER: 93/128
 94: Initializing distributed: GLOBAL_RANK: 94, MEMBER: 95/128
 10: [rank: 10] Seed set to 3311779071
 10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/128
 67: [rank: 67] Seed set to 953859159
 14: [rank: 14] Seed set to 3349570028
 67: Initializing distributed: GLOBAL_RANK: 67, MEMBER: 68/128
 14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/128
  7: [rank: 7] Seed set to 2037256848
  7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/128
107: [rank: 107] Seed set to 2394909498
107: Initializing distributed: GLOBAL_RANK: 107, MEMBER: 108/128
 41: [rank: 41] Seed set to 315430577
 41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/128
 60: [rank: 60] Seed set to 3262689613
 60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/128
112: [rank: 112] Seed set to 2923758506
112: Initializing distributed: GLOBAL_RANK: 112, MEMBER: 113/128
 27: [rank: 27] Seed set to 1002908684
 27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/128
 38: [rank: 38] Seed set to 3341674190
 38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/128
 12: [rank: 12] Seed set to 4161492257
 12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/128
 68: [rank: 68] Seed set to 844553083
 68: Initializing distributed: GLOBAL_RANK: 68, MEMBER: 69/128
  6: [rank: 6] Seed set to 2846866878
  6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/128
125: [rank: 125] Seed set to 2262112129
127: [rank: 127] Seed set to 3955963218
125: Initializing distributed: GLOBAL_RANK: 125, MEMBER: 126/128
127: Initializing distributed: GLOBAL_RANK: 127, MEMBER: 128/128
 46: [rank: 46] Seed set to 3844830177
 46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/128
 69: [rank: 69] Seed set to 1382312270
 70: [rank: 70] Seed set to 4028175771
 69: Initializing distributed: GLOBAL_RANK: 69, MEMBER: 70/128
 70: Initializing distributed: GLOBAL_RANK: 70, MEMBER: 71/128
  3: [rank: 3] Seed set to 4090577753
  3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/128
 79: [rank: 79] Seed set to 2670112453
 79: Initializing distributed: GLOBAL_RANK: 79, MEMBER: 80/128
 50: [rank: 50] Seed set to 317457671
 50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/128
 80: [rank: 80] Seed set to 3862671085
 80: Initializing distributed: GLOBAL_RANK: 80, MEMBER: 81/128
  5: [rank: 5] Seed set to 1315517468
  5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/128
 44: [rank: 44] Seed set to 702139936
 44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/128
126: [rank: 126] Seed set to 3135032266
126: Initializing distributed: GLOBAL_RANK: 126, MEMBER: 127/128
124: [rank: 124] Seed set to 2594302258
124: Initializing distributed: GLOBAL_RANK: 124, MEMBER: 125/128
 31: [rank: 31] Seed set to 4125394782
 31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/128
 77: [rank: 77] Seed set to 990143317
 77: Initializing distributed: GLOBAL_RANK: 77, MEMBER: 78/128
 54: [rank: 54] Seed set to 712688923
  2: [rank: 2] Seed set to 3909146449
 54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/128
  2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/128
 49: [rank: 49] Seed set to 2316218605
 49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/128
119: [rank: 119] Seed set to 4044593993
119: Initializing distributed: GLOBAL_RANK: 119, MEMBER: 120/128
 78: [rank: 78] Seed set to 1311045648
 74: [rank: 74] Seed set to 1773616787
 78: Initializing distributed: GLOBAL_RANK: 78, MEMBER: 79/128
 74: Initializing distributed: GLOBAL_RANK: 74, MEMBER: 75/128
 48: [rank: 48] Seed set to 2320252698
 48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/128
116: [rank: 116] Seed set to 2363975741
116: Initializing distributed: GLOBAL_RANK: 116, MEMBER: 117/128
 11: [rank: 11] Seed set to 1743861090
 11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/128
  9: [rank: 9] Seed set to 2634961644
  9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/128
 32: [rank: 32] Seed set to 2103003229
 32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/128
 23: [rank: 23] Seed set to 3991113728
 23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/128
 24: [rank: 24] Seed set to 4062462410
 24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/128
 81: [rank: 81] Seed set to 3422979223
 81: Initializing distributed: GLOBAL_RANK: 81, MEMBER: 82/128
 15: [rank: 15] Seed set to 3178770128
 20: [rank: 20] Seed set to 636015518
 15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/128
 20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/128
 21: [rank: 21] Seed set to 1632650000
 21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/128
 82: [rank: 82] Seed set to 3931028733
 82: Initializing distributed: GLOBAL_RANK: 82, MEMBER: 83/128
118: [rank: 118] Seed set to 1991396656
118: Initializing distributed: GLOBAL_RANK: 118, MEMBER: 119/128
 55: [rank: 55] Seed set to 2139109406
 55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/128
 53: [rank: 53] Seed set to 3950649061
 53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/128
117: [rank: 117] Seed set to 3406208563
117: Initializing distributed: GLOBAL_RANK: 117, MEMBER: 118/128
115: [rank: 115] Seed set to 3920121379
 51: [rank: 51] Seed set to 1820637060
115: Initializing distributed: GLOBAL_RANK: 115, MEMBER: 116/128
 51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/128
 52: [rank: 52] Seed set to 3030731942
 52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/128
113: [rank: 113] Seed set to 326186579
113: Initializing distributed: GLOBAL_RANK: 113, MEMBER: 114/128
 83: [rank: 83] Seed set to 3822563035
 83: Initializing distributed: GLOBAL_RANK: 83, MEMBER: 84/128
 30: [rank: 30] Seed set to 2305580529
 30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/128
 25: [rank: 25] Seed set to 444426714
 25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/128
 29: [rank: 29] Seed set to 4233751806
 29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/128
 26: [rank: 26] Seed set to 3368430677
 26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/128
 28: [rank: 28] Seed set to 251092894
 28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/128
 85: [rank: 85] Seed set to 1700048119
 85: Initializing distributed: GLOBAL_RANK: 85, MEMBER: 86/128
 86: [rank: 86] Seed set to 1973534660
 86: Initializing distributed: GLOBAL_RANK: 86, MEMBER: 87/128
 84: [rank: 84] Seed set to 521739848
 84: Initializing distributed: GLOBAL_RANK: 84, MEMBER: 85/128
  0: ----------------------------------------------------------------------------------------------------
  0: distributed_backend=nccl
  0: All distributed processes registered. Starting with 128 processes
  0: ----------------------------------------------------------------------------------------------------
  0: 
  0: [NeMo I 2024-05-10 05:25:26 main:199] Warmup allreduce with communicator at 7dd22e1ebeb0, size 16
  1: NCCL version 2.21.5+cuda12.4
  2: NCCL version 2.21.5+cuda12.4
  3: NCCL version 2.21.5+cuda12.4
  0: NCCL version 2.21.5+cuda12.4
  4: NCCL version 2.21.5+cuda12.4
  5: NCCL version 2.21.5+cuda12.4
  6: NCCL version 2.21.5+cuda12.4
  7: NCCL version 2.21.5+cuda12.4
  9: NCCL version 2.21.5+cuda12.4
 10: NCCL version 2.21.5+cuda12.4
 11: NCCL version 2.21.5+cuda12.4
 12: NCCL version 2.21.5+cuda12.4
  8: NCCL version 2.21.5+cuda12.4
 13: NCCL version 2.21.5+cuda12.4
 14: NCCL version 2.21.5+cuda12.4
 15: NCCL version 2.21.5+cuda12.4
 16: NCCL version 2.21.5+cuda12.4
 22: NCCL version 2.21.5+cuda12.4
 23: NCCL version 2.21.5+cuda12.4
 21: NCCL version 2.21.5+cuda12.4
 20: NCCL version 2.21.5+cuda12.4
 19: NCCL version 2.21.5+cuda12.4
 18: NCCL version 2.21.5+cuda12.4
 17: NCCL version 2.21.5+cuda12.4
 25: NCCL version 2.21.5+cuda12.4
 26: NCCL version 2.21.5+cuda12.4
 27: NCCL version 2.21.5+cuda12.4
 28: NCCL version 2.21.5+cuda12.4
 29: NCCL version 2.21.5+cuda12.4
 30: NCCL version 2.21.5+cuda12.4
 31: NCCL version 2.21.5+cuda12.4
 24: NCCL version 2.21.5+cuda12.4
 32: NCCL version 2.21.5+cuda12.4
 38: NCCL version 2.21.5+cuda12.4
 39: NCCL version 2.21.5+cuda12.4
 37: NCCL version 2.21.5+cuda12.4
 36: NCCL version 2.21.5+cuda12.4
 35: NCCL version 2.21.5+cuda12.4
 33: NCCL version 2.21.5+cuda12.4
 34: NCCL version 2.21.5+cuda12.4
 41: NCCL version 2.21.5+cuda12.4
 42: NCCL version 2.21.5+cuda12.4
 43: NCCL version 2.21.5+cuda12.4
 40: NCCL version 2.21.5+cuda12.4
 44: NCCL version 2.21.5+cuda12.4
 45: NCCL version 2.21.5+cuda12.4
 46: NCCL version 2.21.5+cuda12.4
 47: NCCL version 2.21.5+cuda12.4
 48: NCCL version 2.21.5+cuda12.4
 54: NCCL version 2.21.5+cuda12.4
 55: NCCL version 2.21.5+cuda12.4
 53: NCCL version 2.21.5+cuda12.4
 51: NCCL version 2.21.5+cuda12.4
 52: NCCL version 2.21.5+cuda12.4
 49: NCCL version 2.21.5+cuda12.4
 50: NCCL version 2.21.5+cuda12.4
 57: NCCL version 2.21.5+cuda12.4
 58: NCCL version 2.21.5+cuda12.4
 59: NCCL version 2.21.5+cuda12.4
 60: NCCL version 2.21.5+cuda12.4
 61: NCCL version 2.21.5+cuda12.4
 62: NCCL version 2.21.5+cuda12.4
 63: NCCL version 2.21.5+cuda12.4
 56: NCCL version 2.21.5+cuda12.4
 64: NCCL version 2.21.5+cuda12.4
 71: NCCL version 2.21.5+cuda12.4
 70: NCCL version 2.21.5+cuda12.4
 69: NCCL version 2.21.5+cuda12.4
 67: NCCL version 2.21.5+cuda12.4
 68: NCCL version 2.21.5+cuda12.4
 65: NCCL version 2.21.5+cuda12.4
 66: NCCL version 2.21.5+cuda12.4
 73: NCCL version 2.21.5+cuda12.4
 74: NCCL version 2.21.5+cuda12.4
 75: NCCL version 2.21.5+cuda12.4
 76: NCCL version 2.21.5+cuda12.4
 78: NCCL version 2.21.5+cuda12.4
 79: NCCL version 2.21.5+cuda12.4
 77: NCCL version 2.21.5+cuda12.4
 72: NCCL version 2.21.5+cuda12.4
 80: NCCL version 2.21.5+cuda12.4
 87: NCCL version 2.21.5+cuda12.4
 86: NCCL version 2.21.5+cuda12.4
 85: NCCL version 2.21.5+cuda12.4
 83: NCCL version 2.21.5+cuda12.4
 81: NCCL version 2.21.5+cuda12.4
 82: NCCL version 2.21.5+cuda12.4
 84: NCCL version 2.21.5+cuda12.4
 89: NCCL version 2.21.5+cuda12.4
 90: NCCL version 2.21.5+cuda12.4
 91: NCCL version 2.21.5+cuda12.4
 92: NCCL version 2.21.5+cuda12.4
 93: NCCL version 2.21.5+cuda12.4
 94: NCCL version 2.21.5+cuda12.4
 95: NCCL version 2.21.5+cuda12.4
 88: NCCL version 2.21.5+cuda12.4
103: NCCL version 2.21.5+cuda12.4
 96: NCCL version 2.21.5+cuda12.4
102: NCCL version 2.21.5+cuda12.4
100: NCCL version 2.21.5+cuda12.4
101: NCCL version 2.21.5+cuda12.4
 99: NCCL version 2.21.5+cuda12.4
 98: NCCL version 2.21.5+cuda12.4
 97: NCCL version 2.21.5+cuda12.4
105: NCCL version 2.21.5+cuda12.4
106: NCCL version 2.21.5+cuda12.4
107: NCCL version 2.21.5+cuda12.4
108: NCCL version 2.21.5+cuda12.4
109: NCCL version 2.21.5+cuda12.4
110: NCCL version 2.21.5+cuda12.4
104: NCCL version 2.21.5+cuda12.4
111: NCCL version 2.21.5+cuda12.4
112: NCCL version 2.21.5+cuda12.4
119: NCCL version 2.21.5+cuda12.4
118: NCCL version 2.21.5+cuda12.4
117: NCCL version 2.21.5+cuda12.4
115: NCCL version 2.21.5+cuda12.4
116: NCCL version 2.21.5+cuda12.4
114: NCCL version 2.21.5+cuda12.4
113: NCCL version 2.21.5+cuda12.4
121: NCCL version 2.21.5+cuda12.4
122: NCCL version 2.21.5+cuda12.4
123: NCCL version 2.21.5+cuda12.4
120: NCCL version 2.21.5+cuda12.4
125: NCCL version 2.21.5+cuda12.4
126: NCCL version 2.21.5+cuda12.4
127: NCCL version 2.21.5+cuda12.4
124: NCCL version 2.21.5+cuda12.4
  0: [NeMo I 2024-05-10 05:25:38 ddpm:1977] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 8.66e+08. Total number of model parameters: 8.66e+08.
  0: [NeMo I 2024-05-10 05:25:38 ddpm:2005] Building datasets for Stable Diffusion...
  0: [NeMo I 2024-05-10 05:25:38 webdataset:145] Read Webdataset locally. Data stores at /datasets/laion-400m/webdataset-moments-filtered-encoded
  0: [NeMo I 2024-05-10 05:25:38 webdataset:221] Setting nbatches=406 for infinite sampler. world_size=128
  0: [NeMo I 2024-05-10 05:25:38 webdataset:224] Total number of training shards: 832
  0: [NeMo I 2024-05-10 05:25:38 webdataset:225] Total training key count: 832000
  0: [NeMo I 2024-05-10 05:25:38 ddpm:2025] Length of train dataset: 832000
  0: [NeMo I 2024-05-10 05:25:38 ddpm:2030] Finished building datasets for LatentDiffusion.
  0: [NeMo I 2024-05-10 05:25:38 ddpm:2036] Setting up train dataloader with len(len(self._train_ds)): 832000 and consumed samples: 0
  0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 72: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
104: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 96: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 64: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 80: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
120: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 88: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
112: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 73: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
105: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 97: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 81: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
121: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 89: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
113: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 65: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 74: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
122: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
106: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 98: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
107: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
114: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 90: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 82: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 75: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
108: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
109: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
123: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
115: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 83: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 76: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 99: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 77: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 66: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
110: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 92: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
124: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 67: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
111: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 78: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 91: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 84: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
116: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
125: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
100: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 79: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
117: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 85: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 68: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
101: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
126: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 93: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 94: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 95: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 69: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
102: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 70: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
103: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 71: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
118: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
127: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 86: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 87: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
119: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  0: [NeMo I 2024-05-10 05:25:38 modelPT:724] Optimizer config = MegatronDistributedFusedAdam (
  0:     Parameter Group 0
  0:         betas: [0.9, 0.999]
  0:         bias_correction: True
  0:         eps: 1e-08
  0:         lr: 0.00010240000119665638
  0:         weight_decay: 0.0
  0:     adam_w_mode: True
  0:     amsgrad: False
  0:     dtype: torch.float32
  0:     grad_sync_dtype: torch.float16
  0:     param_sync_dtype: torch.float16
  0:     device: cuda:0
  0:     process_group: 0x7dd22e2809f0, world size 128
  0:     distributed_process_group: 0x7dd22e280bb0, world size 8
  0:     redundant_process_group: 0x7dd22e1ebeb0, world size 16
  0:     average_grad_sync: True
  0:     overlap_grad_sync: True
  0:     overlap_param_sync: False
  0:     bucket_cap_mb: 288
  0:     pipeline_size: 2
  0:     contiguous_param_buffer: True
  0:     contiguous_grad_buffer: True
  0:     store_params: True
  0:     store_param_remainders: False
  0:     with_scaled_states: False
  0:     nccl_ub: False
  0:     capturable: True
  0:     )
  0: [NeMo I 2024-05-10 05:25:38 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.WarmupHoldPolicy object at 0x7dd22c58fd30>" 
  0:     will be used during training (effective maximum steps = 3500) - 
  0:     Parameters : 
  0:     (warmup_steps: 1000
  0:     hold_steps: 10000000000000
  0:     max_steps: 3500
  0:     )
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739015, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 249}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739159, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 1024, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 253}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739159, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 257}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739159, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 258}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739159, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 259}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 260}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 261}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0001024, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 263}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 264}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 6513144, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 269}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 270}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318739160, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
  0: 
  0:   | Name  | Type            | Params
  0: ------------------------------------------
  0: 0 | model | LatentDiffusion | 865 M 
  0: ------------------------------------------
  0: 865 M     Trainable params
  0: 0         Non-trainable params
  0: 865 M     Total params
  0: 3,463.643 Total estimated model params size (MB)
  0: SLURM auto-requeueing enabled. Setting signal handlers.
 96: SLURM auto-requeueing enabled. Setting signal handlers.
 24: SLURM auto-requeueing enabled. Setting signal handlers.
 64: SLURM auto-requeueing enabled. Setting signal handlers.
 80: SLURM auto-requeueing enabled. Setting signal handlers.
 32: SLURM auto-requeueing enabled. Setting signal handlers.
112: SLURM auto-requeueing enabled. Setting signal handlers.
  1: SLURM auto-requeueing enabled. Setting signal handlers.
 72: SLURM auto-requeueing enabled. Setting signal handlers.
120: SLURM auto-requeueing enabled. Setting signal handlers.
  8: SLURM auto-requeueing enabled. Setting signal handlers.
 16: SLURM auto-requeueing enabled. Setting signal handlers.
 88: SLURM auto-requeueing enabled. Setting signal handlers.
 56: SLURM auto-requeueing enabled. Setting signal handlers.
104: SLURM auto-requeueing enabled. Setting signal handlers.
  2: SLURM auto-requeueing enabled. Setting signal handlers.
 81: SLURM auto-requeueing enabled. Setting signal handlers.
 25: SLURM auto-requeueing enabled. Setting signal handlers.
 65: SLURM auto-requeueing enabled. Setting signal handlers.
 33: SLURM auto-requeueing enabled. Setting signal handlers.
113: SLURM auto-requeueing enabled. Setting signal handlers.
 73: SLURM auto-requeueing enabled. Setting signal handlers.
 40: SLURM auto-requeueing enabled. Setting signal handlers.
 17: SLURM auto-requeueing enabled. Setting signal handlers.
 57: SLURM auto-requeueing enabled. Setting signal handlers.
 97: SLURM auto-requeueing enabled. Setting signal handlers.
121: SLURM auto-requeueing enabled. Setting signal handlers.
 48: SLURM auto-requeueing enabled. Setting signal handlers.
  9: SLURM auto-requeueing enabled. Setting signal handlers.
 89: SLURM auto-requeueing enabled. Setting signal handlers.
 82: SLURM auto-requeueing enabled. Setting signal handlers.
105: SLURM auto-requeueing enabled. Setting signal handlers.
 74: SLURM auto-requeueing enabled. Setting signal handlers.
122: SLURM auto-requeueing enabled. Setting signal handlers.
 98: SLURM auto-requeueing enabled. Setting signal handlers.
  3: SLURM auto-requeueing enabled. Setting signal handlers.
 49: SLURM auto-requeueing enabled. Setting signal handlers.
 83: SLURM auto-requeueing enabled. Setting signal handlers.
 18: SLURM auto-requeueing enabled. Setting signal handlers.
 34: SLURM auto-requeueing enabled. Setting signal handlers.
 90: SLURM auto-requeueing enabled. Setting signal handlers.
 26: SLURM auto-requeueing enabled. Setting signal handlers.
114: SLURM auto-requeueing enabled. Setting signal handlers.
 10: SLURM auto-requeueing enabled. Setting signal handlers.
 58: SLURM auto-requeueing enabled. Setting signal handlers.
123: SLURM auto-requeueing enabled. Setting signal handlers.
106: SLURM auto-requeueing enabled. Setting signal handlers.
 50: SLURM auto-requeueing enabled. Setting signal handlers.
 51: SLURM auto-requeueing enabled. Setting signal handlers.
 41: SLURM auto-requeueing enabled. Setting signal handlers.
 19: SLURM auto-requeueing enabled. Setting signal handlers.
 66: SLURM auto-requeueing enabled. Setting signal handlers.
115: SLURM auto-requeueing enabled. Setting signal handlers.
 75: SLURM auto-requeueing enabled. Setting signal handlers.
 99: SLURM auto-requeueing enabled. Setting signal handlers.
124: SLURM auto-requeueing enabled. Setting signal handlers.
 35: SLURM auto-requeueing enabled. Setting signal handlers.
 52: SLURM auto-requeueing enabled. Setting signal handlers.
107: SLURM auto-requeueing enabled. Setting signal handlers.
 11: SLURM auto-requeueing enabled. Setting signal handlers.
 42: SLURM auto-requeueing enabled. Setting signal handlers.
 91: SLURM auto-requeueing enabled. Setting signal handlers.
 59: SLURM auto-requeueing enabled. Setting signal handlers.
 27: SLURM auto-requeueing enabled. Setting signal handlers.
  4: SLURM auto-requeueing enabled. Setting signal handlers.
116: SLURM auto-requeueing enabled. Setting signal handlers.
125: SLURM auto-requeueing enabled. Setting signal handlers.
 20: SLURM auto-requeueing enabled. Setting signal handlers.
 84: SLURM auto-requeueing enabled. Setting signal handlers.
 53: SLURM auto-requeueing enabled. Setting signal handlers.
 21: SLURM auto-requeueing enabled. Setting signal handlers.
 85: SLURM auto-requeueing enabled. Setting signal handlers.
117: SLURM auto-requeueing enabled. Setting signal handlers.
  5: SLURM auto-requeueing enabled. Setting signal handlers.
 76: SLURM auto-requeueing enabled. Setting signal handlers.
126: SLURM auto-requeueing enabled. Setting signal handlers.
 67: SLURM auto-requeueing enabled. Setting signal handlers.
 43: SLURM auto-requeueing enabled. Setting signal handlers.
 28: SLURM auto-requeueing enabled. Setting signal handlers.
100: SLURM auto-requeueing enabled. Setting signal handlers.
 36: SLURM auto-requeueing enabled. Setting signal handlers.
 60: SLURM auto-requeueing enabled. Setting signal handlers.
108: SLURM auto-requeueing enabled. Setting signal handlers.
127: SLURM auto-requeueing enabled. Setting signal handlers.
 12: SLURM auto-requeueing enabled. Setting signal handlers.
 86: SLURM auto-requeueing enabled. Setting signal handlers.
 54: SLURM auto-requeueing enabled. Setting signal handlers.
 77: SLURM auto-requeueing enabled. Setting signal handlers.
 22: SLURM auto-requeueing enabled. Setting signal handlers.
118: SLURM auto-requeueing enabled. Setting signal handlers.
 68: SLURM auto-requeueing enabled. Setting signal handlers.
 44: SLURM auto-requeueing enabled. Setting signal handlers.
101: SLURM auto-requeueing enabled. Setting signal handlers.
  6: SLURM auto-requeueing enabled. Setting signal handlers.
 37: SLURM auto-requeueing enabled. Setting signal handlers.
 92: SLURM auto-requeueing enabled. Setting signal handlers.
 61: SLURM auto-requeueing enabled. Setting signal handlers.
 87: SLURM auto-requeueing enabled. Setting signal handlers.
 55: SLURM auto-requeueing enabled. Setting signal handlers.
 13: SLURM auto-requeueing enabled. Setting signal handlers.
 29: SLURM auto-requeueing enabled. Setting signal handlers.
119: SLURM auto-requeueing enabled. Setting signal handlers.
 23: SLURM auto-requeueing enabled. Setting signal handlers.
109: SLURM auto-requeueing enabled. Setting signal handlers.
 93: SLURM auto-requeueing enabled. Setting signal handlers.
 78: SLURM auto-requeueing enabled. Setting signal handlers.
  7: SLURM auto-requeueing enabled. Setting signal handlers.
 38: SLURM auto-requeueing enabled. Setting signal handlers.
 62: SLURM auto-requeueing enabled. Setting signal handlers.
102: SLURM auto-requeueing enabled. Setting signal handlers.
 69: SLURM auto-requeueing enabled. Setting signal handlers.
110: SLURM auto-requeueing enabled. Setting signal handlers.
 45: SLURM auto-requeueing enabled. Setting signal handlers.
 94: SLURM auto-requeueing enabled. Setting signal handlers.
 30: SLURM auto-requeueing enabled. Setting signal handlers.
 70: SLURM auto-requeueing enabled. Setting signal handlers.
 14: SLURM auto-requeueing enabled. Setting signal handlers.
103: SLURM auto-requeueing enabled. Setting signal handlers.
 79: SLURM auto-requeueing enabled. Setting signal handlers.
 39: SLURM auto-requeueing enabled. Setting signal handlers.
 63: SLURM auto-requeueing enabled. Setting signal handlers.
 15: SLURM auto-requeueing enabled. Setting signal handlers.
111: SLURM auto-requeueing enabled. Setting signal handlers.
 95: SLURM auto-requeueing enabled. Setting signal handlers.
 31: SLURM auto-requeueing enabled. Setting signal handlers.
 71: SLURM auto-requeueing enabled. Setting signal handlers.
 46: SLURM auto-requeueing enabled. Setting signal handlers.
 47: SLURM auto-requeueing enabled. Setting signal handlers.
  0: [NeMo W 2024-05-10 05:25:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:104: Total length of `list` across ranks is zero. Please make sure this was your intention.
  0:     
  0: [NeMo W 2024-05-10 05:25:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:121: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.
  0:     
  0: CUDAGraphCallback: disable autocast cache.
  0: [NeMo W 2024-05-10 05:27:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:       warnings.warn(
  0:     
  0: [NeMo W 2024-05-10 05:27:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('global_step', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'global_step': ...})` instead.
  0:     
  0: [NeMo W 2024-05-10 05:27:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('consumed_samples', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'consumed_samples': ...})` instead.
  0:     
  0: [NeMo I 2024-05-10 05:27:59 callbacks:158] CUDAGraphCallback: capturing CUDA graph for module MegatronLatentDiffusion.
  0: CUDAGraphCallback: set optimizer.zero_grad as nop during graph capturing.
  0: :::MLLOG {"namespace": "", "time_ms": 1715318880765, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318880766, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318880768, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318888665, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 102400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318888666, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 102400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318897315, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 204800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318897316, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 204800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318905973, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 307200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318905974, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 307200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318914640, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 409600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318914641, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 409600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318923324, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 512000}}
  0: Epoch 0, global step 500: 'timestamp' reached 1715318923308.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt' as top 1
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715318924232, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 512000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318932205, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 614400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318932207, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 614400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318940913, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 716800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318940914, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 716800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318949623, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 819200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318949730, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 819200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318958347, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 921600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318958348, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 921600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318967061, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1024000}}
  0: Epoch 1, global step 1000: 'timestamp' reached 1715318967043.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt' as top 2
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715318968080, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1024000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318976050, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1126400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318976051, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1126400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318984766, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1228800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318984766, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1228800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318993481, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1331200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715318993482, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1331200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319002199, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1433600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319002200, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1433600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319010918, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1536000}}
  0: Epoch 1, global step 1500: 'timestamp' reached 1715319010918.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt' as top 3
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715319011952, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1536000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319019925, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1638400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319020076, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1638400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319028654, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1740800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319028655, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1740800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319037373, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1843200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319037373, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1843200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319046093, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1945600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319046094, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1945600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319054814, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2048000}}
  0: Epoch 2, global step 2000: 'timestamp' reached 1715319054814.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt' as top 4
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715319055842, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2048000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319063816, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2150400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319063817, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2150400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319072541, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2252800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319072542, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2252800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319081264, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2355200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319081265, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2355200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319089985, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2457600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319090143, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2457600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319098716, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2560000}}
  0: Epoch 3, global step 2500: 'timestamp' reached 1715319098716.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt' as top 5
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715319099745, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2560000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319107722, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2662400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319107723, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2662400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319116444, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2764800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319116445, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2764800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319125169, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2867200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319125170, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2867200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319133894, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2969600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319133895, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2969600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319142618, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3072000}}
  0: Epoch 3, global step 3000: 'timestamp' reached 1715319142617.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt' as top 6
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt in the background
  0: :::MLLOG {"namespace": "", "time_ms": 1715319143646, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3072000}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319151624, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3174400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319151625, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3174400}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319160349, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3276800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319160508, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3276800}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319169085, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3379200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319169086, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3379200}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319177811, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3481600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319177812, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3481600}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319186535, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3584000}}
  0: Epoch 4, global step 3500: 'timestamp' reached 1715319186535.00000 (best 1715318923308.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt' as top 7
  0: Saving /tmp/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt in the background
  0: `Trainer.fit` stopped: `max_steps=3500` reached.
  0: Moving checkpoints to nemologs
  0: total 0
  0: drwxrwxr-x 5 ubuntu ubuntu 200 May 10 05:28 stable-diffusion2-train-240510052401216597839
  0: NCCL version 2.21.5+cuda12.4
  2: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
125: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  4: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
126: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  6: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  1: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  3: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
123: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
127: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  7: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
121: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  5: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 50: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  9: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
114: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 20: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
111: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 47: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 93: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 31: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
124: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
122: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 66: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 63: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 15: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 34: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 73: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 77: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 54: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 98: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 95: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 82: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 84: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
118: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 59: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
109: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
107: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 13: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 80: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 22: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 43: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 18: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 41: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 52: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 89: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
102: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 27: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 29: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
115: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 70: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 68: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 11: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 38: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 36: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 87: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 42: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 19: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
100: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 94: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
116: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 65: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 61: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 57: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 79: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 75: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 76: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 12: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 81: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 33: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 91: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 45: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 53: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 51: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 17: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 67: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
117: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 28: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 25: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
105: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
108: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
110: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 58: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 83: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 10: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 39: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 92: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
113: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 23: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  0: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 49: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 78: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 74: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
106: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 14: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
119: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 71: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 69: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 35: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 90: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 85: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 46: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 44: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 21: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 55: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 99: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
103: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
101: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 26: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 30: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 62: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 60: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
120: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 37: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 97: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
112: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 64: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 72: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 48: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
104: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 16: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 32: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 40: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 86: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 88: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  8: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 96: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 24: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
 56: CKPT_PATH=/nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/
  2: FlashAttention Installed
125: FlashAttention Installed
126: FlashAttention Installed
  4: FlashAttention Installed
  6: FlashAttention Installed
121: FlashAttention Installed
  1: FlashAttention Installed
  3: FlashAttention Installed
  2: Loaded ViT-H-14 model config.
  7: FlashAttention Installed
  5: FlashAttention Installed
127: FlashAttention Installed
123: FlashAttention Installed
124: FlashAttention Installed
114: FlashAttention Installed
  4: Loaded ViT-H-14 model config.
  9: FlashAttention Installed
122: FlashAttention Installed
111: FlashAttention Installed
 20: FlashAttention Installed
 50: FlashAttention Installed
 43: FlashAttention Installed
 47: FlashAttention Installed
 34: FlashAttention Installed
126: Loaded ViT-H-14 model config.
125: Loaded ViT-H-14 model config.
 80: FlashAttention Installed
121: Loaded ViT-H-14 model config.
 84: FlashAttention Installed
 98: FlashAttention Installed
 18: FlashAttention Installed
 77: FlashAttention Installed
107: FlashAttention Installed
  6: Loaded ViT-H-14 model config.
 41: FlashAttention Installed
 63: FlashAttention Installed
 12: FlashAttention Installed
 95: FlashAttention Installed
 93: FlashAttention Installed
 19: FlashAttention Installed
  1: Loaded ViT-H-14 model config.
 59: FlashAttention Installed
109: FlashAttention Installed
 38: FlashAttention Installed
 29: FlashAttention Installed
 13: FlashAttention Installed
  3: Loaded ViT-H-14 model config.
115: FlashAttention Installed
 54: FlashAttention Installed
 73: FlashAttention Installed
 52: FlashAttention Installed
 82: FlashAttention Installed
102: FlashAttention Installed
 94: FlashAttention Installed
 61: FlashAttention Installed
 66: FlashAttention Installed
 70: FlashAttention Installed
116: FlashAttention Installed
100: FlashAttention Installed
 27: FlashAttention Installed
 87: FlashAttention Installed
108: FlashAttention Installed
 45: FlashAttention Installed
  7: Loaded ViT-H-14 model config.
 22: FlashAttention Installed
  5: Loaded ViT-H-14 model config.
 65: FlashAttention Installed
 25: FlashAttention Installed
 57: FlashAttention Installed
 42: FlashAttention Installed
 89: FlashAttention Installed
 51: FlashAttention Installed
118: FlashAttention Installed
 28: FlashAttention Installed
 31: FlashAttention Installed
 58: FlashAttention Installed
  9: Loaded ViT-H-14 model config.
 81: FlashAttention Installed
 10: FlashAttention Installed
 60: FlashAttention Installed
111: Loaded ViT-H-14 model config.
114: Loaded ViT-H-14 model config.
117: FlashAttention Installed
 20: Loaded ViT-H-14 model config.
 33: FlashAttention Installed
106: FlashAttention Installed
 92: FlashAttention Installed
 23: FlashAttention Installed
 49: FlashAttention Installed
 55: FlashAttention Installed
101: FlashAttention Installed
 85: FlashAttention Installed
 44: FlashAttention Installed
 11: FlashAttention Installed
127: Loaded ViT-H-14 model config.
124: Loaded ViT-H-14 model config.
122: Loaded ViT-H-14 model config.
123: Loaded ViT-H-14 model config.
110: FlashAttention Installed
 46: FlashAttention Installed
 36: FlashAttention Installed
 83: FlashAttention Installed
119: FlashAttention Installed
120: FlashAttention Installed
 67: FlashAttention Installed
 76: FlashAttention Installed
 77: Loaded ViT-H-14 model config.
 75: FlashAttention Installed
 68: FlashAttention Installed
 35: FlashAttention Installed
 14: FlashAttention Installed
 39: FlashAttention Installed
 90: FlashAttention Installed
 62: FlashAttention Installed
 17: FlashAttention Installed
 98: Loaded ViT-H-14 model config.
 91: FlashAttention Installed
 50: Loaded ViT-H-14 model config.
 18: Loaded ViT-H-14 model config.
 79: FlashAttention Installed
 15: FlashAttention Installed
 43: Loaded ViT-H-14 model config.
 71: FlashAttention Installed
 74: FlashAttention Installed
 47: Loaded ViT-H-14 model config.
107: Loaded ViT-H-14 model config.
 78: FlashAttention Installed
109: Loaded ViT-H-14 model config.
 41: Loaded ViT-H-14 model config.
 84: Loaded ViT-H-14 model config.
 30: FlashAttention Installed
 26: FlashAttention Installed
 29: Loaded ViT-H-14 model config.
 69: FlashAttention Installed
 21: FlashAttention Installed
 19: Loaded ViT-H-14 model config.
 34: Loaded ViT-H-14 model config.
115: Loaded ViT-H-14 model config.
  0: FlashAttention Installed
 38: Loaded ViT-H-14 model config.
 12: Loaded ViT-H-14 model config.
 13: Loaded ViT-H-14 model config.
  0: [NeMo W 2024-05-10 05:34:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
  0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  0:       ret = run_job(
  0:     
  0: [NeMo W 2024-05-10 05:34:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  0:       warnings.warn(
  0:     
  0: [NeMo W 2024-05-10 05:34:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  0:       warnings.warn(msg)
  0:     
103: FlashAttention Installed
 80: Loaded ViT-H-14 model config.
 99: FlashAttention Installed
 53: FlashAttention Installed
 95: Loaded ViT-H-14 model config.
105: FlashAttention Installed
108: Loaded ViT-H-14 model config.
 93: Loaded ViT-H-14 model config.
 22: Loaded ViT-H-14 model config.
116: Loaded ViT-H-14 model config.
 94: Loaded ViT-H-14 model config.
 89: Loaded ViT-H-14 model config.
 63: Loaded ViT-H-14 model config.
 54: Loaded ViT-H-14 model config.
 42: Loaded ViT-H-14 model config.
 45: Loaded ViT-H-14 model config.
 52: Loaded ViT-H-14 model config.
 73: Loaded ViT-H-14 model config.
 51: Loaded ViT-H-14 model config.
 59: Loaded ViT-H-14 model config.
112: FlashAttention Installed
118: Loaded ViT-H-14 model config.
 66: Loaded ViT-H-14 model config.
 65: Loaded ViT-H-14 model config.
 70: Loaded ViT-H-14 model config.
102: Loaded ViT-H-14 model config.
100: Loaded ViT-H-14 model config.
113: FlashAttention Installed
 92: Loaded ViT-H-14 model config.
 23: Loaded ViT-H-14 model config.
 10: Loaded ViT-H-14 model config.
 61: Loaded ViT-H-14 model config.
101: Loaded ViT-H-14 model config.
 27: Loaded ViT-H-14 model config.
 25: Loaded ViT-H-14 model config.
 28: Loaded ViT-H-14 model config.
 31: Loaded ViT-H-14 model config.
117: Loaded ViT-H-14 model config.
 49: Loaded ViT-H-14 model config.
120: Loaded ViT-H-14 model config.
 55: Loaded ViT-H-14 model config.
106: Loaded ViT-H-14 model config.
 82: Loaded ViT-H-14 model config.
119: Loaded ViT-H-14 model config.
 48: FlashAttention Installed
110: Loaded ViT-H-14 model config.
 87: Loaded ViT-H-14 model config.
 57: Loaded ViT-H-14 model config.
 58: Loaded ViT-H-14 model config.
 44: Loaded ViT-H-14 model config.
 46: Loaded ViT-H-14 model config.
 60: Loaded ViT-H-14 model config.
 32: FlashAttention Installed
 62: Loaded ViT-H-14 model config.
 72: FlashAttention Installed
 11: Loaded ViT-H-14 model config.
 37: FlashAttention Installed
 83: Loaded ViT-H-14 model config.
 81: Loaded ViT-H-14 model config.
 85: Loaded ViT-H-14 model config.
 17: Loaded ViT-H-14 model config.
 14: Loaded ViT-H-14 model config.
 15: Loaded ViT-H-14 model config.
 90: Loaded ViT-H-14 model config.
 91: Loaded ViT-H-14 model config.
 68: Loaded ViT-H-14 model config.
 67: Loaded ViT-H-14 model config.
 21: Loaded ViT-H-14 model config.
 33: Loaded ViT-H-14 model config.
 36: Loaded ViT-H-14 model config.
 39: Loaded ViT-H-14 model config.
 35: Loaded ViT-H-14 model config.
  0: Loaded ViT-H-14 model config.
 64: FlashAttention Installed
 97: FlashAttention Installed
 71: Loaded ViT-H-14 model config.
 86: FlashAttention Installed
104: FlashAttention Installed
 69: Loaded ViT-H-14 model config.
 30: Loaded ViT-H-14 model config.
 26: Loaded ViT-H-14 model config.
 96: FlashAttention Installed
 40: FlashAttention Installed
 53: Loaded ViT-H-14 model config.
 16: FlashAttention Installed
105: Loaded ViT-H-14 model config.
 88: FlashAttention Installed
103: Loaded ViT-H-14 model config.
 99: Loaded ViT-H-14 model config.
  8: FlashAttention Installed
112: Loaded ViT-H-14 model config.
 76: Loaded ViT-H-14 model config.
 74: Loaded ViT-H-14 model config.
 78: Loaded ViT-H-14 model config.
 75: Loaded ViT-H-14 model config.
 79: Loaded ViT-H-14 model config.
113: Loaded ViT-H-14 model config.
 56: FlashAttention Installed
 24: FlashAttention Installed
 48: Loaded ViT-H-14 model config.
 32: Loaded ViT-H-14 model config.
 72: Loaded ViT-H-14 model config.
 37: Loaded ViT-H-14 model config.
 64: Loaded ViT-H-14 model config.
104: Loaded ViT-H-14 model config.
 86: Loaded ViT-H-14 model config.
 97: Loaded ViT-H-14 model config.
 96: Loaded ViT-H-14 model config.
 40: Loaded ViT-H-14 model config.
 16: Loaded ViT-H-14 model config.
 88: Loaded ViT-H-14 model config.
  8: Loaded ViT-H-14 model config.
 56: Loaded ViT-H-14 model config.
 24: Loaded ViT-H-14 model config.
  2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
126: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
125: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
121: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
111: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
114: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
127: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
122: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
124: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
123: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 77: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 98: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
109: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
107: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 84: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
115: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 80: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
108: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 93: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 95: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
116: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 73: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 94: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 89: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 65: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
102: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 70: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 66: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
100: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
118: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 92: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
101: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
106: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
117: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
110: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
119: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
120: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 82: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 87: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 83: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 81: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 90: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 85: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 67: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 68: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 91: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 71: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 69: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
105: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
103: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 99: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
112: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 76: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 78: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 74: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 75: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
113: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 79: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 72: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
104: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 86: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 97: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 96: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 64: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 88: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Using 16bit Automatic Mixed Precision (AMP)
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: IPU available: False, using: 0 IPUs
  0: HPU available: False, using: 0 HPUs
  0: [NeMo W 2024-05-10 05:34:30 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
  2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: Found checkpoints:
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  0: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:279] Ranks 0 has data parallel rank: 0
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:287] Rank 0 has context parallel group: [0]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:291] Ranks 0 has context parallel rank: 0
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:298] Rank 0 has model parallel group: [0]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:308] Rank 0 has tensor model parallel group: [0]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:313] Rank 0 has tensor model parallel rank: 0
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:345] Rank 0 has embedding group: [0]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:352] Rank 0 has pipeline model parallel rank 0
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:34:31 megatron_init:354] Rank 0 has embedding rank: 0
  0: 24-05-10 05:34:31 - PID:859051 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:34:31 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [NeMo I 2024-05-10 05:34:31 ddpm:130] LatentDiffusion: Running in v-prediction mode
  0: [NeMo I 2024-05-10 05:34:31 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:34:31 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:34:32 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:34:32 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:34:32 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:34:32 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:34:33 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
124: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:34:34 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
104: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:34:35 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:34:35 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 35: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: Found checkpoints:
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  2: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  2: making attention of type 'vanilla' with 512 in_channels
  2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  2: making attention of type 'vanilla' with 512 in_channels
  2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  2: Loaded ViT-H-14 model config.
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  4: Found checkpoints:
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  4: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  4: making attention of type 'vanilla' with 512 in_channels
  4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  4: making attention of type 'vanilla' with 512 in_channels
  4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  4: Loaded ViT-H-14 model config.
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:34:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  3: Found checkpoints:
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  3: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  3: making attention of type 'vanilla' with 512 in_channels
  3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  3: making attention of type 'vanilla' with 512 in_channels
  3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  3: Loaded ViT-H-14 model config.
  6: Found checkpoints:
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  6: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  6: making attention of type 'vanilla' with 512 in_channels
  6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  6: making attention of type 'vanilla' with 512 in_channels
  6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  6: Loaded ViT-H-14 model config.
  1: Found checkpoints:
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  1: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  1: making attention of type 'vanilla' with 512 in_channels
  1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  1: making attention of type 'vanilla' with 512 in_channels
  1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  1: Loaded ViT-H-14 model config.
  5: Found checkpoints:
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  5: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  5: making attention of type 'vanilla' with 512 in_channels
  5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  5: making attention of type 'vanilla' with 512 in_channels
  5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  5: Loaded ViT-H-14 model config.
  7: Found checkpoints:
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  7: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  7: making attention of type 'vanilla' with 512 in_channels
  7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  7: making attention of type 'vanilla' with 512 in_channels
  7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  7: Loaded ViT-H-14 model config.
  0: [NeMo I 2024-05-10 05:34:37 utils:92] DiffusionWrapper has 865.91 M params.
  0: [NeMo I 2024-05-10 05:34:37 ddpm:168] Use system random generator since CUDA graph enabled
  0: making attention of type 'vanilla' with 512 in_channels
  0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  0: making attention of type 'vanilla' with 512 in_channels
  0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  0: Loaded ViT-H-14 model config.
126: Found checkpoints:
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
126: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
126: making attention of type 'vanilla' with 512 in_channels
126: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
126: making attention of type 'vanilla' with 512 in_channels
126: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
126: Loaded ViT-H-14 model config.
122: Found checkpoints:
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
122: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
122: making attention of type 'vanilla' with 512 in_channels
122: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
122: making attention of type 'vanilla' with 512 in_channels
122: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
122: Loaded ViT-H-14 model config.
124: Found checkpoints:
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
124: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
124: making attention of type 'vanilla' with 512 in_channels
124: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
124: making attention of type 'vanilla' with 512 in_channels
124: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
124: Loaded ViT-H-14 model config.
120: Found checkpoints:
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
120: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
120: making attention of type 'vanilla' with 512 in_channels
120: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
120: making attention of type 'vanilla' with 512 in_channels
120: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
120: Loaded ViT-H-14 model config.
127: Found checkpoints:
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
127: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
127: making attention of type 'vanilla' with 512 in_channels
127: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
127: making attention of type 'vanilla' with 512 in_channels
127: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
127: Loaded ViT-H-14 model config.
123: Found checkpoints:
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
123: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
123: making attention of type 'vanilla' with 512 in_channels
123: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
123: making attention of type 'vanilla' with 512 in_channels
123: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
123: Loaded ViT-H-14 model config.
125: Found checkpoints:
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
125: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
125: making attention of type 'vanilla' with 512 in_channels
125: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
125: making attention of type 'vanilla' with 512 in_channels
125: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
125: Loaded ViT-H-14 model config.
121: Found checkpoints:
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
121: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
121: making attention of type 'vanilla' with 512 in_channels
121: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
121: making attention of type 'vanilla' with 512 in_channels
121: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
121: Loaded ViT-H-14 model config.
  8: Found checkpoints:
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  8: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  8: making attention of type 'vanilla' with 512 in_channels
  8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  8: making attention of type 'vanilla' with 512 in_channels
  8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  8: Loaded ViT-H-14 model config.
  9: Found checkpoints:
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
  9: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
  9: making attention of type 'vanilla' with 512 in_channels
  9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  9: making attention of type 'vanilla' with 512 in_channels
  9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  9: Loaded ViT-H-14 model config.
 13: Found checkpoints:
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 13: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 13: making attention of type 'vanilla' with 512 in_channels
 13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 13: making attention of type 'vanilla' with 512 in_channels
 13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 13: Loaded ViT-H-14 model config.
 11: Found checkpoints:
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 11: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 11: making attention of type 'vanilla' with 512 in_channels
 11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 11: making attention of type 'vanilla' with 512 in_channels
 11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 11: Loaded ViT-H-14 model config.
 14: Found checkpoints:
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 14: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 14: making attention of type 'vanilla' with 512 in_channels
 14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 14: making attention of type 'vanilla' with 512 in_channels
 14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 14: Loaded ViT-H-14 model config.
 12: Found checkpoints:
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 12: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 12: making attention of type 'vanilla' with 512 in_channels
 12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 12: making attention of type 'vanilla' with 512 in_channels
 12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 12: Loaded ViT-H-14 model config.
 10: Found checkpoints:
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 10: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 10: making attention of type 'vanilla' with 512 in_channels
 10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 10: making attention of type 'vanilla' with 512 in_channels
 10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 10: Loaded ViT-H-14 model config.
 15: Found checkpoints:
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 15: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 15: making attention of type 'vanilla' with 512 in_channels
 15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 15: making attention of type 'vanilla' with 512 in_channels
 15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 15: Loaded ViT-H-14 model config.
106: Found checkpoints:
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
106: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
106: making attention of type 'vanilla' with 512 in_channels
106: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
106: making attention of type 'vanilla' with 512 in_channels
106: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
106: Loaded ViT-H-14 model config.
111: Found checkpoints:
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
111: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
111: making attention of type 'vanilla' with 512 in_channels
111: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
111: making attention of type 'vanilla' with 512 in_channels
111: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
111: Loaded ViT-H-14 model config.
110: Found checkpoints:
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
110: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
110: making attention of type 'vanilla' with 512 in_channels
110: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
110: making attention of type 'vanilla' with 512 in_channels
110: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
110: Loaded ViT-H-14 model config.
109: Found checkpoints:
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
109: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
109: making attention of type 'vanilla' with 512 in_channels
109: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
109: making attention of type 'vanilla' with 512 in_channels
109: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
109: Loaded ViT-H-14 model config.
108: Found checkpoints:
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
108: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
108: making attention of type 'vanilla' with 512 in_channels
108: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
108: making attention of type 'vanilla' with 512 in_channels
108: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
108: Loaded ViT-H-14 model config.
105: Found checkpoints:
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
105: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
105: making attention of type 'vanilla' with 512 in_channels
105: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
105: making attention of type 'vanilla' with 512 in_channels
105: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
105: Loaded ViT-H-14 model config.
107: Found checkpoints:
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
107: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
107: making attention of type 'vanilla' with 512 in_channels
107: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
107: making attention of type 'vanilla' with 512 in_channels
107: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
107: Loaded ViT-H-14 model config.
104: Found checkpoints:
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
104: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
104: making attention of type 'vanilla' with 512 in_channels
104: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
104: making attention of type 'vanilla' with 512 in_channels
104: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
104: Loaded ViT-H-14 model config.
100: Found checkpoints:
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
100: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
100: making attention of type 'vanilla' with 512 in_channels
100: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
100: making attention of type 'vanilla' with 512 in_channels
100: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
100: Loaded ViT-H-14 model config.
103: Found checkpoints:
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
103: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
103: making attention of type 'vanilla' with 512 in_channels
103: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
103: making attention of type 'vanilla' with 512 in_channels
103: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
103: Loaded ViT-H-14 model config.
 99: Found checkpoints:
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 99: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 99: making attention of type 'vanilla' with 512 in_channels
 99: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 99: making attention of type 'vanilla' with 512 in_channels
 99: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 99: Loaded ViT-H-14 model config.
 98: Found checkpoints:
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 98: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 98: making attention of type 'vanilla' with 512 in_channels
 98: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 98: making attention of type 'vanilla' with 512 in_channels
 98: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 98: Loaded ViT-H-14 model config.
 97: Found checkpoints:
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 97: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 97: making attention of type 'vanilla' with 512 in_channels
 97: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 97: making attention of type 'vanilla' with 512 in_channels
 97: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 97: Loaded ViT-H-14 model config.
 96: Found checkpoints:
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 96: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 96: making attention of type 'vanilla' with 512 in_channels
 96: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 96: making attention of type 'vanilla' with 512 in_channels
 96: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 96: Loaded ViT-H-14 model config.
101: Found checkpoints:
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
101: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
101: making attention of type 'vanilla' with 512 in_channels
101: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
101: making attention of type 'vanilla' with 512 in_channels
101: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
101: Loaded ViT-H-14 model config.
102: Found checkpoints:
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
102: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
102: making attention of type 'vanilla' with 512 in_channels
102: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
102: making attention of type 'vanilla' with 512 in_channels
102: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
102: Loaded ViT-H-14 model config.
 25: Found checkpoints:
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 25: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 25: making attention of type 'vanilla' with 512 in_channels
 25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 25: making attention of type 'vanilla' with 512 in_channels
 25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 25: Loaded ViT-H-14 model config.
 27: Found checkpoints:
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 27: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 27: making attention of type 'vanilla' with 512 in_channels
 27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 27: making attention of type 'vanilla' with 512 in_channels
 27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 27: Loaded ViT-H-14 model config.
 28: Found checkpoints:
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 28: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 28: making attention of type 'vanilla' with 512 in_channels
 28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 28: making attention of type 'vanilla' with 512 in_channels
 28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 28: Loaded ViT-H-14 model config.
 30: Found checkpoints:
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 30: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 30: making attention of type 'vanilla' with 512 in_channels
 30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 30: making attention of type 'vanilla' with 512 in_channels
 30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 30: Loaded ViT-H-14 model config.
 26: Found checkpoints:
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 26: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 26: making attention of type 'vanilla' with 512 in_channels
 26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 26: making attention of type 'vanilla' with 512 in_channels
 26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 26: Loaded ViT-H-14 model config.
 74: Found checkpoints:
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 74: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 74: making attention of type 'vanilla' with 512 in_channels
 74: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 74: making attention of type 'vanilla' with 512 in_channels
 74: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 74: Loaded ViT-H-14 model config.
 24: Found checkpoints:
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 24: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 24: making attention of type 'vanilla' with 512 in_channels
 24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 24: making attention of type 'vanilla' with 512 in_channels
 24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 24: Loaded ViT-H-14 model config.
 29: Found checkpoints:
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 29: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 29: making attention of type 'vanilla' with 512 in_channels
 29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 29: making attention of type 'vanilla' with 512 in_channels
 29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 29: Loaded ViT-H-14 model config.
 31: Found checkpoints:
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 31: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 31: making attention of type 'vanilla' with 512 in_channels
 31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 31: making attention of type 'vanilla' with 512 in_channels
 31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 31: Loaded ViT-H-14 model config.
 73: Found checkpoints:
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 73: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 73: making attention of type 'vanilla' with 512 in_channels
 73: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 73: making attention of type 'vanilla' with 512 in_channels
 73: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 73: Loaded ViT-H-14 model config.
 72: Found checkpoints:
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 72: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 72: making attention of type 'vanilla' with 512 in_channels
 72: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 72: making attention of type 'vanilla' with 512 in_channels
 72: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 72: Loaded ViT-H-14 model config.
 77: Found checkpoints:
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 77: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 77: making attention of type 'vanilla' with 512 in_channels
 77: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 77: making attention of type 'vanilla' with 512 in_channels
 77: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 77: Loaded ViT-H-14 model config.
 75: Found checkpoints:
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 75: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 75: making attention of type 'vanilla' with 512 in_channels
 75: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 75: making attention of type 'vanilla' with 512 in_channels
 75: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 75: Loaded ViT-H-14 model config.
 78: Found checkpoints:
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 78: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 78: making attention of type 'vanilla' with 512 in_channels
 78: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 78: making attention of type 'vanilla' with 512 in_channels
 78: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 78: Loaded ViT-H-14 model config.
 76: Found checkpoints:
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 76: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 76: making attention of type 'vanilla' with 512 in_channels
 76: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 76: making attention of type 'vanilla' with 512 in_channels
 76: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 76: Loaded ViT-H-14 model config.
 79: Found checkpoints:
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 79: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 79: making attention of type 'vanilla' with 512 in_channels
 79: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 79: making attention of type 'vanilla' with 512 in_channels
 79: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 79: Loaded ViT-H-14 model config.
 48: Found checkpoints:
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 48: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 48: making attention of type 'vanilla' with 512 in_channels
 48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 48: making attention of type 'vanilla' with 512 in_channels
 48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 48: Loaded ViT-H-14 model config.
 50: Found checkpoints:
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 50: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 50: making attention of type 'vanilla' with 512 in_channels
 50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 50: making attention of type 'vanilla' with 512 in_channels
 50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 50: Loaded ViT-H-14 model config.
 49: Found checkpoints:
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 49: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 49: making attention of type 'vanilla' with 512 in_channels
 49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 49: making attention of type 'vanilla' with 512 in_channels
 49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 49: Loaded ViT-H-14 model config.
 54: Found checkpoints:
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 54: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 54: making attention of type 'vanilla' with 512 in_channels
 54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 54: making attention of type 'vanilla' with 512 in_channels
 54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 54: Loaded ViT-H-14 model config.
 53: Found checkpoints:
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 53: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 53: making attention of type 'vanilla' with 512 in_channels
 53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 53: making attention of type 'vanilla' with 512 in_channels
 53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 53: Loaded ViT-H-14 model config.
 55: Found checkpoints:
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 55: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 55: making attention of type 'vanilla' with 512 in_channels
 55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 55: making attention of type 'vanilla' with 512 in_channels
 55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 55: Loaded ViT-H-14 model config.
 52: Found checkpoints:
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 52: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 52: making attention of type 'vanilla' with 512 in_channels
 52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 52: making attention of type 'vanilla' with 512 in_channels
 52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 52: Loaded ViT-H-14 model config.
 51: Found checkpoints:
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 51: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 51: making attention of type 'vanilla' with 512 in_channels
 51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 51: making attention of type 'vanilla' with 512 in_channels
 51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 51: Loaded ViT-H-14 model config.
 23: Found checkpoints:
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 23: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 23: making attention of type 'vanilla' with 512 in_channels
 23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 23: making attention of type 'vanilla' with 512 in_channels
 23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 23: Loaded ViT-H-14 model config.
 18: Found checkpoints:
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 18: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 18: making attention of type 'vanilla' with 512 in_channels
 18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 18: making attention of type 'vanilla' with 512 in_channels
 18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 18: Loaded ViT-H-14 model config.
 20: Found checkpoints:
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 20: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 20: making attention of type 'vanilla' with 512 in_channels
 20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 20: making attention of type 'vanilla' with 512 in_channels
 20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 20: Loaded ViT-H-14 model config.
 17: Found checkpoints:
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 17: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 17: making attention of type 'vanilla' with 512 in_channels
 17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 17: making attention of type 'vanilla' with 512 in_channels
 17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 17: Loaded ViT-H-14 model config.
 16: Found checkpoints:
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 16: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 16: making attention of type 'vanilla' with 512 in_channels
 16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 16: making attention of type 'vanilla' with 512 in_channels
 16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 16: Loaded ViT-H-14 model config.
 21: Found checkpoints:
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 21: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 21: making attention of type 'vanilla' with 512 in_channels
 21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 21: making attention of type 'vanilla' with 512 in_channels
 21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 21: Loaded ViT-H-14 model config.
 69: Found checkpoints:
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 69: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 69: making attention of type 'vanilla' with 512 in_channels
 69: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 69: making attention of type 'vanilla' with 512 in_channels
 69: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 69: Loaded ViT-H-14 model config.
 65: Found checkpoints:
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 65: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 65: making attention of type 'vanilla' with 512 in_channels
 65: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: making attention of type 'vanilla' with 512 in_channels
 65: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 65: Loaded ViT-H-14 model config.
 66: Found checkpoints:
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 66: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 66: making attention of type 'vanilla' with 512 in_channels
 66: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 66: making attention of type 'vanilla' with 512 in_channels
 66: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 66: Loaded ViT-H-14 model config.
 22: Found checkpoints:
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 22: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 22: making attention of type 'vanilla' with 512 in_channels
 22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 22: making attention of type 'vanilla' with 512 in_channels
 22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 22: Loaded ViT-H-14 model config.
112: Found checkpoints:
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
112: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
112: making attention of type 'vanilla' with 512 in_channels
112: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
112: making attention of type 'vanilla' with 512 in_channels
112: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
112: Loaded ViT-H-14 model config.
 71: Found checkpoints:
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 71: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 71: making attention of type 'vanilla' with 512 in_channels
 71: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 71: making attention of type 'vanilla' with 512 in_channels
 71: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 71: Loaded ViT-H-14 model config.
 19: Found checkpoints:
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 19: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 19: making attention of type 'vanilla' with 512 in_channels
 19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 19: making attention of type 'vanilla' with 512 in_channels
 19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 19: Loaded ViT-H-14 model config.
 67: Found checkpoints:
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 67: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 67: making attention of type 'vanilla' with 512 in_channels
 67: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 67: making attention of type 'vanilla' with 512 in_channels
 67: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 67: Loaded ViT-H-14 model config.
 90: Found checkpoints:
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 90: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 90: making attention of type 'vanilla' with 512 in_channels
 90: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 90: making attention of type 'vanilla' with 512 in_channels
 90: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 90: Loaded ViT-H-14 model config.
 93: Found checkpoints:
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 93: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 93: making attention of type 'vanilla' with 512 in_channels
 93: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 93: making attention of type 'vanilla' with 512 in_channels
 93: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 93: Loaded ViT-H-14 model config.
 92: Found checkpoints:
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 92: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 92: making attention of type 'vanilla' with 512 in_channels
 92: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 92: making attention of type 'vanilla' with 512 in_channels
 92: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 92: Loaded ViT-H-14 model config.
116: Found checkpoints:
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
116: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
116: making attention of type 'vanilla' with 512 in_channels
116: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
116: making attention of type 'vanilla' with 512 in_channels
116: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
116: Loaded ViT-H-14 model config.
 64: Found checkpoints:
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 64: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 64: making attention of type 'vanilla' with 512 in_channels
 64: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 64: making attention of type 'vanilla' with 512 in_channels
 64: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 64: Loaded ViT-H-14 model config.
 68: Found checkpoints:
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 68: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 68: making attention of type 'vanilla' with 512 in_channels
 68: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 68: making attention of type 'vanilla' with 512 in_channels
 68: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 68: Loaded ViT-H-14 model config.
114: Found checkpoints:
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
114: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
114: making attention of type 'vanilla' with 512 in_channels
114: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
114: making attention of type 'vanilla' with 512 in_channels
114: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
114: Loaded ViT-H-14 model config.
 70: Found checkpoints:
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 70: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 70: making attention of type 'vanilla' with 512 in_channels
 70: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 70: making attention of type 'vanilla' with 512 in_channels
 70: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 70: Loaded ViT-H-14 model config.
117: Found checkpoints:
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
117: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
117: making attention of type 'vanilla' with 512 in_channels
117: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: making attention of type 'vanilla' with 512 in_channels
117: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
117: Loaded ViT-H-14 model config.
115: Found checkpoints:
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
115: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
115: making attention of type 'vanilla' with 512 in_channels
115: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
115: making attention of type 'vanilla' with 512 in_channels
115: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
115: Loaded ViT-H-14 model config.
118: Found checkpoints:
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
118: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
118: making attention of type 'vanilla' with 512 in_channels
118: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
118: making attention of type 'vanilla' with 512 in_channels
118: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
118: Loaded ViT-H-14 model config.
 88: Found checkpoints:
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 88: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 88: making attention of type 'vanilla' with 512 in_channels
 88: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 88: making attention of type 'vanilla' with 512 in_channels
 88: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 88: Loaded ViT-H-14 model config.
 94: Found checkpoints:
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 94: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 94: making attention of type 'vanilla' with 512 in_channels
 94: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 94: making attention of type 'vanilla' with 512 in_channels
 94: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 94: Loaded ViT-H-14 model config.
119: Found checkpoints:
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
119: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
119: making attention of type 'vanilla' with 512 in_channels
119: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
119: making attention of type 'vanilla' with 512 in_channels
119: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
119: Loaded ViT-H-14 model config.
 95: Found checkpoints:
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 95: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 95: making attention of type 'vanilla' with 512 in_channels
 95: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 95: making attention of type 'vanilla' with 512 in_channels
 95: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 95: Loaded ViT-H-14 model config.
 89: Found checkpoints:
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 89: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 89: making attention of type 'vanilla' with 512 in_channels
 89: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 89: making attention of type 'vanilla' with 512 in_channels
 89: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 89: Loaded ViT-H-14 model config.
 91: Found checkpoints:
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 91: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 91: making attention of type 'vanilla' with 512 in_channels
 91: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 91: making attention of type 'vanilla' with 512 in_channels
 91: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 91: Loaded ViT-H-14 model config.
113: Found checkpoints:
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
113: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
113: making attention of type 'vanilla' with 512 in_channels
113: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
113: making attention of type 'vanilla' with 512 in_channels
113: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
113: Loaded ViT-H-14 model config.
 80: Found checkpoints:
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 80: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 80: making attention of type 'vanilla' with 512 in_channels
 80: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 80: making attention of type 'vanilla' with 512 in_channels
 80: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 80: Loaded ViT-H-14 model config.
 85: Found checkpoints:
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 85: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 85: making attention of type 'vanilla' with 512 in_channels
 85: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 85: making attention of type 'vanilla' with 512 in_channels
 85: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 85: Loaded ViT-H-14 model config.
 83: Found checkpoints:
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 83: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 83: making attention of type 'vanilla' with 512 in_channels
 83: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 83: making attention of type 'vanilla' with 512 in_channels
 83: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 83: Loaded ViT-H-14 model config.
 86: Found checkpoints:
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 86: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 86: making attention of type 'vanilla' with 512 in_channels
 86: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 86: making attention of type 'vanilla' with 512 in_channels
 86: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 86: Loaded ViT-H-14 model config.
 84: Found checkpoints:
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 84: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 84: making attention of type 'vanilla' with 512 in_channels
 84: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 84: making attention of type 'vanilla' with 512 in_channels
 84: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 84: Loaded ViT-H-14 model config.
 82: Found checkpoints:
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 82: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 82: making attention of type 'vanilla' with 512 in_channels
 82: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 82: making attention of type 'vanilla' with 512 in_channels
 82: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 82: Loaded ViT-H-14 model config.
 56: Found checkpoints:
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 56: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 56: making attention of type 'vanilla' with 512 in_channels
 56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 56: making attention of type 'vanilla' with 512 in_channels
 56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 56: Loaded ViT-H-14 model config.
 57: Found checkpoints:
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 57: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 57: making attention of type 'vanilla' with 512 in_channels
 57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 57: making attention of type 'vanilla' with 512 in_channels
 57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 57: Loaded ViT-H-14 model config.
 58: Found checkpoints:
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 58: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 58: making attention of type 'vanilla' with 512 in_channels
 58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 58: making attention of type 'vanilla' with 512 in_channels
 58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 58: Loaded ViT-H-14 model config.
 87: Found checkpoints:
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 87: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 87: making attention of type 'vanilla' with 512 in_channels
 87: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 87: making attention of type 'vanilla' with 512 in_channels
 87: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 87: Loaded ViT-H-14 model config.
 61: Found checkpoints:
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 61: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 61: making attention of type 'vanilla' with 512 in_channels
 61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 61: making attention of type 'vanilla' with 512 in_channels
 61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 61: Loaded ViT-H-14 model config.
 62: Found checkpoints:
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 62: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 62: making attention of type 'vanilla' with 512 in_channels
 62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 62: making attention of type 'vanilla' with 512 in_channels
 62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 62: Loaded ViT-H-14 model config.
 81: Found checkpoints:
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 81: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 81: making attention of type 'vanilla' with 512 in_channels
 81: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 81: making attention of type 'vanilla' with 512 in_channels
 81: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 81: Loaded ViT-H-14 model config.
 60: Found checkpoints:
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 60: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 60: making attention of type 'vanilla' with 512 in_channels
 60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 60: making attention of type 'vanilla' with 512 in_channels
 60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 60: Loaded ViT-H-14 model config.
 63: Found checkpoints:
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 63: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 63: making attention of type 'vanilla' with 512 in_channels
 63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: making attention of type 'vanilla' with 512 in_channels
 63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 63: Loaded ViT-H-14 model config.
 59: Found checkpoints:
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 59: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 59: making attention of type 'vanilla' with 512 in_channels
 59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 59: making attention of type 'vanilla' with 512 in_channels
 59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 59: Loaded ViT-H-14 model config.
 44: Found checkpoints:
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 44: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 44: making attention of type 'vanilla' with 512 in_channels
 44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 44: making attention of type 'vanilla' with 512 in_channels
 44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 44: Loaded ViT-H-14 model config.
 32: Found checkpoints:
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 32: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 32: making attention of type 'vanilla' with 512 in_channels
 32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 32: making attention of type 'vanilla' with 512 in_channels
 32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 32: Loaded ViT-H-14 model config.
 39: Found checkpoints:
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 39: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 39: making attention of type 'vanilla' with 512 in_channels
 39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 39: making attention of type 'vanilla' with 512 in_channels
 39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 39: Loaded ViT-H-14 model config.
 45: Found checkpoints:
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 45: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 45: making attention of type 'vanilla' with 512 in_channels
 45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 45: making attention of type 'vanilla' with 512 in_channels
 45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 45: Loaded ViT-H-14 model config.
 35: Found checkpoints:
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 35: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 35: making attention of type 'vanilla' with 512 in_channels
 35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 35: making attention of type 'vanilla' with 512 in_channels
 35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 35: Loaded ViT-H-14 model config.
 43: Found checkpoints:
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 43: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 43: making attention of type 'vanilla' with 512 in_channels
 43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 43: making attention of type 'vanilla' with 512 in_channels
 43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 43: Loaded ViT-H-14 model config.
 47: Found checkpoints:
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 47: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 47: making attention of type 'vanilla' with 512 in_channels
 47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 47: making attention of type 'vanilla' with 512 in_channels
 47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 47: Loaded ViT-H-14 model config.
 46: Found checkpoints:
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 46: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 46: making attention of type 'vanilla' with 512 in_channels
 46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 46: making attention of type 'vanilla' with 512 in_channels
 46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 46: Loaded ViT-H-14 model config.
 36: Found checkpoints:
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 36: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 36: making attention of type 'vanilla' with 512 in_channels
 36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 36: making attention of type 'vanilla' with 512 in_channels
 36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 36: Loaded ViT-H-14 model config.
 38: Found checkpoints:
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 38: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 38: making attention of type 'vanilla' with 512 in_channels
 38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 38: making attention of type 'vanilla' with 512 in_channels
 38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 38: Loaded ViT-H-14 model config.
 37: Found checkpoints:
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 37: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 37: making attention of type 'vanilla' with 512 in_channels
 37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 37: making attention of type 'vanilla' with 512 in_channels
 37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 37: Loaded ViT-H-14 model config.
 41: Found checkpoints:
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 41: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 41: making attention of type 'vanilla' with 512 in_channels
 41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 41: making attention of type 'vanilla' with 512 in_channels
 41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 41: Loaded ViT-H-14 model config.
 40: Found checkpoints:
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 40: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 40: making attention of type 'vanilla' with 512 in_channels
 40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 40: making attention of type 'vanilla' with 512 in_channels
 40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 40: Loaded ViT-H-14 model config.
 42: Found checkpoints:
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 42: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 42: making attention of type 'vanilla' with 512 in_channels
 42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: making attention of type 'vanilla' with 512 in_channels
 42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 42: Loaded ViT-H-14 model config.
 34: Found checkpoints:
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 34: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 34: making attention of type 'vanilla' with 512 in_channels
 34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 34: making attention of type 'vanilla' with 512 in_channels
 34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 34: Loaded ViT-H-14 model config.
 33: Found checkpoints:
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319054814.0-step=2000-consumed_samples=2048000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319098716.0-step=2500-consumed_samples=2560000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319142617.0-step=3000-consumed_samples=3072000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319186535.0-step=3500-consumed_samples=3584000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318923308.0-step=500-consumed_samples=512000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715318967043.0-step=1000-consumed_samples=1024000.0.ckpt
 33: /nemologs/stable-diffusion2-train-240510052401216597839/checkpoints/stable-diffusion2-train-240510052401216597839--timestamp=1715319010918.0-step=1500-consumed_samples=1536000.0.ckpt
 33: making attention of type 'vanilla' with 512 in_channels
 33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 33: making attention of type 'vanilla' with 512 in_channels
 33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 33: Loaded ViT-H-14 model config.
  2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
126: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
122: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
127: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
123: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
124: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
120: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
125: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
121: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
106: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
111: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
110: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
109: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
105: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
108: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
107: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
100: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
103: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
104: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 99: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 98: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 97: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
101: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
102: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 96: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 74: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 73: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 72: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 77: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 75: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 76: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 78: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 79: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 69: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 65: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 66: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 90: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 71: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 67: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
112: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 93: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
116: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
114: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 92: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 64: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 68: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
115: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 70: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
117: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
118: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 88: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 89: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 95: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 94: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
119: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
113: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 91: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 80: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 85: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 83: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 86: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 84: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 81: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 82: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 87: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:34:47 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
  0: [NeMo I 2024-05-10 05:34:47 ddpm:261] It has 1242 entries
  0: [NeMo I 2024-05-10 05:34:47 ddpm:262] Existing model has 1240 entries
  0: [NeMo I 2024-05-10 05:34:47 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
  4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/128
  2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/128
 44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:34:48 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
  0: [NeMo I 2024-05-10 05:34:48 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
  0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
  0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
  0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
  0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
  0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
  0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
  0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
  0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
  0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
  0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
  0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
  0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
  0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
  0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
  0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
  0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
  0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
  0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
  0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
  0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
  0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
  0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
  0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
  0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
  0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
  0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
  0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
  0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
  0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
  0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
  0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
  0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
  0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
  0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
  0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
  0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
  0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
  0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
  0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
  0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
  0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
  0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
  0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
  0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
  0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
  0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
  0: [NeMo I 2024-05-10 05:34:48 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
  3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/128
  6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/128
  1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/128
  5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/128
  7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/128
  0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
126: Initializing distributed: GLOBAL_RANK: 126, MEMBER: 127/128
127: Initializing distributed: GLOBAL_RANK: 127, MEMBER: 128/128
123: Initializing distributed: GLOBAL_RANK: 123, MEMBER: 124/128
122: Initializing distributed: GLOBAL_RANK: 122, MEMBER: 123/128
124: Initializing distributed: GLOBAL_RANK: 124, MEMBER: 125/128
120: Initializing distributed: GLOBAL_RANK: 120, MEMBER: 121/128
125: Initializing distributed: GLOBAL_RANK: 125, MEMBER: 126/128
121: Initializing distributed: GLOBAL_RANK: 121, MEMBER: 122/128
  9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/128
 11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/128
 14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/128
  8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/128
 12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/128
 13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/128
 15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/128
103: Initializing distributed: GLOBAL_RANK: 103, MEMBER: 104/128
 10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/128
101: Initializing distributed: GLOBAL_RANK: 101, MEMBER: 102/128
106: Initializing distributed: GLOBAL_RANK: 106, MEMBER: 107/128
111: Initializing distributed: GLOBAL_RANK: 111, MEMBER: 112/128
109: Initializing distributed: GLOBAL_RANK: 109, MEMBER: 110/128
 96: Initializing distributed: GLOBAL_RANK: 96, MEMBER: 97/128
110: Initializing distributed: GLOBAL_RANK: 110, MEMBER: 111/128
105: Initializing distributed: GLOBAL_RANK: 105, MEMBER: 106/128
108: Initializing distributed: GLOBAL_RANK: 108, MEMBER: 109/128
107: Initializing distributed: GLOBAL_RANK: 107, MEMBER: 108/128
 74: Initializing distributed: GLOBAL_RANK: 74, MEMBER: 75/128
 26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/128
 24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/128
104: Initializing distributed: GLOBAL_RANK: 104, MEMBER: 105/128
 28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/128
 29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/128
 25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/128
 27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/128
 30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/128
100: Initializing distributed: GLOBAL_RANK: 100, MEMBER: 101/128
 98: Initializing distributed: GLOBAL_RANK: 98, MEMBER: 99/128
 90: Initializing distributed: GLOBAL_RANK: 90, MEMBER: 91/128
 97: Initializing distributed: GLOBAL_RANK: 97, MEMBER: 98/128
 99: Initializing distributed: GLOBAL_RANK: 99, MEMBER: 100/128
 92: Initializing distributed: GLOBAL_RANK: 92, MEMBER: 93/128
102: Initializing distributed: GLOBAL_RANK: 102, MEMBER: 103/128
 49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/128
 48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/128
 31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/128
 54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/128
 17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/128
 18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/128
 16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/128
 23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/128
 21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/128
 20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/128
 73: Initializing distributed: GLOBAL_RANK: 73, MEMBER: 74/128
 75: Initializing distributed: GLOBAL_RANK: 75, MEMBER: 76/128
112: Initializing distributed: GLOBAL_RANK: 112, MEMBER: 113/128
 53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/128
 55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/128
 50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/128
 51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/128
 52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/128
 72: Initializing distributed: GLOBAL_RANK: 72, MEMBER: 73/128
 77: Initializing distributed: GLOBAL_RANK: 77, MEMBER: 78/128
 78: Initializing distributed: GLOBAL_RANK: 78, MEMBER: 79/128
114: Initializing distributed: GLOBAL_RANK: 114, MEMBER: 115/128
 76: Initializing distributed: GLOBAL_RANK: 76, MEMBER: 77/128
118: Initializing distributed: GLOBAL_RANK: 118, MEMBER: 119/128
 79: Initializing distributed: GLOBAL_RANK: 79, MEMBER: 80/128
119: Initializing distributed: GLOBAL_RANK: 119, MEMBER: 120/128
 22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/128
 85: Initializing distributed: GLOBAL_RANK: 85, MEMBER: 86/128
115: Initializing distributed: GLOBAL_RANK: 115, MEMBER: 116/128
 80: Initializing distributed: GLOBAL_RANK: 80, MEMBER: 81/128
 19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/128
117: Initializing distributed: GLOBAL_RANK: 117, MEMBER: 118/128
 70: Initializing distributed: GLOBAL_RANK: 70, MEMBER: 71/128
 93: Initializing distributed: GLOBAL_RANK: 93, MEMBER: 94/128
 69: Initializing distributed: GLOBAL_RANK: 69, MEMBER: 70/128
 65: Initializing distributed: GLOBAL_RANK: 65, MEMBER: 66/128
 95: Initializing distributed: GLOBAL_RANK: 95, MEMBER: 96/128
116: Initializing distributed: GLOBAL_RANK: 116, MEMBER: 117/128
 67: Initializing distributed: GLOBAL_RANK: 67, MEMBER: 68/128
 71: Initializing distributed: GLOBAL_RANK: 71, MEMBER: 72/128
 66: Initializing distributed: GLOBAL_RANK: 66, MEMBER: 67/128
 89: Initializing distributed: GLOBAL_RANK: 89, MEMBER: 90/128
 94: Initializing distributed: GLOBAL_RANK: 94, MEMBER: 95/128
 68: Initializing distributed: GLOBAL_RANK: 68, MEMBER: 69/128
 61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/128
 58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/128
 91: Initializing distributed: GLOBAL_RANK: 91, MEMBER: 92/128
113: Initializing distributed: GLOBAL_RANK: 113, MEMBER: 114/128
 88: Initializing distributed: GLOBAL_RANK: 88, MEMBER: 89/128
 64: Initializing distributed: GLOBAL_RANK: 64, MEMBER: 65/128
 56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/128
 43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/128
 44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/128
 42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/128
 86: Initializing distributed: GLOBAL_RANK: 86, MEMBER: 87/128
 83: Initializing distributed: GLOBAL_RANK: 83, MEMBER: 84/128
 81: Initializing distributed: GLOBAL_RANK: 81, MEMBER: 82/128
 84: Initializing distributed: GLOBAL_RANK: 84, MEMBER: 85/128
 82: Initializing distributed: GLOBAL_RANK: 82, MEMBER: 83/128
 62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/128
 63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/128
 57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/128
 87: Initializing distributed: GLOBAL_RANK: 87, MEMBER: 88/128
 59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/128
 41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/128
 47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/128
 40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/128
 45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/128
 60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/128
 35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/128
 39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/128
 32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/128
 37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/128
 38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/128
 34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/128
 33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/128
 36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/128
 46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/128
  0: ----------------------------------------------------------------------------------------------------
  0: distributed_backend=nccl
  0: All distributed processes registered. Starting with 128 processes
  0: ----------------------------------------------------------------------------------------------------
  0: 
  0: Global ID: 0, local ID: 0, world size: 128
  0: Rank 0 before barrier
  0: NCCL version 2.21.5+cuda12.4
  1: Global ID: 1, local ID: 1, world size: 128
  1: Rank 1 before barrier
  2: Global ID: 2, local ID: 2, world size: 128
  2: Rank 2 before barrier
  3: Global ID: 3, local ID: 3, world size: 128
  3: Rank 3 before barrier
  4: Global ID: 4, local ID: 4, world size: 128
  4: Rank 4 before barrier
  5: Global ID: 5, local ID: 5, world size: 128
  5: Rank 5 before barrier
  6: Global ID: 6, local ID: 6, world size: 128
  6: Rank 6 before barrier
  7: Global ID: 7, local ID: 7, world size: 128
  7: Rank 7 before barrier
  8: Global ID: 8, local ID: 0, world size: 128
  8: Rank 8 before barrier
  9: Global ID: 9, local ID: 1, world size: 128
  9: Rank 9 before barrier
 10: Global ID: 10, local ID: 2, world size: 128
 10: Rank 10 before barrier
 11: Global ID: 11, local ID: 3, world size: 128
 11: Rank 11 before barrier
 12: Global ID: 12, local ID: 4, world size: 128
 12: Rank 12 before barrier
 13: Global ID: 13, local ID: 5, world size: 128
 13: Rank 13 before barrier
 14: Global ID: 14, local ID: 6, world size: 128
 14: Rank 14 before barrier
 15: Global ID: 15, local ID: 7, world size: 128
 15: Rank 15 before barrier
 16: Global ID: 16, local ID: 0, world size: 128
 16: Rank 16 before barrier
 17: Global ID: 17, local ID: 1, world size: 128
 17: Rank 17 before barrier
 18: Global ID: 18, local ID: 2, world size: 128
 18: Rank 18 before barrier
 19: Global ID: 19, local ID: 3, world size: 128
 19: Rank 19 before barrier
 20: Global ID: 20, local ID: 4, world size: 128
 20: Rank 20 before barrier
 21: Global ID: 21, local ID: 5, world size: 128
 21: Rank 21 before barrier
 22: Global ID: 22, local ID: 6, world size: 128
 22: Rank 22 before barrier
 23: Global ID: 23, local ID: 7, world size: 128
 23: Rank 23 before barrier
 24: Global ID: 24, local ID: 0, world size: 128
 24: Rank 24 before barrier
 25: Global ID: 25, local ID: 1, world size: 128
 25: Rank 25 before barrier
 26: Global ID: 26, local ID: 2, world size: 128
 26: Rank 26 before barrier
 27: Global ID: 27, local ID: 3, world size: 128
 27: Rank 27 before barrier
 28: Global ID: 28, local ID: 4, world size: 128
 28: Rank 28 before barrier
 29: Global ID: 29, local ID: 5, world size: 128
 29: Rank 29 before barrier
 30: Global ID: 30, local ID: 6, world size: 128
 30: Rank 30 before barrier
 31: Global ID: 31, local ID: 7, world size: 128
 31: Rank 31 before barrier
 32: Global ID: 32, local ID: 0, world size: 128
 32: Rank 32 before barrier
 33: Global ID: 33, local ID: 1, world size: 128
 33: Rank 33 before barrier
 34: Global ID: 34, local ID: 2, world size: 128
 34: Rank 34 before barrier
 35: Global ID: 35, local ID: 3, world size: 128
 35: Rank 35 before barrier
 36: Global ID: 36, local ID: 4, world size: 128
 36: Rank 36 before barrier
 37: Global ID: 37, local ID: 5, world size: 128
 37: Rank 37 before barrier
 38: Global ID: 38, local ID: 6, world size: 128
 38: Rank 38 before barrier
 39: Global ID: 39, local ID: 7, world size: 128
 39: Rank 39 before barrier
 40: Global ID: 40, local ID: 0, world size: 128
 40: Rank 40 before barrier
 41: Global ID: 41, local ID: 1, world size: 128
 41: Rank 41 before barrier
 42: Global ID: 42, local ID: 2, world size: 128
 42: Rank 42 before barrier
 43: Global ID: 43, local ID: 3, world size: 128
 43: Rank 43 before barrier
 44: Global ID: 44, local ID: 4, world size: 128
 44: Rank 44 before barrier
 45: Global ID: 45, local ID: 5, world size: 128
 45: Rank 45 before barrier
 46: Global ID: 46, local ID: 6, world size: 128
 46: Rank 46 before barrier
 47: Global ID: 47, local ID: 7, world size: 128
 47: Rank 47 before barrier
 48: Global ID: 48, local ID: 0, world size: 128
 48: Rank 48 before barrier
 49: Global ID: 49, local ID: 1, world size: 128
 49: Rank 49 before barrier
 50: Global ID: 50, local ID: 2, world size: 128
 50: Rank 50 before barrier
 51: Global ID: 51, local ID: 3, world size: 128
 51: Rank 51 before barrier
 52: Global ID: 52, local ID: 4, world size: 128
 52: Rank 52 before barrier
 53: Global ID: 53, local ID: 5, world size: 128
 53: Rank 53 before barrier
 54: Global ID: 54, local ID: 6, world size: 128
 54: Rank 54 before barrier
 55: Global ID: 55, local ID: 7, world size: 128
 55: Rank 55 before barrier
 56: Global ID: 56, local ID: 0, world size: 128
 56: Rank 56 before barrier
 57: Global ID: 57, local ID: 1, world size: 128
 57: Rank 57 before barrier
 58: Global ID: 58, local ID: 2, world size: 128
 58: Rank 58 before barrier
 59: Global ID: 59, local ID: 3, world size: 128
 59: Rank 59 before barrier
 60: Global ID: 60, local ID: 4, world size: 128
 60: Rank 60 before barrier
 61: Global ID: 61, local ID: 5, world size: 128
 61: Rank 61 before barrier
 62: Global ID: 62, local ID: 6, world size: 128
 62: Rank 62 before barrier
 63: Global ID: 63, local ID: 7, world size: 128
 63: Rank 63 before barrier
 64: Global ID: 64, local ID: 0, world size: 128
 64: Rank 64 before barrier
 65: Global ID: 65, local ID: 1, world size: 128
 65: Rank 65 before barrier
 66: Global ID: 66, local ID: 2, world size: 128
 66: Rank 66 before barrier
 67: Global ID: 67, local ID: 3, world size: 128
 67: Rank 67 before barrier
 68: Global ID: 68, local ID: 4, world size: 128
 68: Rank 68 before barrier
 69: Global ID: 69, local ID: 5, world size: 128
 69: Rank 69 before barrier
 70: Global ID: 70, local ID: 6, world size: 128
 70: Rank 70 before barrier
 71: Global ID: 71, local ID: 7, world size: 128
 71: Rank 71 before barrier
 72: Global ID: 72, local ID: 0, world size: 128
 72: Rank 72 before barrier
 73: Global ID: 73, local ID: 1, world size: 128
 73: Rank 73 before barrier
 74: Global ID: 74, local ID: 2, world size: 128
 74: Rank 74 before barrier
 75: Global ID: 75, local ID: 3, world size: 128
 75: Rank 75 before barrier
 76: Global ID: 76, local ID: 4, world size: 128
 76: Rank 76 before barrier
 77: Global ID: 77, local ID: 5, world size: 128
 77: Rank 77 before barrier
 78: Global ID: 78, local ID: 6, world size: 128
 78: Rank 78 before barrier
 79: Global ID: 79, local ID: 7, world size: 128
 79: Rank 79 before barrier
 80: Global ID: 80, local ID: 0, world size: 128
 80: Rank 80 before barrier
 81: Global ID: 81, local ID: 1, world size: 128
 81: Rank 81 before barrier
 82: Global ID: 82, local ID: 2, world size: 128
 82: Rank 82 before barrier
 83: Global ID: 83, local ID: 3, world size: 128
 83: Rank 83 before barrier
 84: Global ID: 84, local ID: 4, world size: 128
 84: Rank 84 before barrier
 85: Global ID: 85, local ID: 5, world size: 128
 85: Rank 85 before barrier
 86: Global ID: 86, local ID: 6, world size: 128
 86: Rank 86 before barrier
 87: Global ID: 87, local ID: 7, world size: 128
 87: Rank 87 before barrier
 88: Global ID: 88, local ID: 0, world size: 128
 88: Rank 88 before barrier
 89: Global ID: 89, local ID: 1, world size: 128
 89: Rank 89 before barrier
 90: Global ID: 90, local ID: 2, world size: 128
 90: Rank 90 before barrier
 91: Global ID: 91, local ID: 3, world size: 128
 91: Rank 91 before barrier
 92: Global ID: 92, local ID: 4, world size: 128
 92: Rank 92 before barrier
 93: Global ID: 93, local ID: 5, world size: 128
 93: Rank 93 before barrier
 94: Global ID: 94, local ID: 6, world size: 128
 94: Rank 94 before barrier
 95: Global ID: 95, local ID: 7, world size: 128
 95: Rank 95 before barrier
 96: Global ID: 96, local ID: 0, world size: 128
 96: Rank 96 before barrier
 97: Global ID: 97, local ID: 1, world size: 128
 97: Rank 97 before barrier
 98: Global ID: 98, local ID: 2, world size: 128
 98: Rank 98 before barrier
 99: Global ID: 99, local ID: 3, world size: 128
 99: Rank 99 before barrier
100: Global ID: 100, local ID: 4, world size: 128
100: Rank 100 before barrier
101: Global ID: 101, local ID: 5, world size: 128
101: Rank 101 before barrier
102: Global ID: 102, local ID: 6, world size: 128
102: Rank 102 before barrier
103: Global ID: 103, local ID: 7, world size: 128
103: Rank 103 before barrier
104: Global ID: 104, local ID: 0, world size: 128
104: Rank 104 before barrier
105: Global ID: 105, local ID: 1, world size: 128
105: Rank 105 before barrier
106: Global ID: 106, local ID: 2, world size: 128
106: Rank 106 before barrier
107: Global ID: 107, local ID: 3, world size: 128
107: Rank 107 before barrier
108: Global ID: 108, local ID: 4, world size: 128
108: Rank 108 before barrier
109: Global ID: 109, local ID: 5, world size: 128
109: Rank 109 before barrier
110: Global ID: 110, local ID: 6, world size: 128
110: Rank 110 before barrier
111: Global ID: 111, local ID: 7, world size: 128
111: Rank 111 before barrier
112: Global ID: 112, local ID: 0, world size: 128
112: Rank 112 before barrier
113: Global ID: 113, local ID: 1, world size: 128
113: Rank 113 before barrier
114: Global ID: 114, local ID: 2, world size: 128
114: Rank 114 before barrier
115: Global ID: 115, local ID: 3, world size: 128
115: Rank 115 before barrier
116: Global ID: 116, local ID: 4, world size: 128
116: Rank 116 before barrier
117: Global ID: 117, local ID: 5, world size: 128
117: Rank 117 before barrier
118: Global ID: 118, local ID: 6, world size: 128
118: Rank 118 before barrier
119: Global ID: 119, local ID: 7, world size: 128
119: Rank 119 before barrier
120: Global ID: 120, local ID: 0, world size: 128
120: Rank 120 before barrier
121: Global ID: 121, local ID: 1, world size: 128
121: Rank 121 before barrier
122: Global ID: 122, local ID: 2, world size: 128
122: Rank 122 before barrier
123: Global ID: 123, local ID: 3, world size: 128
123: Rank 123 before barrier
124: Global ID: 124, local ID: 4, world size: 128
124: Rank 124 before barrier
125: Global ID: 125, local ID: 5, world size: 128
125: Rank 125 before barrier
126: Global ID: 126, local ID: 6, world size: 128
126: Rank 126 before barrier
127: Global ID: 127, local ID: 7, world size: 128
127: Rank 127 before barrier
  0: :::MLLOG {"namespace": "", "time_ms": 1715319303596, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 2048000}}
  0: Assigned 235 prompts for this worker.
  0: :::MLLOG {"namespace": "", "time_ms": 1715319380558, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 2048000}}
120: Assigned 234 prompts for this worker.
 40: Assigned 235 prompts for this worker.
 16: Assigned 235 prompts for this worker.
112: Assigned 234 prompts for this worker.
 72: Assigned 234 prompts for this worker.
 41: Assigned 235 prompts for this worker.
 98: Assigned 234 prompts for this worker.
 50: Assigned 234 prompts for this worker.
 73: Assigned 234 prompts for this worker.
 97: Assigned 234 prompts for this worker.
 24: Assigned 235 prompts for this worker.
 96: Assigned 234 prompts for this worker.
 80: Assigned 234 prompts for this worker.
 33: Assigned 235 prompts for this worker.
114: Assigned 234 prompts for this worker.
 48: Assigned 234 prompts for this worker.
 56: Assigned 234 prompts for this worker.
 66: Assigned 234 prompts for this worker.
115: Assigned 234 prompts for this worker.
  3: Assigned 235 prompts for this worker.
 32: Assigned 235 prompts for this worker.
122: Assigned 234 prompts for this worker.
 42: Assigned 235 prompts for this worker.
 57: Assigned 234 prompts for this worker.
 65: Assigned 234 prompts for this worker.
124: Assigned 234 prompts for this worker.
 17: Assigned 235 prompts for this worker.
 58: Assigned 234 prompts for this worker.
 64: Assigned 234 prompts for this worker.
  8: Assigned 235 prompts for this worker.
 49: Assigned 234 prompts for this worker.
113: Assigned 234 prompts for this worker.
 34: Assigned 235 prompts for this worker.
 35: Assigned 235 prompts for this worker.
123: Assigned 234 prompts for this worker.
 74: Assigned 234 prompts for this worker.
 18: Assigned 235 prompts for this worker.
 20: Assigned 235 prompts for this worker.
 90: Assigned 234 prompts for this worker.
  1: Assigned 235 prompts for this worker.
  2: Assigned 235 prompts for this worker.
 51: Assigned 234 prompts for this worker.
 36: Assigned 235 prompts for this worker.
118: Assigned 234 prompts for this worker.
121: Assigned 234 prompts for this worker.
 11: Assigned 235 prompts for this worker.
 43: Assigned 235 prompts for this worker.
 19: Assigned 235 prompts for this worker.
 59: Assigned 234 prompts for this worker.
 67: Assigned 234 prompts for this worker.
  5: Assigned 235 prompts for this worker.
125: Assigned 234 prompts for this worker.
106: Assigned 234 prompts for this worker.
  9: Assigned 235 prompts for this worker.
 68: Assigned 234 prompts for this worker.
 82: Assigned 234 prompts for this worker.
 10: Assigned 235 prompts for this worker.
 92: Assigned 234 prompts for this worker.
 12: Assigned 235 prompts for this worker.
 28: Assigned 235 prompts for this worker.
 69: Assigned 234 prompts for this worker.
 54: Assigned 234 prompts for this worker.
 45: Assigned 235 prompts for this worker.
 60: Assigned 234 prompts for this worker.
 70: Assigned 234 prompts for this worker.
117: Assigned 234 prompts for this worker.
 44: Assigned 235 prompts for this worker.
 37: Assigned 235 prompts for this worker.
 46: Assigned 235 prompts for this worker.
 99: Assigned 234 prompts for this worker.
 21: Assigned 235 prompts for this worker.
 89: Assigned 234 prompts for this worker.
 81: Assigned 234 prompts for this worker.
 13: Assigned 235 prompts for this worker.
 88: Assigned 234 prompts for this worker.
 83: Assigned 234 prompts for this worker.
127: Assigned 234 prompts for this worker.
 84: Assigned 234 prompts for this worker.
 27: Assigned 235 prompts for this worker.
 86: Assigned 234 prompts for this worker.
116: Assigned 234 prompts for this worker.
 25: Assigned 235 prompts for this worker.
119: Assigned 234 prompts for this worker.
  4: Assigned 235 prompts for this worker.
104: Assigned 234 prompts for this worker.
 26: Assigned 235 prompts for this worker.
 55: Assigned 234 prompts for this worker.
105: Assigned 234 prompts for this worker.
 38: Assigned 235 prompts for this worker.
 29: Assigned 235 prompts for this worker.
108: Assigned 234 prompts for this worker.
 22: Assigned 235 prompts for this worker.
 39: Assigned 235 prompts for this worker.
 76: Assigned 234 prompts for this worker.
109: Assigned 234 prompts for this worker.
110: Assigned 234 prompts for this worker.
 71: Assigned 234 prompts for this worker.
126: Assigned 234 prompts for this worker.
 30: Assigned 235 prompts for this worker.
 85: Assigned 234 prompts for this worker.
 62: Assigned 234 prompts for this worker.
 52: Assigned 234 prompts for this worker.
111: Assigned 234 prompts for this worker.
103: Assigned 234 prompts for this worker.
 14: Assigned 235 prompts for this worker.
100: Assigned 234 prompts for this worker.
 77: Assigned 234 prompts for this worker.
 93: Assigned 234 prompts for this worker.
 31: Assigned 235 prompts for this worker.
 63: Assigned 234 prompts for this worker.
  7: Assigned 235 prompts for this worker.
 47: Assigned 235 prompts for this worker.
 23: Assigned 235 prompts for this worker.
 61: Assigned 234 prompts for this worker.
101: Assigned 234 prompts for this worker.
 53: Assigned 234 prompts for this worker.
107: Assigned 234 prompts for this worker.
102: Assigned 234 prompts for this worker.
 91: Assigned 234 prompts for this worker.
 75: Assigned 234 prompts for this worker.
 95: Assigned 234 prompts for this worker.
 87: Assigned 234 prompts for this worker.
 78: Assigned 234 prompts for this worker.
 94: Assigned 234 prompts for this worker.
 79: Assigned 234 prompts for this worker.
  6: Assigned 235 prompts for this worker.
 15: Assigned 235 prompts for this worker.
  0: Calculating FID activations:   0%|          | 0/8 [00:00<?, ?it/s]Calculating FID activations:  12%|█▎        | 1/8 [00:00<00:06,  1.08it/s]Calculating FID activations: 100%|██████████| 8/8 [00:01<00:00, 10.18it/s]Calculating FID activations: 100%|██████████| 8/8 [00:01<00:00,  6.38it/s]
  0: Computed feature activations of size torch.Size([235, 2048])
  0: :::MLLOG {"namespace": "", "time_ms": 1715319389731, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 125.61215573200917, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 2048000, "metric": "FID"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319394710, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.13308359682559967, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 2048000, "metric": "CLIP"}}
  0: Using 16bit Automatic Mixed Precision (AMP)
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: IPU available: False, using: 0 IPUs
  0: HPU available: False, using: 0 HPUs
  0: [NeMo W 2024-05-10 05:36:34 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:279] Ranks 0 has data parallel rank: 0
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:287] Rank 0 has context parallel group: [0]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:291] Ranks 0 has context parallel rank: 0
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:298] Rank 0 has model parallel group: [0]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:308] Rank 0 has tensor model parallel group: [0]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:313] Rank 0 has tensor model parallel rank: 0
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:345] Rank 0 has embedding group: [0]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:352] Rank 0 has pipeline model parallel rank 0
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:36:36 megatron_init:354] Rank 0 has embedding rank: 0
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  2: [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:36:36 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [NeMo I 2024-05-10 05:36:36 ddpm:130] LatentDiffusion: Running in v-prediction mode
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  7: [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  5: [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:36:36 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:36:37 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:36:38 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:36:39 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 11: [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [rank103]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [rank102]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [rank97]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: [rank96]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [rank100]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [rank99]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [rank101]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [rank98]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:39 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
123: [rank123]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [rank120]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [rank125]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [rank122]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [rank124]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [rank126]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [rank127]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [rank121]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [rank95]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [rank92]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [rank91]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [rank89]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [rank88]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [rank94]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [rank93]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [rank90]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [rank74]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [rank73]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [rank75]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [rank77]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [rank79]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [rank78]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [rank72]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [rank76]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [rank68]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [rank65]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [rank66]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [rank70]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [rank64]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [rank71]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [rank67]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [rank69]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: [rank56]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [rank58]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [rank57]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [rank62]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [rank63]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [rank60]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [rank59]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [rank61]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [rank108]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [rank111]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [rank104]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [rank109]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [rank105]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [rank106]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [rank107]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [rank110]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [rank50]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [rank54]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [rank55]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [rank51]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [rank53]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: [rank48]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [rank49]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [rank52]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:39 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:36:40 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:36:40 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:36:40 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 86: [rank86]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [rank80]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [rank85]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [rank84]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [rank87]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [rank83]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [rank82]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [rank81]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:40 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:36:40 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 41: [rank41]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [rank45]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [rank46]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [rank44]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [rank42]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: [rank40]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [rank43]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [rank47]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [rank38]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [rank32]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [rank36]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [rank37]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [rank34]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [rank35]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [rank39]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [rank33]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:36:40 utils:92] DiffusionWrapper has 865.91 M params.
  0: [NeMo I 2024-05-10 05:36:40 ddpm:168] Use system random generator since CUDA graph enabled
114: [rank114]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [rank119]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [rank115]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [rank118]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [rank117]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [rank113]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [rank116]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [rank112]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: making attention of type 'vanilla' with 512 in_channels
  0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  0: making attention of type 'vanilla' with 512 in_channels
  0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  0: Loaded ViT-H-14 model config.
  2: Computed feature activations of size torch.Size([235, 2048])
  2: making attention of type 'vanilla' with 512 in_channels
  2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  2: making attention of type 'vanilla' with 512 in_channels
  2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  2: Loaded ViT-H-14 model config.
  7: Computed feature activations of size torch.Size([235, 2048])
  7: making attention of type 'vanilla' with 512 in_channels
  7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  7: making attention of type 'vanilla' with 512 in_channels
  7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  7: Loaded ViT-H-14 model config.
  3: Computed feature activations of size torch.Size([235, 2048])
  3: making attention of type 'vanilla' with 512 in_channels
  3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  3: making attention of type 'vanilla' with 512 in_channels
  3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  3: Loaded ViT-H-14 model config.
  6: Computed feature activations of size torch.Size([235, 2048])
  6: making attention of type 'vanilla' with 512 in_channels
  6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  6: making attention of type 'vanilla' with 512 in_channels
  6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  6: Loaded ViT-H-14 model config.
  1: Computed feature activations of size torch.Size([235, 2048])
  1: making attention of type 'vanilla' with 512 in_channels
  1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  1: making attention of type 'vanilla' with 512 in_channels
  1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  1: Loaded ViT-H-14 model config.
  4: Computed feature activations of size torch.Size([235, 2048])
  4: making attention of type 'vanilla' with 512 in_channels
  4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  4: making attention of type 'vanilla' with 512 in_channels
  4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  4: Loaded ViT-H-14 model config.
  5: Computed feature activations of size torch.Size([235, 2048])
  5: making attention of type 'vanilla' with 512 in_channels
  5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  5: making attention of type 'vanilla' with 512 in_channels
  5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  5: Loaded ViT-H-14 model config.
  9: Computed feature activations of size torch.Size([235, 2048])
  9: making attention of type 'vanilla' with 512 in_channels
  9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  9: making attention of type 'vanilla' with 512 in_channels
  9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  9: Loaded ViT-H-14 model config.
 12: Computed feature activations of size torch.Size([235, 2048])
 12: making attention of type 'vanilla' with 512 in_channels
 12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 12: making attention of type 'vanilla' with 512 in_channels
 12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 12: Loaded ViT-H-14 model config.
 11: Computed feature activations of size torch.Size([235, 2048])
 11: making attention of type 'vanilla' with 512 in_channels
 11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 11: making attention of type 'vanilla' with 512 in_channels
 11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 11: Loaded ViT-H-14 model config.
 15: Computed feature activations of size torch.Size([235, 2048])
 15: making attention of type 'vanilla' with 512 in_channels
 15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 15: making attention of type 'vanilla' with 512 in_channels
 15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 15: Loaded ViT-H-14 model config.
120: Computed feature activations of size torch.Size([234, 2048])
120: making attention of type 'vanilla' with 512 in_channels
120: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
120: making attention of type 'vanilla' with 512 in_channels
120: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
120: Loaded ViT-H-14 model config.
103: Computed feature activations of size torch.Size([234, 2048])
103: making attention of type 'vanilla' with 512 in_channels
103: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
103: making attention of type 'vanilla' with 512 in_channels
103: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
103: Loaded ViT-H-14 model config.
100: Computed feature activations of size torch.Size([234, 2048])
100: making attention of type 'vanilla' with 512 in_channels
100: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
100: making attention of type 'vanilla' with 512 in_channels
100: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
100: Loaded ViT-H-14 model config.
 72: Computed feature activations of size torch.Size([234, 2048])
 72: making attention of type 'vanilla' with 512 in_channels
 72: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 72: making attention of type 'vanilla' with 512 in_channels
 72: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 72: Loaded ViT-H-14 model config.
125: Computed feature activations of size torch.Size([234, 2048])
125: making attention of type 'vanilla' with 512 in_channels
125: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
125: making attention of type 'vanilla' with 512 in_channels
125: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
125: Loaded ViT-H-14 model config.
 97: Computed feature activations of size torch.Size([234, 2048])
 97: making attention of type 'vanilla' with 512 in_channels
 97: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 97: making attention of type 'vanilla' with 512 in_channels
 97: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 97: Loaded ViT-H-14 model config.
 73: Computed feature activations of size torch.Size([234, 2048])
 73: making attention of type 'vanilla' with 512 in_channels
 73: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 73: making attention of type 'vanilla' with 512 in_channels
 73: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 73: Loaded ViT-H-14 model config.
102: Computed feature activations of size torch.Size([234, 2048])
102: making attention of type 'vanilla' with 512 in_channels
102: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
102: making attention of type 'vanilla' with 512 in_channels
102: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
102: Loaded ViT-H-14 model config.
123: Computed feature activations of size torch.Size([234, 2048])
123: making attention of type 'vanilla' with 512 in_channels
123: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
123: making attention of type 'vanilla' with 512 in_channels
123: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
123: Loaded ViT-H-14 model config.
 92: Computed feature activations of size torch.Size([234, 2048])
 92: making attention of type 'vanilla' with 512 in_channels
 92: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 92: making attention of type 'vanilla' with 512 in_channels
 92: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 92: Loaded ViT-H-14 model config.
 78: Computed feature activations of size torch.Size([234, 2048])
 78: making attention of type 'vanilla' with 512 in_channels
 78: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 78: making attention of type 'vanilla' with 512 in_channels
 78: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 78: Loaded ViT-H-14 model config.
 95: Computed feature activations of size torch.Size([234, 2048])
 95: making attention of type 'vanilla' with 512 in_channels
 95: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 95: making attention of type 'vanilla' with 512 in_channels
 95: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 95: Loaded ViT-H-14 model config.
 75: Computed feature activations of size torch.Size([234, 2048])
 75: making attention of type 'vanilla' with 512 in_channels
 75: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 75: making attention of type 'vanilla' with 512 in_channels
 75: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 75: Loaded ViT-H-14 model config.
 89: Computed feature activations of size torch.Size([234, 2048])
 89: making attention of type 'vanilla' with 512 in_channels
 89: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 89: making attention of type 'vanilla' with 512 in_channels
 89: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 89: Loaded ViT-H-14 model config.
 74: Computed feature activations of size torch.Size([234, 2048])
 74: making attention of type 'vanilla' with 512 in_channels
 74: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 74: making attention of type 'vanilla' with 512 in_channels
 74: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 74: Loaded ViT-H-14 model config.
 70: Computed feature activations of size torch.Size([234, 2048])
 70: making attention of type 'vanilla' with 512 in_channels
 70: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 70: making attention of type 'vanilla' with 512 in_channels
 70: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 70: Loaded ViT-H-14 model config.
 91: Computed feature activations of size torch.Size([234, 2048])
 91: making attention of type 'vanilla' with 512 in_channels
 91: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 91: making attention of type 'vanilla' with 512 in_channels
 91: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 91: Loaded ViT-H-14 model config.
 29: Computed feature activations of size torch.Size([235, 2048])
 29: making attention of type 'vanilla' with 512 in_channels
 29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 29: making attention of type 'vanilla' with 512 in_channels
 29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 29: Loaded ViT-H-14 model config.
 24: Computed feature activations of size torch.Size([235, 2048])
 24: making attention of type 'vanilla' with 512 in_channels
 24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 24: making attention of type 'vanilla' with 512 in_channels
 24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 24: Loaded ViT-H-14 model config.
 28: Computed feature activations of size torch.Size([235, 2048])
 28: making attention of type 'vanilla' with 512 in_channels
 28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 28: making attention of type 'vanilla' with 512 in_channels
 28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 28: Loaded ViT-H-14 model config.
 62: Computed feature activations of size torch.Size([234, 2048])
 62: making attention of type 'vanilla' with 512 in_channels
 62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 62: making attention of type 'vanilla' with 512 in_channels
 62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 62: Loaded ViT-H-14 model config.
 76: Computed feature activations of size torch.Size([234, 2048])
 76: making attention of type 'vanilla' with 512 in_channels
 76: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 76: making attention of type 'vanilla' with 512 in_channels
 76: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 76: Loaded ViT-H-14 model config.
 79: Computed feature activations of size torch.Size([234, 2048])
 79: making attention of type 'vanilla' with 512 in_channels
 79: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 79: making attention of type 'vanilla' with 512 in_channels
 79: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 79: Loaded ViT-H-14 model config.
 66: Computed feature activations of size torch.Size([234, 2048])
 66: making attention of type 'vanilla' with 512 in_channels
 66: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 66: making attention of type 'vanilla' with 512 in_channels
 66: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 66: Loaded ViT-H-14 model config.
105: Computed feature activations of size torch.Size([234, 2048])
105: making attention of type 'vanilla' with 512 in_channels
105: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
105: making attention of type 'vanilla' with 512 in_channels
105: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
105: Loaded ViT-H-14 model config.
 31: Computed feature activations of size torch.Size([235, 2048])
 31: making attention of type 'vanilla' with 512 in_channels
 31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 31: making attention of type 'vanilla' with 512 in_channels
 31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 31: Loaded ViT-H-14 model config.
 13: Computed feature activations of size torch.Size([235, 2048])
 13: making attention of type 'vanilla' with 512 in_channels
 13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 13: making attention of type 'vanilla' with 512 in_channels
 13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 13: Loaded ViT-H-14 model config.
 65: Computed feature activations of size torch.Size([234, 2048])
 65: making attention of type 'vanilla' with 512 in_channels
 65: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: making attention of type 'vanilla' with 512 in_channels
 65: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 65: Loaded ViT-H-14 model config.
 68: Computed feature activations of size torch.Size([234, 2048])
 68: making attention of type 'vanilla' with 512 in_channels
 68: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 68: making attention of type 'vanilla' with 512 in_channels
 68: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 68: Loaded ViT-H-14 model config.
 57: Computed feature activations of size torch.Size([234, 2048])
 57: making attention of type 'vanilla' with 512 in_channels
 57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 57: making attention of type 'vanilla' with 512 in_channels
 57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 57: Loaded ViT-H-14 model config.
111: Computed feature activations of size torch.Size([234, 2048])
111: making attention of type 'vanilla' with 512 in_channels
111: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
111: making attention of type 'vanilla' with 512 in_channels
111: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
111: Loaded ViT-H-14 model config.
 54: Computed feature activations of size torch.Size([234, 2048])
 54: making attention of type 'vanilla' with 512 in_channels
 54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 54: making attention of type 'vanilla' with 512 in_channels
 54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 54: Loaded ViT-H-14 model config.
 51: Computed feature activations of size torch.Size([234, 2048])
 51: making attention of type 'vanilla' with 512 in_channels
 51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 51: making attention of type 'vanilla' with 512 in_channels
 51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 51: Loaded ViT-H-14 model config.
 50: Computed feature activations of size torch.Size([234, 2048])
 50: making attention of type 'vanilla' with 512 in_channels
 50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 50: making attention of type 'vanilla' with 512 in_channels
 50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 50: Loaded ViT-H-14 model config.
 55: Computed feature activations of size torch.Size([234, 2048])
 55: making attention of type 'vanilla' with 512 in_channels
 55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 55: making attention of type 'vanilla' with 512 in_channels
 55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 55: Loaded ViT-H-14 model config.
104: Computed feature activations of size torch.Size([234, 2048])
104: making attention of type 'vanilla' with 512 in_channels
104: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
104: making attention of type 'vanilla' with 512 in_channels
104: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
104: Loaded ViT-H-14 model config.
 58: Computed feature activations of size torch.Size([234, 2048])
 58: making attention of type 'vanilla' with 512 in_channels
 58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 58: making attention of type 'vanilla' with 512 in_channels
 58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 58: Loaded ViT-H-14 model config.
 53: Computed feature activations of size torch.Size([234, 2048])
 53: making attention of type 'vanilla' with 512 in_channels
 53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 53: making attention of type 'vanilla' with 512 in_channels
 53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 53: Loaded ViT-H-14 model config.
109: Computed feature activations of size torch.Size([234, 2048])
109: making attention of type 'vanilla' with 512 in_channels
109: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
109: making attention of type 'vanilla' with 512 in_channels
109: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
109: Loaded ViT-H-14 model config.
  8: Computed feature activations of size torch.Size([235, 2048])
  8: making attention of type 'vanilla' with 512 in_channels
  8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  8: making attention of type 'vanilla' with 512 in_channels
  8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  8: Loaded ViT-H-14 model config.
108: Computed feature activations of size torch.Size([234, 2048])
108: making attention of type 'vanilla' with 512 in_channels
108: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
108: making attention of type 'vanilla' with 512 in_channels
108: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
108: Loaded ViT-H-14 model config.
 63: Computed feature activations of size torch.Size([234, 2048])
 63: making attention of type 'vanilla' with 512 in_channels
 63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: making attention of type 'vanilla' with 512 in_channels
 63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 63: Loaded ViT-H-14 model config.
 99: Computed feature activations of size torch.Size([234, 2048])
 99: making attention of type 'vanilla' with 512 in_channels
 99: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 99: making attention of type 'vanilla' with 512 in_channels
 99: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 99: Loaded ViT-H-14 model config.
 56: Computed feature activations of size torch.Size([234, 2048])
 56: making attention of type 'vanilla' with 512 in_channels
 56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 56: making attention of type 'vanilla' with 512 in_channels
 56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 56: Loaded ViT-H-14 model config.
 10: Computed feature activations of size torch.Size([235, 2048])
 10: making attention of type 'vanilla' with 512 in_channels
 10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 10: making attention of type 'vanilla' with 512 in_channels
 10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 10: Loaded ViT-H-14 model config.
127: Computed feature activations of size torch.Size([234, 2048])
127: making attention of type 'vanilla' with 512 in_channels
127: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
127: making attention of type 'vanilla' with 512 in_channels
127: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
127: Loaded ViT-H-14 model config.
 67: Computed feature activations of size torch.Size([234, 2048])
 67: making attention of type 'vanilla' with 512 in_channels
 67: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 67: making attention of type 'vanilla' with 512 in_channels
 67: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 67: Loaded ViT-H-14 model config.
126: Computed feature activations of size torch.Size([234, 2048])
126: making attention of type 'vanilla' with 512 in_channels
126: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
126: making attention of type 'vanilla' with 512 in_channels
126: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
126: Loaded ViT-H-14 model config.
 14: Computed feature activations of size torch.Size([235, 2048])
 14: making attention of type 'vanilla' with 512 in_channels
 14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 14: making attention of type 'vanilla' with 512 in_channels
 14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 14: Loaded ViT-H-14 model config.
 98: Computed feature activations of size torch.Size([234, 2048])
 98: making attention of type 'vanilla' with 512 in_channels
 98: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 98: making attention of type 'vanilla' with 512 in_channels
 98: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 98: Loaded ViT-H-14 model config.
 96: Computed feature activations of size torch.Size([234, 2048])
 96: making attention of type 'vanilla' with 512 in_channels
 96: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 96: making attention of type 'vanilla' with 512 in_channels
 96: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 96: Loaded ViT-H-14 model config.
 77: Computed feature activations of size torch.Size([234, 2048])
 77: making attention of type 'vanilla' with 512 in_channels
 77: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 77: making attention of type 'vanilla' with 512 in_channels
 77: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 77: Loaded ViT-H-14 model config.
122: Computed feature activations of size torch.Size([234, 2048])
122: making attention of type 'vanilla' with 512 in_channels
122: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
122: making attention of type 'vanilla' with 512 in_channels
122: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
122: Loaded ViT-H-14 model config.
 93: Computed feature activations of size torch.Size([234, 2048])
 93: making attention of type 'vanilla' with 512 in_channels
 93: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 93: making attention of type 'vanilla' with 512 in_channels
 93: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 93: Loaded ViT-H-14 model config.
101: Computed feature activations of size torch.Size([234, 2048])
101: making attention of type 'vanilla' with 512 in_channels
101: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
101: making attention of type 'vanilla' with 512 in_channels
101: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
101: Loaded ViT-H-14 model config.
 64: Computed feature activations of size torch.Size([234, 2048])
 64: making attention of type 'vanilla' with 512 in_channels
 64: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 64: making attention of type 'vanilla' with 512 in_channels
 64: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 64: Loaded ViT-H-14 model config.
121: Computed feature activations of size torch.Size([234, 2048])
121: making attention of type 'vanilla' with 512 in_channels
121: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
121: making attention of type 'vanilla' with 512 in_channels
121: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
121: Loaded ViT-H-14 model config.
124: Computed feature activations of size torch.Size([234, 2048])
124: making attention of type 'vanilla' with 512 in_channels
124: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
124: making attention of type 'vanilla' with 512 in_channels
124: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
124: Loaded ViT-H-14 model config.
 26: Computed feature activations of size torch.Size([235, 2048])
 26: making attention of type 'vanilla' with 512 in_channels
 26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 26: making attention of type 'vanilla' with 512 in_channels
 26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 26: Loaded ViT-H-14 model config.
 90: Computed feature activations of size torch.Size([234, 2048])
 90: making attention of type 'vanilla' with 512 in_channels
 90: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 90: making attention of type 'vanilla' with 512 in_channels
 90: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 90: Loaded ViT-H-14 model config.
 30: Computed feature activations of size torch.Size([235, 2048])
 30: making attention of type 'vanilla' with 512 in_channels
 30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 30: making attention of type 'vanilla' with 512 in_channels
 30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 30: Loaded ViT-H-14 model config.
 88: Computed feature activations of size torch.Size([234, 2048])
 88: making attention of type 'vanilla' with 512 in_channels
 88: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 88: making attention of type 'vanilla' with 512 in_channels
 88: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 88: Loaded ViT-H-14 model config.
 94: Computed feature activations of size torch.Size([234, 2048])
 94: making attention of type 'vanilla' with 512 in_channels
 94: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 94: making attention of type 'vanilla' with 512 in_channels
 94: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 94: Loaded ViT-H-14 model config.
 69: Computed feature activations of size torch.Size([234, 2048])
 69: making attention of type 'vanilla' with 512 in_channels
 69: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 69: making attention of type 'vanilla' with 512 in_channels
 69: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 69: Loaded ViT-H-14 model config.
 80: Computed feature activations of size torch.Size([234, 2048])
 80: making attention of type 'vanilla' with 512 in_channels
 80: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 80: making attention of type 'vanilla' with 512 in_channels
 80: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 80: Loaded ViT-H-14 model config.
 71: Computed feature activations of size torch.Size([234, 2048])
 71: making attention of type 'vanilla' with 512 in_channels
 71: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 71: making attention of type 'vanilla' with 512 in_channels
 71: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 71: Loaded ViT-H-14 model config.
 25: Computed feature activations of size torch.Size([235, 2048])
 25: making attention of type 'vanilla' with 512 in_channels
 25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 25: making attention of type 'vanilla' with 512 in_channels
 25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 25: Loaded ViT-H-14 model config.
 27: Computed feature activations of size torch.Size([235, 2048])
 27: making attention of type 'vanilla' with 512 in_channels
 27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 27: making attention of type 'vanilla' with 512 in_channels
 27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 27: Loaded ViT-H-14 model config.
106: Computed feature activations of size torch.Size([234, 2048])
106: making attention of type 'vanilla' with 512 in_channels
106: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
106: making attention of type 'vanilla' with 512 in_channels
106: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
106: Loaded ViT-H-14 model config.
 60: Computed feature activations of size torch.Size([234, 2048])
 60: making attention of type 'vanilla' with 512 in_channels
 60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 60: making attention of type 'vanilla' with 512 in_channels
 60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 60: Loaded ViT-H-14 model config.
 84: Computed feature activations of size torch.Size([234, 2048])
 84: making attention of type 'vanilla' with 512 in_channels
 84: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 84: making attention of type 'vanilla' with 512 in_channels
 84: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 84: Loaded ViT-H-14 model config.
 59: Computed feature activations of size torch.Size([234, 2048])
 59: making attention of type 'vanilla' with 512 in_channels
 59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 59: making attention of type 'vanilla' with 512 in_channels
 59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 59: Loaded ViT-H-14 model config.
 52: Computed feature activations of size torch.Size([234, 2048])
 52: making attention of type 'vanilla' with 512 in_channels
 52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 52: making attention of type 'vanilla' with 512 in_channels
 52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 52: Loaded ViT-H-14 model config.
 87: Computed feature activations of size torch.Size([234, 2048])
 87: making attention of type 'vanilla' with 512 in_channels
 87: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 87: making attention of type 'vanilla' with 512 in_channels
 87: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 87: Loaded ViT-H-14 model config.
 85: Computed feature activations of size torch.Size([234, 2048])
 85: making attention of type 'vanilla' with 512 in_channels
 85: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 85: making attention of type 'vanilla' with 512 in_channels
 85: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 85: Loaded ViT-H-14 model config.
107: Computed feature activations of size torch.Size([234, 2048])
107: making attention of type 'vanilla' with 512 in_channels
107: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
107: making attention of type 'vanilla' with 512 in_channels
107: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
107: Loaded ViT-H-14 model config.
 49: Computed feature activations of size torch.Size([234, 2048])
 49: making attention of type 'vanilla' with 512 in_channels
 49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 49: making attention of type 'vanilla' with 512 in_channels
 49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 49: Loaded ViT-H-14 model config.
 86: Computed feature activations of size torch.Size([234, 2048])
 86: making attention of type 'vanilla' with 512 in_channels
 86: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 86: making attention of type 'vanilla' with 512 in_channels
 86: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 86: Loaded ViT-H-14 model config.
 48: Computed feature activations of size torch.Size([234, 2048])
 48: making attention of type 'vanilla' with 512 in_channels
 48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 48: making attention of type 'vanilla' with 512 in_channels
 48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 48: Loaded ViT-H-14 model config.
 61: Computed feature activations of size torch.Size([234, 2048])
 61: making attention of type 'vanilla' with 512 in_channels
 61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 61: making attention of type 'vanilla' with 512 in_channels
 61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 61: Loaded ViT-H-14 model config.
110: Computed feature activations of size torch.Size([234, 2048])
110: making attention of type 'vanilla' with 512 in_channels
110: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
110: making attention of type 'vanilla' with 512 in_channels
110: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
110: Loaded ViT-H-14 model config.
 17: Computed feature activations of size torch.Size([235, 2048])
 17: making attention of type 'vanilla' with 512 in_channels
 17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 17: making attention of type 'vanilla' with 512 in_channels
 17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 17: Loaded ViT-H-14 model config.
 44: Computed feature activations of size torch.Size([235, 2048])
 44: making attention of type 'vanilla' with 512 in_channels
 44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 44: making attention of type 'vanilla' with 512 in_channels
 44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 44: Loaded ViT-H-14 model config.
 81: Computed feature activations of size torch.Size([234, 2048])
 81: making attention of type 'vanilla' with 512 in_channels
 81: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 81: making attention of type 'vanilla' with 512 in_channels
 81: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 81: Loaded ViT-H-14 model config.
 16: Computed feature activations of size torch.Size([235, 2048])
 16: making attention of type 'vanilla' with 512 in_channels
 16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 16: making attention of type 'vanilla' with 512 in_channels
 16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 16: Loaded ViT-H-14 model config.
 42: Computed feature activations of size torch.Size([235, 2048])
 42: making attention of type 'vanilla' with 512 in_channels
 42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: making attention of type 'vanilla' with 512 in_channels
 42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 42: Loaded ViT-H-14 model config.
 41: Computed feature activations of size torch.Size([235, 2048])
 41: making attention of type 'vanilla' with 512 in_channels
 41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 41: making attention of type 'vanilla' with 512 in_channels
 41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 41: Loaded ViT-H-14 model config.
 40: Computed feature activations of size torch.Size([235, 2048])
 40: making attention of type 'vanilla' with 512 in_channels
 40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 40: making attention of type 'vanilla' with 512 in_channels
 40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 40: Loaded ViT-H-14 model config.
 45: Computed feature activations of size torch.Size([235, 2048])
 45: making attention of type 'vanilla' with 512 in_channels
 45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 45: making attention of type 'vanilla' with 512 in_channels
 45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 45: Loaded ViT-H-14 model config.
 19: Computed feature activations of size torch.Size([235, 2048])
 19: making attention of type 'vanilla' with 512 in_channels
 19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 19: making attention of type 'vanilla' with 512 in_channels
 19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 19: Loaded ViT-H-14 model config.
 21: Computed feature activations of size torch.Size([235, 2048])
 21: making attention of type 'vanilla' with 512 in_channels
 21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 21: making attention of type 'vanilla' with 512 in_channels
 21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 21: Loaded ViT-H-14 model config.
 20: Computed feature activations of size torch.Size([235, 2048])
 20: making attention of type 'vanilla' with 512 in_channels
 20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 20: making attention of type 'vanilla' with 512 in_channels
 20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 20: Loaded ViT-H-14 model config.
 82: Computed feature activations of size torch.Size([234, 2048])
 82: making attention of type 'vanilla' with 512 in_channels
 82: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 82: making attention of type 'vanilla' with 512 in_channels
 82: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 82: Loaded ViT-H-14 model config.
 38: Computed feature activations of size torch.Size([235, 2048])
 38: making attention of type 'vanilla' with 512 in_channels
 38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 38: making attention of type 'vanilla' with 512 in_channels
 38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 38: Loaded ViT-H-14 model config.
 37: Computed feature activations of size torch.Size([235, 2048])
 37: making attention of type 'vanilla' with 512 in_channels
 37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 37: making attention of type 'vanilla' with 512 in_channels
 37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 37: Loaded ViT-H-14 model config.
 32: Computed feature activations of size torch.Size([235, 2048])
 32: making attention of type 'vanilla' with 512 in_channels
 32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 32: making attention of type 'vanilla' with 512 in_channels
 32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 32: Loaded ViT-H-14 model config.
 83: Computed feature activations of size torch.Size([234, 2048])
 83: making attention of type 'vanilla' with 512 in_channels
 83: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 83: making attention of type 'vanilla' with 512 in_channels
 83: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 83: Loaded ViT-H-14 model config.
 43: Computed feature activations of size torch.Size([235, 2048])
 43: making attention of type 'vanilla' with 512 in_channels
 43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 43: making attention of type 'vanilla' with 512 in_channels
 43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 43: Loaded ViT-H-14 model config.
 47: Computed feature activations of size torch.Size([235, 2048])
 47: making attention of type 'vanilla' with 512 in_channels
 47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 47: making attention of type 'vanilla' with 512 in_channels
 47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 47: Loaded ViT-H-14 model config.
 46: Computed feature activations of size torch.Size([235, 2048])
 46: making attention of type 'vanilla' with 512 in_channels
 46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 46: making attention of type 'vanilla' with 512 in_channels
 46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 46: Loaded ViT-H-14 model config.
118: Computed feature activations of size torch.Size([234, 2048])
118: making attention of type 'vanilla' with 512 in_channels
118: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
118: making attention of type 'vanilla' with 512 in_channels
118: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
118: Loaded ViT-H-14 model config.
116: Computed feature activations of size torch.Size([234, 2048])
116: making attention of type 'vanilla' with 512 in_channels
116: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
116: making attention of type 'vanilla' with 512 in_channels
116: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
116: Loaded ViT-H-14 model config.
 18: Computed feature activations of size torch.Size([235, 2048])
 18: making attention of type 'vanilla' with 512 in_channels
 18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 18: making attention of type 'vanilla' with 512 in_channels
 18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 18: Loaded ViT-H-14 model config.
 22: Computed feature activations of size torch.Size([235, 2048])
 22: making attention of type 'vanilla' with 512 in_channels
 22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 22: making attention of type 'vanilla' with 512 in_channels
 22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 22: Loaded ViT-H-14 model config.
115: Computed feature activations of size torch.Size([234, 2048])
115: making attention of type 'vanilla' with 512 in_channels
115: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
115: making attention of type 'vanilla' with 512 in_channels
115: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
115: Loaded ViT-H-14 model config.
119: Computed feature activations of size torch.Size([234, 2048])
119: making attention of type 'vanilla' with 512 in_channels
119: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
119: making attention of type 'vanilla' with 512 in_channels
119: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
119: Loaded ViT-H-14 model config.
 23: Computed feature activations of size torch.Size([235, 2048])
 23: making attention of type 'vanilla' with 512 in_channels
 23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 23: making attention of type 'vanilla' with 512 in_channels
 23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 23: Loaded ViT-H-14 model config.
114: Computed feature activations of size torch.Size([234, 2048])
114: making attention of type 'vanilla' with 512 in_channels
114: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
114: making attention of type 'vanilla' with 512 in_channels
114: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
114: Loaded ViT-H-14 model config.
113: Computed feature activations of size torch.Size([234, 2048])
113: making attention of type 'vanilla' with 512 in_channels
113: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
113: making attention of type 'vanilla' with 512 in_channels
113: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
113: Loaded ViT-H-14 model config.
 35: Computed feature activations of size torch.Size([235, 2048])
 35: making attention of type 'vanilla' with 512 in_channels
 35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 35: making attention of type 'vanilla' with 512 in_channels
 35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 35: Loaded ViT-H-14 model config.
 39: Computed feature activations of size torch.Size([235, 2048])
 39: making attention of type 'vanilla' with 512 in_channels
 39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 39: making attention of type 'vanilla' with 512 in_channels
 39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 39: Loaded ViT-H-14 model config.
 34: Computed feature activations of size torch.Size([235, 2048])
 34: making attention of type 'vanilla' with 512 in_channels
 34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 34: making attention of type 'vanilla' with 512 in_channels
 34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 34: Loaded ViT-H-14 model config.
 36: Computed feature activations of size torch.Size([235, 2048])
 36: making attention of type 'vanilla' with 512 in_channels
 36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 36: making attention of type 'vanilla' with 512 in_channels
 36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 36: Loaded ViT-H-14 model config.
 33: Computed feature activations of size torch.Size([235, 2048])
 33: making attention of type 'vanilla' with 512 in_channels
 33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 33: making attention of type 'vanilla' with 512 in_channels
 33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 33: Loaded ViT-H-14 model config.
112: Computed feature activations of size torch.Size([234, 2048])
112: making attention of type 'vanilla' with 512 in_channels
112: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
112: making attention of type 'vanilla' with 512 in_channels
112: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
112: Loaded ViT-H-14 model config.
117: Computed feature activations of size torch.Size([234, 2048])
117: making attention of type 'vanilla' with 512 in_channels
117: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: making attention of type 'vanilla' with 512 in_channels
117: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
117: Loaded ViT-H-14 model config.
  2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
103: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
100: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 97: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
125: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
123: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
102: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
120: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 72: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 78: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 95: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 75: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 92: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 91: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 89: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 74: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 73: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 70: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 76: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 66: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 65: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 79: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
111: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 68: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
105: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
109: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
108: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
104: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 67: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 99: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 87: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 80: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 98: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
127: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 96: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 86: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 84: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
101: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
124: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
126: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 85: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
121: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
122: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 77: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 93: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 90: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 64: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 88: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 94: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 81: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 69: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 71: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
107: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
106: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
110: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:36:51 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
  0: [NeMo I 2024-05-10 05:36:51 ddpm:261] It has 1242 entries
 38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:36:51 ddpm:262] Existing model has 1240 entries
  0: [NeMo I 2024-05-10 05:36:51 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 82: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
118: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
119: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
116: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
114: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
115: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 83: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
113: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:36:51 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
  0: [NeMo I 2024-05-10 05:36:51 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
  0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
  0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
  0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
  0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
  0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
  0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
  0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
  0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
  0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
  0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
  0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
  0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
  0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
  0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
  0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
  0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
  0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
  0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
  0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
  0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
  0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
  0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
  0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
  0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
  0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
  0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
  0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
  0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
  0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
  0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
  0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
  0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
  0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
  0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
  0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
  0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
  0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
  0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
  0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
  0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
  0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
  0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
  0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
  0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
  0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
  0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
  0: [NeMo I 2024-05-10 05:36:51 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
112: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
117: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Global ID: 0, local ID: 0, world size: 128
  0: Rank 0 before barrier
  1: Global ID: 1, local ID: 1, world size: 128
  1: Rank 1 before barrier
  2: Global ID: 2, local ID: 2, world size: 128
  2: Rank 2 before barrier
  3: Global ID: 3, local ID: 3, world size: 128
  3: Rank 3 before barrier
  4: Global ID: 4, local ID: 4, world size: 128
  4: Rank 4 before barrier
  5: Global ID: 5, local ID: 5, world size: 128
  5: Rank 5 before barrier
  6: Global ID: 6, local ID: 6, world size: 128
  6: Rank 6 before barrier
  7: Global ID: 7, local ID: 7, world size: 128
  7: Rank 7 before barrier
  8: Global ID: 8, local ID: 0, world size: 128
  8: Rank 8 before barrier
  9: Global ID: 9, local ID: 1, world size: 128
  9: Rank 9 before barrier
 10: Global ID: 10, local ID: 2, world size: 128
 10: Rank 10 before barrier
 11: Global ID: 11, local ID: 3, world size: 128
 11: Rank 11 before barrier
 12: Global ID: 12, local ID: 4, world size: 128
 12: Rank 12 before barrier
 13: Global ID: 13, local ID: 5, world size: 128
 13: Rank 13 before barrier
 14: Global ID: 14, local ID: 6, world size: 128
 14: Rank 14 before barrier
 15: Global ID: 15, local ID: 7, world size: 128
 15: Rank 15 before barrier
 16: Global ID: 16, local ID: 0, world size: 128
 16: Rank 16 before barrier
 17: Global ID: 17, local ID: 1, world size: 128
 17: Rank 17 before barrier
 18: Global ID: 18, local ID: 2, world size: 128
 18: Rank 18 before barrier
 19: Global ID: 19, local ID: 3, world size: 128
 19: Rank 19 before barrier
 20: Global ID: 20, local ID: 4, world size: 128
 20: Rank 20 before barrier
 21: Global ID: 21, local ID: 5, world size: 128
 21: Rank 21 before barrier
 22: Global ID: 22, local ID: 6, world size: 128
 22: Rank 22 before barrier
 23: Global ID: 23, local ID: 7, world size: 128
 23: Rank 23 before barrier
 24: Global ID: 24, local ID: 0, world size: 128
 24: Rank 24 before barrier
 25: Global ID: 25, local ID: 1, world size: 128
 25: Rank 25 before barrier
 26: Global ID: 26, local ID: 2, world size: 128
 26: Rank 26 before barrier
 27: Global ID: 27, local ID: 3, world size: 128
 27: Rank 27 before barrier
 28: Global ID: 28, local ID: 4, world size: 128
 28: Rank 28 before barrier
 29: Global ID: 29, local ID: 5, world size: 128
 29: Rank 29 before barrier
 30: Global ID: 30, local ID: 6, world size: 128
 30: Rank 30 before barrier
 31: Global ID: 31, local ID: 7, world size: 128
 31: Rank 31 before barrier
 32: Global ID: 32, local ID: 0, world size: 128
 32: Rank 32 before barrier
 33: Global ID: 33, local ID: 1, world size: 128
 33: Rank 33 before barrier
 34: Global ID: 34, local ID: 2, world size: 128
 34: Rank 34 before barrier
 35: Global ID: 35, local ID: 3, world size: 128
 35: Rank 35 before barrier
 36: Global ID: 36, local ID: 4, world size: 128
 36: Rank 36 before barrier
 37: Global ID: 37, local ID: 5, world size: 128
 37: Rank 37 before barrier
 38: Global ID: 38, local ID: 6, world size: 128
 38: Rank 38 before barrier
 39: Global ID: 39, local ID: 7, world size: 128
 39: Rank 39 before barrier
 40: Global ID: 40, local ID: 0, world size: 128
 40: Rank 40 before barrier
 41: Global ID: 41, local ID: 1, world size: 128
 41: Rank 41 before barrier
 42: Global ID: 42, local ID: 2, world size: 128
 42: Rank 42 before barrier
 43: Global ID: 43, local ID: 3, world size: 128
 43: Rank 43 before barrier
 44: Global ID: 44, local ID: 4, world size: 128
 44: Rank 44 before barrier
 45: Global ID: 45, local ID: 5, world size: 128
 45: Rank 45 before barrier
 46: Global ID: 46, local ID: 6, world size: 128
 46: Rank 46 before barrier
 47: Global ID: 47, local ID: 7, world size: 128
 47: Rank 47 before barrier
 48: Global ID: 48, local ID: 0, world size: 128
 48: Rank 48 before barrier
 49: Global ID: 49, local ID: 1, world size: 128
 49: Rank 49 before barrier
 50: Global ID: 50, local ID: 2, world size: 128
 50: Rank 50 before barrier
 51: Global ID: 51, local ID: 3, world size: 128
 51: Rank 51 before barrier
 52: Global ID: 52, local ID: 4, world size: 128
 52: Rank 52 before barrier
 53: Global ID: 53, local ID: 5, world size: 128
 53: Rank 53 before barrier
 54: Global ID: 54, local ID: 6, world size: 128
 54: Rank 54 before barrier
 55: Global ID: 55, local ID: 7, world size: 128
 55: Rank 55 before barrier
 56: Global ID: 56, local ID: 0, world size: 128
 56: Rank 56 before barrier
 57: Global ID: 57, local ID: 1, world size: 128
 57: Rank 57 before barrier
 58: Global ID: 58, local ID: 2, world size: 128
 58: Rank 58 before barrier
 59: Global ID: 59, local ID: 3, world size: 128
 59: Rank 59 before barrier
 60: Global ID: 60, local ID: 4, world size: 128
 60: Rank 60 before barrier
 61: Global ID: 61, local ID: 5, world size: 128
 61: Rank 61 before barrier
 62: Global ID: 62, local ID: 6, world size: 128
 62: Rank 62 before barrier
 63: Global ID: 63, local ID: 7, world size: 128
 63: Rank 63 before barrier
 64: Global ID: 64, local ID: 0, world size: 128
 64: Rank 64 before barrier
 65: Global ID: 65, local ID: 1, world size: 128
 65: Rank 65 before barrier
 66: Global ID: 66, local ID: 2, world size: 128
 66: Rank 66 before barrier
 67: Global ID: 67, local ID: 3, world size: 128
 67: Rank 67 before barrier
 68: Global ID: 68, local ID: 4, world size: 128
 68: Rank 68 before barrier
 69: Global ID: 69, local ID: 5, world size: 128
 69: Rank 69 before barrier
 70: Global ID: 70, local ID: 6, world size: 128
 70: Rank 70 before barrier
 71: Global ID: 71, local ID: 7, world size: 128
 71: Rank 71 before barrier
 72: Global ID: 72, local ID: 0, world size: 128
 72: Rank 72 before barrier
 73: Global ID: 73, local ID: 1, world size: 128
 73: Rank 73 before barrier
 74: Global ID: 74, local ID: 2, world size: 128
 74: Rank 74 before barrier
 75: Global ID: 75, local ID: 3, world size: 128
 75: Rank 75 before barrier
 76: Global ID: 76, local ID: 4, world size: 128
 76: Rank 76 before barrier
 77: Global ID: 77, local ID: 5, world size: 128
 77: Rank 77 before barrier
 78: Global ID: 78, local ID: 6, world size: 128
 78: Rank 78 before barrier
 79: Global ID: 79, local ID: 7, world size: 128
 79: Rank 79 before barrier
 80: Global ID: 80, local ID: 0, world size: 128
 80: Rank 80 before barrier
 81: Global ID: 81, local ID: 1, world size: 128
 81: Rank 81 before barrier
 82: Global ID: 82, local ID: 2, world size: 128
 82: Rank 82 before barrier
 83: Global ID: 83, local ID: 3, world size: 128
 83: Rank 83 before barrier
 84: Global ID: 84, local ID: 4, world size: 128
 84: Rank 84 before barrier
 85: Global ID: 85, local ID: 5, world size: 128
 85: Rank 85 before barrier
 86: Global ID: 86, local ID: 6, world size: 128
 86: Rank 86 before barrier
 87: Global ID: 87, local ID: 7, world size: 128
 87: Rank 87 before barrier
 88: Global ID: 88, local ID: 0, world size: 128
 88: Rank 88 before barrier
 89: Global ID: 89, local ID: 1, world size: 128
 89: Rank 89 before barrier
 90: Global ID: 90, local ID: 2, world size: 128
 90: Rank 90 before barrier
 91: Global ID: 91, local ID: 3, world size: 128
 91: Rank 91 before barrier
 92: Global ID: 92, local ID: 4, world size: 128
 92: Rank 92 before barrier
 93: Global ID: 93, local ID: 5, world size: 128
 93: Rank 93 before barrier
 94: Global ID: 94, local ID: 6, world size: 128
 94: Rank 94 before barrier
 95: Global ID: 95, local ID: 7, world size: 128
 95: Rank 95 before barrier
 96: Global ID: 96, local ID: 0, world size: 128
 96: Rank 96 before barrier
 97: Global ID: 97, local ID: 1, world size: 128
 97: Rank 97 before barrier
 98: Global ID: 98, local ID: 2, world size: 128
 98: Rank 98 before barrier
 99: Global ID: 99, local ID: 3, world size: 128
 99: Rank 99 before barrier
100: Global ID: 100, local ID: 4, world size: 128
100: Rank 100 before barrier
101: Global ID: 101, local ID: 5, world size: 128
101: Rank 101 before barrier
102: Global ID: 102, local ID: 6, world size: 128
102: Rank 102 before barrier
103: Global ID: 103, local ID: 7, world size: 128
103: Rank 103 before barrier
104: Global ID: 104, local ID: 0, world size: 128
104: Rank 104 before barrier
105: Global ID: 105, local ID: 1, world size: 128
105: Rank 105 before barrier
106: Global ID: 106, local ID: 2, world size: 128
106: Rank 106 before barrier
107: Global ID: 107, local ID: 3, world size: 128
107: Rank 107 before barrier
108: Global ID: 108, local ID: 4, world size: 128
108: Rank 108 before barrier
109: Global ID: 109, local ID: 5, world size: 128
109: Rank 109 before barrier
110: Global ID: 110, local ID: 6, world size: 128
110: Rank 110 before barrier
111: Global ID: 111, local ID: 7, world size: 128
111: Rank 111 before barrier
112: Global ID: 112, local ID: 0, world size: 128
112: Rank 112 before barrier
113: Global ID: 113, local ID: 1, world size: 128
113: Rank 113 before barrier
114: Global ID: 114, local ID: 2, world size: 128
114: Rank 114 before barrier
115: Global ID: 115, local ID: 3, world size: 128
115: Rank 115 before barrier
116: Global ID: 116, local ID: 4, world size: 128
116: Rank 116 before barrier
117: Global ID: 117, local ID: 5, world size: 128
117: Rank 117 before barrier
118: Global ID: 118, local ID: 6, world size: 128
118: Rank 118 before barrier
119: Global ID: 119, local ID: 7, world size: 128
119: Rank 119 before barrier
120: Global ID: 120, local ID: 0, world size: 128
120: Rank 120 before barrier
121: Global ID: 121, local ID: 1, world size: 128
121: Rank 121 before barrier
122: Global ID: 122, local ID: 2, world size: 128
122: Rank 122 before barrier
123: Global ID: 123, local ID: 3, world size: 128
123: Rank 123 before barrier
124: Global ID: 124, local ID: 4, world size: 128
124: Rank 124 before barrier
125: Global ID: 125, local ID: 5, world size: 128
125: Rank 125 before barrier
126: Global ID: 126, local ID: 6, world size: 128
126: Rank 126 before barrier
127: Global ID: 127, local ID: 7, world size: 128
127: Rank 127 before barrier
  0: :::MLLOG {"namespace": "", "time_ms": 1715319419392, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 2560000}}
  0: Assigned 235 prompts for this worker.
  0: :::MLLOG {"namespace": "", "time_ms": 1715319495918, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 2560000}}
 32: Assigned 235 prompts for this worker.
 88: Assigned 234 prompts for this worker.
 40: Assigned 235 prompts for this worker.
 80: Assigned 234 prompts for this worker.
 16: Assigned 235 prompts for this worker.
120: Assigned 234 prompts for this worker.
 96: Assigned 234 prompts for this worker.
 41: Assigned 235 prompts for this worker.
 24: Assigned 235 prompts for this worker.
  8: Assigned 235 prompts for this worker.
112: Assigned 234 prompts for this worker.
 25: Assigned 235 prompts for this worker.
 97: Assigned 234 prompts for this worker.
 57: Assigned 234 prompts for this worker.
 65: Assigned 234 prompts for this worker.
 18: Assigned 235 prompts for this worker.
 89: Assigned 234 prompts for this worker.
114: Assigned 234 prompts for this worker.
 72: Assigned 234 prompts for this worker.
 49: Assigned 234 prompts for this worker.
  9: Assigned 235 prompts for this worker.
 33: Assigned 235 prompts for this worker.
 19: Assigned 235 prompts for this worker.
 64: Assigned 234 prompts for this worker.
121: Assigned 234 prompts for this worker.
 10: Assigned 235 prompts for this worker.
 66: Assigned 234 prompts for this worker.
 73: Assigned 234 prompts for this worker.
104: Assigned 234 prompts for this worker.
 56: Assigned 234 prompts for this worker.
 48: Assigned 234 prompts for this worker.
 50: Assigned 234 prompts for this worker.
 34: Assigned 235 prompts for this worker.
 51: Assigned 234 prompts for this worker.
 92: Assigned 234 prompts for this worker.
 11: Assigned 235 prompts for this worker.
 99: Assigned 234 prompts for this worker.
122: Assigned 234 prompts for this worker.
113: Assigned 234 prompts for this worker.
123: Assigned 234 prompts for this worker.
 26: Assigned 235 prompts for this worker.
 17: Assigned 235 prompts for this worker.
 58: Assigned 234 prompts for this worker.
 74: Assigned 234 prompts for this worker.
105: Assigned 234 prompts for this worker.
 42: Assigned 235 prompts for this worker.
 36: Assigned 235 prompts for this worker.
107: Assigned 234 prompts for this worker.
 91: Assigned 234 prompts for this worker.
 81: Assigned 234 prompts for this worker.
115: Assigned 234 prompts for this worker.
  1: Assigned 235 prompts for this worker.
 67: Assigned 234 prompts for this worker.
 20: Assigned 235 prompts for this worker.
 75: Assigned 234 prompts for this worker.
108: Assigned 234 prompts for this worker.
 12: Assigned 235 prompts for this worker.
 90: Assigned 234 prompts for this worker.
125: Assigned 234 prompts for this worker.
 52: Assigned 234 prompts for this worker.
 98: Assigned 234 prompts for this worker.
  2: Assigned 235 prompts for this worker.
 82: Assigned 234 prompts for this worker.
  3: Assigned 235 prompts for this worker.
 43: Assigned 235 prompts for this worker.
 27: Assigned 235 prompts for this worker.
 68: Assigned 234 prompts for this worker.
 83: Assigned 234 prompts for this worker.
 76: Assigned 234 prompts for this worker.
 59: Assigned 234 prompts for this worker.
 77: Assigned 234 prompts for this worker.
124: Assigned 234 prompts for this worker.
106: Assigned 234 prompts for this worker.
 35: Assigned 235 prompts for this worker.
117: Assigned 234 prompts for this worker.
 55: Assigned 234 prompts for this worker.
 13: Assigned 235 prompts for this worker.
 44: Assigned 235 prompts for this worker.
 21: Assigned 235 prompts for this worker.
 45: Assigned 235 prompts for this worker.
 46: Assigned 235 prompts for this worker.
 60: Assigned 234 prompts for this worker.
 28: Assigned 235 prompts for this worker.
 29: Assigned 235 prompts for this worker.
 93: Assigned 234 prompts for this worker.
  4: Assigned 235 prompts for this worker.
109: Assigned 234 prompts for this worker.
100: Assigned 234 prompts for this worker.
126: Assigned 234 prompts for this worker.
 53: Assigned 234 prompts for this worker.
 84: Assigned 234 prompts for this worker.
 47: Assigned 235 prompts for this worker.
 22: Assigned 235 prompts for this worker.
 14: Assigned 235 prompts for this worker.
 23: Assigned 235 prompts for this worker.
 78: Assigned 234 prompts for this worker.
 30: Assigned 235 prompts for this worker.
 94: Assigned 234 prompts for this worker.
 61: Assigned 234 prompts for this worker.
101: Assigned 234 prompts for this worker.
116: Assigned 234 prompts for this worker.
111: Assigned 234 prompts for this worker.
 95: Assigned 234 prompts for this worker.
 62: Assigned 234 prompts for this worker.
103: Assigned 234 prompts for this worker.
 85: Assigned 234 prompts for this worker.
  5: Assigned 235 prompts for this worker.
 54: Assigned 234 prompts for this worker.
110: Assigned 234 prompts for this worker.
 37: Assigned 235 prompts for this worker.
102: Assigned 234 prompts for this worker.
 70: Assigned 234 prompts for this worker.
 86: Assigned 234 prompts for this worker.
  6: Assigned 235 prompts for this worker.
 79: Assigned 234 prompts for this worker.
127: Assigned 234 prompts for this worker.
 15: Assigned 235 prompts for this worker.
 38: Assigned 235 prompts for this worker.
 31: Assigned 235 prompts for this worker.
 69: Assigned 234 prompts for this worker.
118: Assigned 234 prompts for this worker.
  7: Assigned 235 prompts for this worker.
 63: Assigned 234 prompts for this worker.
 71: Assigned 234 prompts for this worker.
119: Assigned 234 prompts for this worker.
 39: Assigned 235 prompts for this worker.
 87: Assigned 234 prompts for this worker.
  0: Calculating FID activations:   0%|          | 0/8 [00:00<?, ?it/s]Calculating FID activations:  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Calculating FID activations:  62%|██████▎   | 5/8 [00:00<00:00,  8.63it/s]Calculating FID activations: 100%|██████████| 8/8 [00:00<00:00,  8.15it/s]
  0: Computed feature activations of size torch.Size([235, 2048])
  0: :::MLLOG {"namespace": "", "time_ms": 1715319504990, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 98.56991740633237, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 2560000, "metric": "FID"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319509950, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1519721895456314, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 2560000, "metric": "CLIP"}}
  0: Using 16bit Automatic Mixed Precision (AMP)
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: IPU available: False, using: 0 IPUs
  0: HPU available: False, using: 0 HPUs
  0: [NeMo W 2024-05-10 05:38:29 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:279] Ranks 0 has data parallel rank: 0
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:287] Rank 0 has context parallel group: [0]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:291] Ranks 0 has context parallel rank: 0
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:298] Rank 0 has model parallel group: [0]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:308] Rank 0 has tensor model parallel group: [0]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:313] Rank 0 has tensor model parallel rank: 0
  7: [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:345] Rank 0 has embedding group: [0]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:352] Rank 0 has pipeline model parallel rank 0
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127]]
  0: [NeMo I 2024-05-10 05:38:31 megatron_init:354] Rank 0 has embedding rank: 0
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
  0: [NeMo W 2024-05-10 05:38:31 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 8, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': No
  0: ne, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
  0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 24612, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
  0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
  0: [NeMo I 2024-05-10 05:38:31 ddpm:130] LatentDiffusion: Running in v-prediction mode
  0: [NeMo I 2024-05-10 05:38:31 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  5: [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:31 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  4: [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:31 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:38:31 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:38:31 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:38:32 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:38:32 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:38:34 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
  0: [NeMo I 2024-05-10 05:38:34 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 11: [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  8: [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [rank57]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: [rank56]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [rank63]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [rank58]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [rank60]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [rank59]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [rank61]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [rank62]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [rank92]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [rank95]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [rank91]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [rank88]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [rank94]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: [rank89]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [rank90]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [rank93]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [rank50]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [rank55]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: [rank51]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [rank49]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: [rank53]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: [rank48]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [rank52]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [rank54]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:34 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 74: [rank74]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [rank73]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [rank77]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [rank72]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [rank75]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [rank78]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [rank76]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [rank79]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [rank104]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [rank108]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [rank109]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [rank111]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [rank105]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [rank106]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [rank107]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [rank110]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [rank123]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [rank125]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: [rank120]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [rank126]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [rank122]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [rank127]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [rank124]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [rank121]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [rank98]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [rank100]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: [rank103]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [rank97]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [rank102]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: [rank96]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [rank101]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [rank99]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [rank65]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [rank66]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [rank68]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [rank70]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [rank64]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [rank67]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [rank69]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [rank71]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [rank85]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [rank81]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [rank86]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [rank87]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [rank84]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: [rank80]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [rank83]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [rank82]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 32: [rank32]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [rank38]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [rank36]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [rank37]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [rank34]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [rank39]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: [rank33]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [rank35]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
  0: [NeMo I 2024-05-10 05:38:35 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 44: [rank44]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: [rank40]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [rank45]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [rank46]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [rank42]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [rank41]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [rank47]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [rank43]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: [rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: [NeMo I 2024-05-10 05:38:35 utils:92] DiffusionWrapper has 865.91 M params.
  0: [NeMo I 2024-05-10 05:38:35 ddpm:168] Use system random generator since CUDA graph enabled
114: [rank114]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [rank118]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [rank119]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [rank115]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [rank113]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
112: [rank112]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [rank116]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [rank117]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: making attention of type 'vanilla' with 512 in_channels
  0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  0: making attention of type 'vanilla' with 512 in_channels
  0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  0: Loaded ViT-H-14 model config.
  6: Computed feature activations of size torch.Size([235, 2048])
  6: making attention of type 'vanilla' with 512 in_channels
  6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  6: making attention of type 'vanilla' with 512 in_channels
  6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  6: Loaded ViT-H-14 model config.
  7: Computed feature activations of size torch.Size([235, 2048])
  7: making attention of type 'vanilla' with 512 in_channels
  7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  7: making attention of type 'vanilla' with 512 in_channels
  7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  7: Loaded ViT-H-14 model config.
  2: Computed feature activations of size torch.Size([235, 2048])
  2: making attention of type 'vanilla' with 512 in_channels
  2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  2: making attention of type 'vanilla' with 512 in_channels
  2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  2: Loaded ViT-H-14 model config.
  3: Computed feature activations of size torch.Size([235, 2048])
  3: making attention of type 'vanilla' with 512 in_channels
  3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  3: making attention of type 'vanilla' with 512 in_channels
  3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  3: Loaded ViT-H-14 model config.
  4: Computed feature activations of size torch.Size([235, 2048])
  4: making attention of type 'vanilla' with 512 in_channels
  4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  4: making attention of type 'vanilla' with 512 in_channels
  4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  4: Loaded ViT-H-14 model config.
  5: Computed feature activations of size torch.Size([235, 2048])
  5: making attention of type 'vanilla' with 512 in_channels
  5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  5: making attention of type 'vanilla' with 512 in_channels
  5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  5: Loaded ViT-H-14 model config.
  1: Computed feature activations of size torch.Size([235, 2048])
  1: making attention of type 'vanilla' with 512 in_channels
  1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  1: making attention of type 'vanilla' with 512 in_channels
  1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  1: Loaded ViT-H-14 model config.
 56: Computed feature activations of size torch.Size([234, 2048])
 56: making attention of type 'vanilla' with 512 in_channels
 56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 56: making attention of type 'vanilla' with 512 in_channels
 56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 56: Loaded ViT-H-14 model config.
 58: Computed feature activations of size torch.Size([234, 2048])
 58: making attention of type 'vanilla' with 512 in_channels
 58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 58: making attention of type 'vanilla' with 512 in_channels
 58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 58: Loaded ViT-H-14 model config.
 57: Computed feature activations of size torch.Size([234, 2048])
 57: making attention of type 'vanilla' with 512 in_channels
 57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 57: making attention of type 'vanilla' with 512 in_channels
 57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 57: Loaded ViT-H-14 model config.
 50: Computed feature activations of size torch.Size([234, 2048])
 50: making attention of type 'vanilla' with 512 in_channels
 50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 50: making attention of type 'vanilla' with 512 in_channels
 50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 50: Loaded ViT-H-14 model config.
 12: Computed feature activations of size torch.Size([235, 2048])
 12: making attention of type 'vanilla' with 512 in_channels
 12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 12: making attention of type 'vanilla' with 512 in_channels
 12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 12: Loaded ViT-H-14 model config.
 11: Computed feature activations of size torch.Size([235, 2048])
 11: making attention of type 'vanilla' with 512 in_channels
 11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 11: making attention of type 'vanilla' with 512 in_channels
 11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 11: Loaded ViT-H-14 model config.
 63: Computed feature activations of size torch.Size([234, 2048])
 63: making attention of type 'vanilla' with 512 in_channels
 63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 63: making attention of type 'vanilla' with 512 in_channels
 63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 63: Loaded ViT-H-14 model config.
 49: Computed feature activations of size torch.Size([234, 2048])
 49: making attention of type 'vanilla' with 512 in_channels
 49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 49: making attention of type 'vanilla' with 512 in_channels
 49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 49: Loaded ViT-H-14 model config.
 92: Computed feature activations of size torch.Size([234, 2048])
 92: making attention of type 'vanilla' with 512 in_channels
 92: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 92: making attention of type 'vanilla' with 512 in_channels
 92: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 92: Loaded ViT-H-14 model config.
 91: Computed feature activations of size torch.Size([234, 2048])
 91: making attention of type 'vanilla' with 512 in_channels
 91: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 91: making attention of type 'vanilla' with 512 in_channels
 91: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 91: Loaded ViT-H-14 model config.
 95: Computed feature activations of size torch.Size([234, 2048])
 95: making attention of type 'vanilla' with 512 in_channels
 95: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 95: making attention of type 'vanilla' with 512 in_channels
 95: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 95: Loaded ViT-H-14 model config.
 55: Computed feature activations of size torch.Size([234, 2048])
 55: making attention of type 'vanilla' with 512 in_channels
 55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 55: making attention of type 'vanilla' with 512 in_channels
 55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 55: Loaded ViT-H-14 model config.
 72: Computed feature activations of size torch.Size([234, 2048])
 72: making attention of type 'vanilla' with 512 in_channels
 72: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 72: making attention of type 'vanilla' with 512 in_channels
 72: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 72: Loaded ViT-H-14 model config.
 53: Computed feature activations of size torch.Size([234, 2048])
 53: making attention of type 'vanilla' with 512 in_channels
 53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 53: making attention of type 'vanilla' with 512 in_channels
 53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 53: Loaded ViT-H-14 model config.
 94: Computed feature activations of size torch.Size([234, 2048])
 94: making attention of type 'vanilla' with 512 in_channels
 94: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 94: making attention of type 'vanilla' with 512 in_channels
 94: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 94: Loaded ViT-H-14 model config.
 51: Computed feature activations of size torch.Size([234, 2048])
 51: making attention of type 'vanilla' with 512 in_channels
 51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 51: making attention of type 'vanilla' with 512 in_channels
 51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 51: Loaded ViT-H-14 model config.
 89: Computed feature activations of size torch.Size([234, 2048])
 89: making attention of type 'vanilla' with 512 in_channels
 89: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 89: making attention of type 'vanilla' with 512 in_channels
 89: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 89: Loaded ViT-H-14 model config.
 73: Computed feature activations of size torch.Size([234, 2048])
 73: making attention of type 'vanilla' with 512 in_channels
 73: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 73: making attention of type 'vanilla' with 512 in_channels
 73: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 73: Loaded ViT-H-14 model config.
 74: Computed feature activations of size torch.Size([234, 2048])
 74: making attention of type 'vanilla' with 512 in_channels
 74: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 74: making attention of type 'vanilla' with 512 in_channels
 74: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 74: Loaded ViT-H-14 model config.
 75: Computed feature activations of size torch.Size([234, 2048])
 75: making attention of type 'vanilla' with 512 in_channels
 75: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 75: making attention of type 'vanilla' with 512 in_channels
 75: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 75: Loaded ViT-H-14 model config.
111: Computed feature activations of size torch.Size([234, 2048])
111: making attention of type 'vanilla' with 512 in_channels
111: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
111: making attention of type 'vanilla' with 512 in_channels
111: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
111: Loaded ViT-H-14 model config.
 79: Computed feature activations of size torch.Size([234, 2048])
 79: making attention of type 'vanilla' with 512 in_channels
 79: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 79: making attention of type 'vanilla' with 512 in_channels
 79: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 79: Loaded ViT-H-14 model config.
 13: Computed feature activations of size torch.Size([235, 2048])
 13: making attention of type 'vanilla' with 512 in_channels
 13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 13: making attention of type 'vanilla' with 512 in_channels
 13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 13: Loaded ViT-H-14 model config.
105: Computed feature activations of size torch.Size([234, 2048])
105: making attention of type 'vanilla' with 512 in_channels
105: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
105: making attention of type 'vanilla' with 512 in_channels
105: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
105: Loaded ViT-H-14 model config.
104: Computed feature activations of size torch.Size([234, 2048])
104: making attention of type 'vanilla' with 512 in_channels
104: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
104: making attention of type 'vanilla' with 512 in_channels
104: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
104: Loaded ViT-H-14 model config.
 66: Computed feature activations of size torch.Size([234, 2048])
 66: making attention of type 'vanilla' with 512 in_channels
 66: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 66: making attention of type 'vanilla' with 512 in_channels
 66: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 66: Loaded ViT-H-14 model config.
 76: Computed feature activations of size torch.Size([234, 2048])
 76: making attention of type 'vanilla' with 512 in_channels
 76: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 76: making attention of type 'vanilla' with 512 in_channels
 76: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 76: Loaded ViT-H-14 model config.
103: Computed feature activations of size torch.Size([234, 2048])
103: making attention of type 'vanilla' with 512 in_channels
103: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
103: making attention of type 'vanilla' with 512 in_channels
103: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
103: Loaded ViT-H-14 model config.
120: Computed feature activations of size torch.Size([234, 2048])
120: making attention of type 'vanilla' with 512 in_channels
120: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
120: making attention of type 'vanilla' with 512 in_channels
120: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
120: Loaded ViT-H-14 model config.
 98: Computed feature activations of size torch.Size([234, 2048])
 98: making attention of type 'vanilla' with 512 in_channels
 98: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 98: making attention of type 'vanilla' with 512 in_channels
 98: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 98: Loaded ViT-H-14 model config.
126: Computed feature activations of size torch.Size([234, 2048])
126: making attention of type 'vanilla' with 512 in_channels
126: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
126: making attention of type 'vanilla' with 512 in_channels
126: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
126: Loaded ViT-H-14 model config.
127: Computed feature activations of size torch.Size([234, 2048])
127: making attention of type 'vanilla' with 512 in_channels
127: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
127: making attention of type 'vanilla' with 512 in_channels
127: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
127: Loaded ViT-H-14 model config.
 26: Computed feature activations of size torch.Size([235, 2048])
 26: making attention of type 'vanilla' with 512 in_channels
 26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 26: making attention of type 'vanilla' with 512 in_channels
 26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 26: Loaded ViT-H-14 model config.
 97: Computed feature activations of size torch.Size([234, 2048])
 97: making attention of type 'vanilla' with 512 in_channels
 97: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 97: making attention of type 'vanilla' with 512 in_channels
 97: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 97: Loaded ViT-H-14 model config.
 15: Computed feature activations of size torch.Size([235, 2048])
 15: making attention of type 'vanilla' with 512 in_channels
 15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 15: making attention of type 'vanilla' with 512 in_channels
 15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 15: Loaded ViT-H-14 model config.
 80: Computed feature activations of size torch.Size([234, 2048])
 80: making attention of type 'vanilla' with 512 in_channels
 80: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 80: making attention of type 'vanilla' with 512 in_channels
 80: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 80: Loaded ViT-H-14 model config.
109: Computed feature activations of size torch.Size([234, 2048])
109: making attention of type 'vanilla' with 512 in_channels
109: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
109: making attention of type 'vanilla' with 512 in_channels
109: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
109: Loaded ViT-H-14 model config.
 70: Computed feature activations of size torch.Size([234, 2048])
 70: making attention of type 'vanilla' with 512 in_channels
 70: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 70: making attention of type 'vanilla' with 512 in_channels
 70: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 70: Loaded ViT-H-14 model config.
108: Computed feature activations of size torch.Size([234, 2048])
108: making attention of type 'vanilla' with 512 in_channels
108: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
108: making attention of type 'vanilla' with 512 in_channels
108: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
108: Loaded ViT-H-14 model config.
125: Computed feature activations of size torch.Size([234, 2048])
125: making attention of type 'vanilla' with 512 in_channels
125: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
125: making attention of type 'vanilla' with 512 in_channels
125: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
125: Loaded ViT-H-14 model config.
 65: Computed feature activations of size torch.Size([234, 2048])
 65: making attention of type 'vanilla' with 512 in_channels
 65: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 65: making attention of type 'vanilla' with 512 in_channels
 65: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 65: Loaded ViT-H-14 model config.
 68: Computed feature activations of size torch.Size([234, 2048])
 68: making attention of type 'vanilla' with 512 in_channels
 68: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 68: making attention of type 'vanilla' with 512 in_channels
 68: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 68: Loaded ViT-H-14 model config.
 86: Computed feature activations of size torch.Size([234, 2048])
 86: making attention of type 'vanilla' with 512 in_channels
 86: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 86: making attention of type 'vanilla' with 512 in_channels
 86: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 86: Loaded ViT-H-14 model config.
100: Computed feature activations of size torch.Size([234, 2048])
100: making attention of type 'vanilla' with 512 in_channels
100: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
100: making attention of type 'vanilla' with 512 in_channels
100: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
100: Loaded ViT-H-14 model config.
  8: Computed feature activations of size torch.Size([235, 2048])
  8: making attention of type 'vanilla' with 512 in_channels
  8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  8: making attention of type 'vanilla' with 512 in_channels
  8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  8: Loaded ViT-H-14 model config.
 62: Computed feature activations of size torch.Size([234, 2048])
 62: making attention of type 'vanilla' with 512 in_channels
 62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 62: making attention of type 'vanilla' with 512 in_channels
 62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 62: Loaded ViT-H-14 model config.
123: Computed feature activations of size torch.Size([234, 2048])
123: making attention of type 'vanilla' with 512 in_channels
123: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
123: making attention of type 'vanilla' with 512 in_channels
123: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
123: Loaded ViT-H-14 model config.
 84: Computed feature activations of size torch.Size([234, 2048])
 84: making attention of type 'vanilla' with 512 in_channels
 84: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 84: making attention of type 'vanilla' with 512 in_channels
 84: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 84: Loaded ViT-H-14 model config.
 85: Computed feature activations of size torch.Size([234, 2048])
 85: making attention of type 'vanilla' with 512 in_channels
 85: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 85: making attention of type 'vanilla' with 512 in_channels
 85: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 85: Loaded ViT-H-14 model config.
 87: Computed feature activations of size torch.Size([234, 2048])
 87: making attention of type 'vanilla' with 512 in_channels
 87: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 87: making attention of type 'vanilla' with 512 in_channels
 87: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 87: Loaded ViT-H-14 model config.
 67: Computed feature activations of size torch.Size([234, 2048])
 67: making attention of type 'vanilla' with 512 in_channels
 67: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 67: making attention of type 'vanilla' with 512 in_channels
 67: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 67: Loaded ViT-H-14 model config.
 29: Computed feature activations of size torch.Size([235, 2048])
 29: making attention of type 'vanilla' with 512 in_channels
 29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 29: making attention of type 'vanilla' with 512 in_channels
 29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 29: Loaded ViT-H-14 model config.
 59: Computed feature activations of size torch.Size([234, 2048])
 59: making attention of type 'vanilla' with 512 in_channels
 59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 59: making attention of type 'vanilla' with 512 in_channels
 59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 59: Loaded ViT-H-14 model config.
 90: Computed feature activations of size torch.Size([234, 2048])
 90: making attention of type 'vanilla' with 512 in_channels
 90: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 90: making attention of type 'vanilla' with 512 in_channels
 90: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 90: Loaded ViT-H-14 model config.
 28: Computed feature activations of size torch.Size([235, 2048])
 28: making attention of type 'vanilla' with 512 in_channels
 28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 28: making attention of type 'vanilla' with 512 in_channels
 28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 28: Loaded ViT-H-14 model config.
 82: Computed feature activations of size torch.Size([234, 2048])
 82: making attention of type 'vanilla' with 512 in_channels
 82: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 82: making attention of type 'vanilla' with 512 in_channels
 82: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 82: Loaded ViT-H-14 model config.
 31: Computed feature activations of size torch.Size([235, 2048])
 31: making attention of type 'vanilla' with 512 in_channels
 31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 31: making attention of type 'vanilla' with 512 in_channels
 31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 31: Loaded ViT-H-14 model config.
  9: Computed feature activations of size torch.Size([235, 2048])
  9: making attention of type 'vanilla' with 512 in_channels
  9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
  9: making attention of type 'vanilla' with 512 in_channels
  9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
  9: Loaded ViT-H-14 model config.
 81: Computed feature activations of size torch.Size([234, 2048])
 81: making attention of type 'vanilla' with 512 in_channels
 81: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 81: making attention of type 'vanilla' with 512 in_channels
 81: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 81: Loaded ViT-H-14 model config.
121: Computed feature activations of size torch.Size([234, 2048])
121: making attention of type 'vanilla' with 512 in_channels
121: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
121: making attention of type 'vanilla' with 512 in_channels
121: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
121: Loaded ViT-H-14 model config.
 10: Computed feature activations of size torch.Size([235, 2048])
 10: making attention of type 'vanilla' with 512 in_channels
 10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 10: making attention of type 'vanilla' with 512 in_channels
 10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 10: Loaded ViT-H-14 model config.
 93: Computed feature activations of size torch.Size([234, 2048])
 93: making attention of type 'vanilla' with 512 in_channels
 93: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 93: making attention of type 'vanilla' with 512 in_channels
 93: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 93: Loaded ViT-H-14 model config.
 61: Computed feature activations of size torch.Size([234, 2048])
 61: making attention of type 'vanilla' with 512 in_channels
 61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 61: making attention of type 'vanilla' with 512 in_channels
 61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 61: Loaded ViT-H-14 model config.
 60: Computed feature activations of size torch.Size([234, 2048])
 60: making attention of type 'vanilla' with 512 in_channels
 60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 60: making attention of type 'vanilla' with 512 in_channels
 60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 60: Loaded ViT-H-14 model config.
 54: Computed feature activations of size torch.Size([234, 2048])
 54: making attention of type 'vanilla' with 512 in_channels
 54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 54: making attention of type 'vanilla' with 512 in_channels
 54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 54: Loaded ViT-H-14 model config.
 48: Computed feature activations of size torch.Size([234, 2048])
 48: making attention of type 'vanilla' with 512 in_channels
 48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 48: making attention of type 'vanilla' with 512 in_channels
 48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 48: Loaded ViT-H-14 model config.
 88: Computed feature activations of size torch.Size([234, 2048])
 88: making attention of type 'vanilla' with 512 in_channels
 88: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 88: making attention of type 'vanilla' with 512 in_channels
 88: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 88: Loaded ViT-H-14 model config.
 77: Computed feature activations of size torch.Size([234, 2048])
 77: making attention of type 'vanilla' with 512 in_channels
 77: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 77: making attention of type 'vanilla' with 512 in_channels
 77: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 77: Loaded ViT-H-14 model config.
 52: Computed feature activations of size torch.Size([234, 2048])
 52: making attention of type 'vanilla' with 512 in_channels
 52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 52: making attention of type 'vanilla' with 512 in_channels
 52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 52: Loaded ViT-H-14 model config.
 14: Computed feature activations of size torch.Size([235, 2048])
 14: making attention of type 'vanilla' with 512 in_channels
 14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 14: making attention of type 'vanilla' with 512 in_channels
 14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 14: Loaded ViT-H-14 model config.
 78: Computed feature activations of size torch.Size([234, 2048])
 78: making attention of type 'vanilla' with 512 in_channels
 78: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 78: making attention of type 'vanilla' with 512 in_channels
 78: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 78: Loaded ViT-H-14 model config.
106: Computed feature activations of size torch.Size([234, 2048])
106: making attention of type 'vanilla' with 512 in_channels
106: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
106: making attention of type 'vanilla' with 512 in_channels
106: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
106: Loaded ViT-H-14 model config.
110: Computed feature activations of size torch.Size([234, 2048])
110: making attention of type 'vanilla' with 512 in_channels
110: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
110: making attention of type 'vanilla' with 512 in_channels
110: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
110: Loaded ViT-H-14 model config.
 99: Computed feature activations of size torch.Size([234, 2048])
 99: making attention of type 'vanilla' with 512 in_channels
 99: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 99: making attention of type 'vanilla' with 512 in_channels
 99: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 99: Loaded ViT-H-14 model config.
107: Computed feature activations of size torch.Size([234, 2048])
107: making attention of type 'vanilla' with 512 in_channels
107: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
107: making attention of type 'vanilla' with 512 in_channels
107: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
107: Loaded ViT-H-14 model config.
 64: Computed feature activations of size torch.Size([234, 2048])
 64: making attention of type 'vanilla' with 512 in_channels
 64: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 64: making attention of type 'vanilla' with 512 in_channels
 64: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 64: Loaded ViT-H-14 model config.
122: Computed feature activations of size torch.Size([234, 2048])
122: making attention of type 'vanilla' with 512 in_channels
122: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
122: making attention of type 'vanilla' with 512 in_channels
122: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
122: Loaded ViT-H-14 model config.
 69: Computed feature activations of size torch.Size([234, 2048])
 69: making attention of type 'vanilla' with 512 in_channels
 69: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 69: making attention of type 'vanilla' with 512 in_channels
 69: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 69: Loaded ViT-H-14 model config.
 71: Computed feature activations of size torch.Size([234, 2048])
 71: making attention of type 'vanilla' with 512 in_channels
 71: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 71: making attention of type 'vanilla' with 512 in_channels
 71: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 71: Loaded ViT-H-14 model config.
 38: Computed feature activations of size torch.Size([235, 2048])
 38: making attention of type 'vanilla' with 512 in_channels
 38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 38: making attention of type 'vanilla' with 512 in_channels
 38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 38: Loaded ViT-H-14 model config.
 96: Computed feature activations of size torch.Size([234, 2048])
 96: making attention of type 'vanilla' with 512 in_channels
 96: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 96: making attention of type 'vanilla' with 512 in_channels
 96: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 96: Loaded ViT-H-14 model config.
 32: Computed feature activations of size torch.Size([235, 2048])
 32: making attention of type 'vanilla' with 512 in_channels
 32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 32: making attention of type 'vanilla' with 512 in_channels
 32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 32: Loaded ViT-H-14 model config.
 37: Computed feature activations of size torch.Size([235, 2048])
 37: making attention of type 'vanilla' with 512 in_channels
 37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 37: making attention of type 'vanilla' with 512 in_channels
 37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 37: Loaded ViT-H-14 model config.
102: Computed feature activations of size torch.Size([234, 2048])
102: making attention of type 'vanilla' with 512 in_channels
102: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
102: making attention of type 'vanilla' with 512 in_channels
102: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
102: Loaded ViT-H-14 model config.
124: Computed feature activations of size torch.Size([234, 2048])
124: making attention of type 'vanilla' with 512 in_channels
124: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
124: making attention of type 'vanilla' with 512 in_channels
124: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
124: Loaded ViT-H-14 model config.
 24: Computed feature activations of size torch.Size([235, 2048])
 24: making attention of type 'vanilla' with 512 in_channels
 24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 24: making attention of type 'vanilla' with 512 in_channels
 24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 24: Loaded ViT-H-14 model config.
 30: Computed feature activations of size torch.Size([235, 2048])
 30: making attention of type 'vanilla' with 512 in_channels
 30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 30: making attention of type 'vanilla' with 512 in_channels
 30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 30: Loaded ViT-H-14 model config.
101: Computed feature activations of size torch.Size([234, 2048])
101: making attention of type 'vanilla' with 512 in_channels
101: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
101: making attention of type 'vanilla' with 512 in_channels
101: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
101: Loaded ViT-H-14 model config.
 25: Computed feature activations of size torch.Size([235, 2048])
 25: making attention of type 'vanilla' with 512 in_channels
 25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 25: making attention of type 'vanilla' with 512 in_channels
 25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 25: Loaded ViT-H-14 model config.
 27: Computed feature activations of size torch.Size([235, 2048])
 27: making attention of type 'vanilla' with 512 in_channels
 27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 27: making attention of type 'vanilla' with 512 in_channels
 27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 27: Loaded ViT-H-14 model config.
 83: Computed feature activations of size torch.Size([234, 2048])
 83: making attention of type 'vanilla' with 512 in_channels
 83: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 83: making attention of type 'vanilla' with 512 in_channels
 83: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 83: Loaded ViT-H-14 model config.
 41: Computed feature activations of size torch.Size([235, 2048])
 41: making attention of type 'vanilla' with 512 in_channels
 41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 41: making attention of type 'vanilla' with 512 in_channels
 41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 41: Loaded ViT-H-14 model config.
 45: Computed feature activations of size torch.Size([235, 2048])
 45: making attention of type 'vanilla' with 512 in_channels
 45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 45: making attention of type 'vanilla' with 512 in_channels
 45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 45: Loaded ViT-H-14 model config.
 42: Computed feature activations of size torch.Size([235, 2048])
 42: making attention of type 'vanilla' with 512 in_channels
 42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 42: making attention of type 'vanilla' with 512 in_channels
 42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 42: Loaded ViT-H-14 model config.
 44: Computed feature activations of size torch.Size([235, 2048])
 44: making attention of type 'vanilla' with 512 in_channels
 44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 44: making attention of type 'vanilla' with 512 in_channels
 44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 44: Loaded ViT-H-14 model config.
 40: Computed feature activations of size torch.Size([235, 2048])
 40: making attention of type 'vanilla' with 512 in_channels
 40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 40: making attention of type 'vanilla' with 512 in_channels
 40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 40: Loaded ViT-H-14 model config.
 17: Computed feature activations of size torch.Size([235, 2048])
 17: making attention of type 'vanilla' with 512 in_channels
 17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 17: making attention of type 'vanilla' with 512 in_channels
 17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 17: Loaded ViT-H-14 model config.
 19: Computed feature activations of size torch.Size([235, 2048])
 19: making attention of type 'vanilla' with 512 in_channels
 19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 19: making attention of type 'vanilla' with 512 in_channels
 19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 19: Loaded ViT-H-14 model config.
 16: Computed feature activations of size torch.Size([235, 2048])
 16: making attention of type 'vanilla' with 512 in_channels
 16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 16: making attention of type 'vanilla' with 512 in_channels
 16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 16: Loaded ViT-H-14 model config.
 39: Computed feature activations of size torch.Size([235, 2048])
 39: making attention of type 'vanilla' with 512 in_channels
 39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 39: making attention of type 'vanilla' with 512 in_channels
 39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 39: Loaded ViT-H-14 model config.
 36: Computed feature activations of size torch.Size([235, 2048])
 36: making attention of type 'vanilla' with 512 in_channels
 36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 36: making attention of type 'vanilla' with 512 in_channels
 36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 36: Loaded ViT-H-14 model config.
 35: Computed feature activations of size torch.Size([235, 2048])
 35: making attention of type 'vanilla' with 512 in_channels
 35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 35: making attention of type 'vanilla' with 512 in_channels
 35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 35: Loaded ViT-H-14 model config.
 20: Computed feature activations of size torch.Size([235, 2048])
 20: making attention of type 'vanilla' with 512 in_channels
 20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 20: making attention of type 'vanilla' with 512 in_channels
 20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 20: Loaded ViT-H-14 model config.
 34: Computed feature activations of size torch.Size([235, 2048])
 34: making attention of type 'vanilla' with 512 in_channels
 34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 34: making attention of type 'vanilla' with 512 in_channels
 34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 34: Loaded ViT-H-14 model config.
 33: Computed feature activations of size torch.Size([235, 2048])
 33: making attention of type 'vanilla' with 512 in_channels
 33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 33: making attention of type 'vanilla' with 512 in_channels
 33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 33: Loaded ViT-H-14 model config.
 46: Computed feature activations of size torch.Size([235, 2048])
 46: making attention of type 'vanilla' with 512 in_channels
 46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 46: making attention of type 'vanilla' with 512 in_channels
 46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 46: Loaded ViT-H-14 model config.
 43: Computed feature activations of size torch.Size([235, 2048])
 43: making attention of type 'vanilla' with 512 in_channels
 43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 43: making attention of type 'vanilla' with 512 in_channels
 43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 43: Loaded ViT-H-14 model config.
 47: Computed feature activations of size torch.Size([235, 2048])
 47: making attention of type 'vanilla' with 512 in_channels
 47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 47: making attention of type 'vanilla' with 512 in_channels
 47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 47: Loaded ViT-H-14 model config.
116: Computed feature activations of size torch.Size([234, 2048])
116: making attention of type 'vanilla' with 512 in_channels
116: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
116: making attention of type 'vanilla' with 512 in_channels
116: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
116: Loaded ViT-H-14 model config.
118: Computed feature activations of size torch.Size([234, 2048])
118: making attention of type 'vanilla' with 512 in_channels
118: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
118: making attention of type 'vanilla' with 512 in_channels
118: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
118: Loaded ViT-H-14 model config.
117: Computed feature activations of size torch.Size([234, 2048])
117: making attention of type 'vanilla' with 512 in_channels
117: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
117: making attention of type 'vanilla' with 512 in_channels
117: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
117: Loaded ViT-H-14 model config.
119: Computed feature activations of size torch.Size([234, 2048])
119: making attention of type 'vanilla' with 512 in_channels
119: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
119: making attention of type 'vanilla' with 512 in_channels
119: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
119: Loaded ViT-H-14 model config.
115: Computed feature activations of size torch.Size([234, 2048])
115: making attention of type 'vanilla' with 512 in_channels
115: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
115: making attention of type 'vanilla' with 512 in_channels
115: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
115: Loaded ViT-H-14 model config.
112: Computed feature activations of size torch.Size([234, 2048])
112: making attention of type 'vanilla' with 512 in_channels
112: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
112: making attention of type 'vanilla' with 512 in_channels
112: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
112: Loaded ViT-H-14 model config.
 18: Computed feature activations of size torch.Size([235, 2048])
 18: making attention of type 'vanilla' with 512 in_channels
 18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 18: making attention of type 'vanilla' with 512 in_channels
 18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 18: Loaded ViT-H-14 model config.
 21: Computed feature activations of size torch.Size([235, 2048])
 21: making attention of type 'vanilla' with 512 in_channels
 21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 21: making attention of type 'vanilla' with 512 in_channels
 21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 21: Loaded ViT-H-14 model config.
 22: Computed feature activations of size torch.Size([235, 2048])
 22: making attention of type 'vanilla' with 512 in_channels
 22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 22: making attention of type 'vanilla' with 512 in_channels
 22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 22: Loaded ViT-H-14 model config.
 23: Computed feature activations of size torch.Size([235, 2048])
 23: making attention of type 'vanilla' with 512 in_channels
 23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 23: making attention of type 'vanilla' with 512 in_channels
 23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 23: Loaded ViT-H-14 model config.
113: Computed feature activations of size torch.Size([234, 2048])
113: making attention of type 'vanilla' with 512 in_channels
113: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
113: making attention of type 'vanilla' with 512 in_channels
113: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
113: Loaded ViT-H-14 model config.
114: Computed feature activations of size torch.Size([234, 2048])
114: making attention of type 'vanilla' with 512 in_channels
114: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
114: making attention of type 'vanilla' with 512 in_channels
114: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
114: Loaded ViT-H-14 model config.
  0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 95: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
111: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 66: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
125: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 70: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 80: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 98: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 91: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 92: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 94: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 74: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 75: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 73: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
104: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 72: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 65: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
108: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 89: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
103: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
109: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
100: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 68: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 87: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
123: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
127: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 93: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 85: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
120: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
105: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 67: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 79: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 84: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
126: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 76: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 86: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 77: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
121: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 82: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 97: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 81: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 88: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 78: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 90: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
122: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 64: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
102: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 83: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
106: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 99: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
124: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
107: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 69: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
110: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 96: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
101: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:38:46 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
  0: [NeMo I 2024-05-10 05:38:46 ddpm:261] It has 1242 entries
  0: [NeMo I 2024-05-10 05:38:46 ddpm:262] Existing model has 1240 entries
 71: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:38:46 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
119: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
118: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
115: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
112: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: [NeMo I 2024-05-10 05:38:46 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
  0: [NeMo I 2024-05-10 05:38:46 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
  0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
  0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
  0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
  0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
  0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
  0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
  0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
  0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
  0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
  0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
  0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
  0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
  0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
  0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
  0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
  0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
  0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
  0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
  0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
  0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
  0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
  0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
  0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
  0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
  0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
  0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
  0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
  0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
  0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
  0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
  0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
  0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
  0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
  0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
  0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
  0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
  0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
  0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
  0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
  0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
  0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
  0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
  0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
  0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
  0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
  0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
  0: [NeMo I 2024-05-10 05:38:46 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
116: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
117: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
114: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
113: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
  0: Global ID: 0, local ID: 0, world size: 128
  0: Rank 0 before barrier
  1: Global ID: 1, local ID: 1, world size: 128
  1: Rank 1 before barrier
  2: Global ID: 2, local ID: 2, world size: 128
  2: Rank 2 before barrier
  3: Global ID: 3, local ID: 3, world size: 128
  3: Rank 3 before barrier
  4: Global ID: 4, local ID: 4, world size: 128
  4: Rank 4 before barrier
  5: Global ID: 5, local ID: 5, world size: 128
  5: Rank 5 before barrier
  6: Global ID: 6, local ID: 6, world size: 128
  6: Rank 6 before barrier
  7: Global ID: 7, local ID: 7, world size: 128
  7: Rank 7 before barrier
  8: Global ID: 8, local ID: 0, world size: 128
  8: Rank 8 before barrier
  9: Global ID: 9, local ID: 1, world size: 128
  9: Rank 9 before barrier
 10: Global ID: 10, local ID: 2, world size: 128
 10: Rank 10 before barrier
 11: Global ID: 11, local ID: 3, world size: 128
 11: Rank 11 before barrier
 12: Global ID: 12, local ID: 4, world size: 128
 12: Rank 12 before barrier
 13: Global ID: 13, local ID: 5, world size: 128
 13: Rank 13 before barrier
 14: Global ID: 14, local ID: 6, world size: 128
 14: Rank 14 before barrier
 15: Global ID: 15, local ID: 7, world size: 128
 15: Rank 15 before barrier
 16: Global ID: 16, local ID: 0, world size: 128
 16: Rank 16 before barrier
 17: Global ID: 17, local ID: 1, world size: 128
 17: Rank 17 before barrier
 18: Global ID: 18, local ID: 2, world size: 128
 18: Rank 18 before barrier
 19: Global ID: 19, local ID: 3, world size: 128
 19: Rank 19 before barrier
 20: Global ID: 20, local ID: 4, world size: 128
 20: Rank 20 before barrier
 21: Global ID: 21, local ID: 5, world size: 128
 21: Rank 21 before barrier
 22: Global ID: 22, local ID: 6, world size: 128
 22: Rank 22 before barrier
 23: Global ID: 23, local ID: 7, world size: 128
 23: Rank 23 before barrier
 24: Global ID: 24, local ID: 0, world size: 128
 24: Rank 24 before barrier
 25: Global ID: 25, local ID: 1, world size: 128
 25: Rank 25 before barrier
 26: Global ID: 26, local ID: 2, world size: 128
 26: Rank 26 before barrier
 27: Global ID: 27, local ID: 3, world size: 128
 27: Rank 27 before barrier
 28: Global ID: 28, local ID: 4, world size: 128
 28: Rank 28 before barrier
 29: Global ID: 29, local ID: 5, world size: 128
 29: Rank 29 before barrier
 30: Global ID: 30, local ID: 6, world size: 128
 30: Rank 30 before barrier
 31: Global ID: 31, local ID: 7, world size: 128
 31: Rank 31 before barrier
 32: Global ID: 32, local ID: 0, world size: 128
 32: Rank 32 before barrier
 33: Global ID: 33, local ID: 1, world size: 128
 33: Rank 33 before barrier
 34: Global ID: 34, local ID: 2, world size: 128
 34: Rank 34 before barrier
 35: Global ID: 35, local ID: 3, world size: 128
 35: Rank 35 before barrier
 36: Global ID: 36, local ID: 4, world size: 128
 36: Rank 36 before barrier
 37: Global ID: 37, local ID: 5, world size: 128
 37: Rank 37 before barrier
 38: Global ID: 38, local ID: 6, world size: 128
 38: Rank 38 before barrier
 39: Global ID: 39, local ID: 7, world size: 128
 39: Rank 39 before barrier
 40: Global ID: 40, local ID: 0, world size: 128
 40: Rank 40 before barrier
 41: Global ID: 41, local ID: 1, world size: 128
 41: Rank 41 before barrier
 42: Global ID: 42, local ID: 2, world size: 128
 42: Rank 42 before barrier
 43: Global ID: 43, local ID: 3, world size: 128
 43: Rank 43 before barrier
 44: Global ID: 44, local ID: 4, world size: 128
 44: Rank 44 before barrier
 45: Global ID: 45, local ID: 5, world size: 128
 45: Rank 45 before barrier
 46: Global ID: 46, local ID: 6, world size: 128
 46: Rank 46 before barrier
 47: Global ID: 47, local ID: 7, world size: 128
 47: Rank 47 before barrier
 48: Global ID: 48, local ID: 0, world size: 128
 48: Rank 48 before barrier
 49: Global ID: 49, local ID: 1, world size: 128
 49: Rank 49 before barrier
 50: Global ID: 50, local ID: 2, world size: 128
 50: Rank 50 before barrier
 51: Global ID: 51, local ID: 3, world size: 128
 51: Rank 51 before barrier
 52: Global ID: 52, local ID: 4, world size: 128
 52: Rank 52 before barrier
 53: Global ID: 53, local ID: 5, world size: 128
 53: Rank 53 before barrier
 54: Global ID: 54, local ID: 6, world size: 128
 54: Rank 54 before barrier
 55: Global ID: 55, local ID: 7, world size: 128
 55: Rank 55 before barrier
 56: Global ID: 56, local ID: 0, world size: 128
 56: Rank 56 before barrier
 57: Global ID: 57, local ID: 1, world size: 128
 57: Rank 57 before barrier
 58: Global ID: 58, local ID: 2, world size: 128
 58: Rank 58 before barrier
 59: Global ID: 59, local ID: 3, world size: 128
 59: Rank 59 before barrier
 60: Global ID: 60, local ID: 4, world size: 128
 60: Rank 60 before barrier
 61: Global ID: 61, local ID: 5, world size: 128
 61: Rank 61 before barrier
 62: Global ID: 62, local ID: 6, world size: 128
 62: Rank 62 before barrier
 63: Global ID: 63, local ID: 7, world size: 128
 63: Rank 63 before barrier
 64: Global ID: 64, local ID: 0, world size: 128
 64: Rank 64 before barrier
 65: Global ID: 65, local ID: 1, world size: 128
 65: Rank 65 before barrier
 66: Global ID: 66, local ID: 2, world size: 128
 66: Rank 66 before barrier
 67: Global ID: 67, local ID: 3, world size: 128
 67: Rank 67 before barrier
 68: Global ID: 68, local ID: 4, world size: 128
 68: Rank 68 before barrier
 69: Global ID: 69, local ID: 5, world size: 128
 69: Rank 69 before barrier
 70: Global ID: 70, local ID: 6, world size: 128
 70: Rank 70 before barrier
 71: Global ID: 71, local ID: 7, world size: 128
 71: Rank 71 before barrier
 72: Global ID: 72, local ID: 0, world size: 128
 72: Rank 72 before barrier
 73: Global ID: 73, local ID: 1, world size: 128
 73: Rank 73 before barrier
 74: Global ID: 74, local ID: 2, world size: 128
 74: Rank 74 before barrier
 75: Global ID: 75, local ID: 3, world size: 128
 75: Rank 75 before barrier
 76: Global ID: 76, local ID: 4, world size: 128
 76: Rank 76 before barrier
 77: Global ID: 77, local ID: 5, world size: 128
 77: Rank 77 before barrier
 78: Global ID: 78, local ID: 6, world size: 128
 78: Rank 78 before barrier
 79: Global ID: 79, local ID: 7, world size: 128
 79: Rank 79 before barrier
 80: Global ID: 80, local ID: 0, world size: 128
 80: Rank 80 before barrier
 81: Global ID: 81, local ID: 1, world size: 128
 81: Rank 81 before barrier
 82: Global ID: 82, local ID: 2, world size: 128
 82: Rank 82 before barrier
 83: Global ID: 83, local ID: 3, world size: 128
 83: Rank 83 before barrier
 84: Global ID: 84, local ID: 4, world size: 128
 84: Rank 84 before barrier
 85: Global ID: 85, local ID: 5, world size: 128
 85: Rank 85 before barrier
 86: Global ID: 86, local ID: 6, world size: 128
 86: Rank 86 before barrier
 87: Global ID: 87, local ID: 7, world size: 128
 87: Rank 87 before barrier
 88: Global ID: 88, local ID: 0, world size: 128
 88: Rank 88 before barrier
 89: Global ID: 89, local ID: 1, world size: 128
 89: Rank 89 before barrier
 90: Global ID: 90, local ID: 2, world size: 128
 90: Rank 90 before barrier
 91: Global ID: 91, local ID: 3, world size: 128
 91: Rank 91 before barrier
 92: Global ID: 92, local ID: 4, world size: 128
 92: Rank 92 before barrier
 93: Global ID: 93, local ID: 5, world size: 128
 93: Rank 93 before barrier
 94: Global ID: 94, local ID: 6, world size: 128
 94: Rank 94 before barrier
 95: Global ID: 95, local ID: 7, world size: 128
 95: Rank 95 before barrier
 96: Global ID: 96, local ID: 0, world size: 128
 96: Rank 96 before barrier
 97: Global ID: 97, local ID: 1, world size: 128
 97: Rank 97 before barrier
 98: Global ID: 98, local ID: 2, world size: 128
 98: Rank 98 before barrier
 99: Global ID: 99, local ID: 3, world size: 128
 99: Rank 99 before barrier
100: Global ID: 100, local ID: 4, world size: 128
100: Rank 100 before barrier
101: Global ID: 101, local ID: 5, world size: 128
101: Rank 101 before barrier
102: Global ID: 102, local ID: 6, world size: 128
102: Rank 102 before barrier
103: Global ID: 103, local ID: 7, world size: 128
103: Rank 103 before barrier
104: Global ID: 104, local ID: 0, world size: 128
104: Rank 104 before barrier
105: Global ID: 105, local ID: 1, world size: 128
105: Rank 105 before barrier
106: Global ID: 106, local ID: 2, world size: 128
106: Rank 106 before barrier
107: Global ID: 107, local ID: 3, world size: 128
107: Rank 107 before barrier
108: Global ID: 108, local ID: 4, world size: 128
108: Rank 108 before barrier
109: Global ID: 109, local ID: 5, world size: 128
109: Rank 109 before barrier
110: Global ID: 110, local ID: 6, world size: 128
110: Rank 110 before barrier
111: Global ID: 111, local ID: 7, world size: 128
111: Rank 111 before barrier
112: Global ID: 112, local ID: 0, world size: 128
112: Rank 112 before barrier
113: Global ID: 113, local ID: 1, world size: 128
113: Rank 113 before barrier
114: Global ID: 114, local ID: 2, world size: 128
114: Rank 114 before barrier
115: Global ID: 115, local ID: 3, world size: 128
115: Rank 115 before barrier
116: Global ID: 116, local ID: 4, world size: 128
116: Rank 116 before barrier
117: Global ID: 117, local ID: 5, world size: 128
117: Rank 117 before barrier
118: Global ID: 118, local ID: 6, world size: 128
118: Rank 118 before barrier
119: Global ID: 119, local ID: 7, world size: 128
119: Rank 119 before barrier
120: Global ID: 120, local ID: 0, world size: 128
120: Rank 120 before barrier
121: Global ID: 121, local ID: 1, world size: 128
121: Rank 121 before barrier
122: Global ID: 122, local ID: 2, world size: 128
122: Rank 122 before barrier
123: Global ID: 123, local ID: 3, world size: 128
123: Rank 123 before barrier
124: Global ID: 124, local ID: 4, world size: 128
124: Rank 124 before barrier
125: Global ID: 125, local ID: 5, world size: 128
125: Rank 125 before barrier
126: Global ID: 126, local ID: 6, world size: 128
126: Rank 126 before barrier
127: Global ID: 127, local ID: 7, world size: 128
127: Rank 127 before barrier
  0: :::MLLOG {"namespace": "", "time_ms": 1715319535369, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 3072000}}
  0: Assigned 235 prompts for this worker.
  0: :::MLLOG {"namespace": "", "time_ms": 1715319612482, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 3072000}}
120: Assigned 234 prompts for this worker.
 32: Assigned 235 prompts for this worker.
 88: Assigned 234 prompts for this worker.
112: Assigned 234 prompts for this worker.
 40: Assigned 235 prompts for this worker.
  8: Assigned 235 prompts for this worker.
 33: Assigned 235 prompts for this worker.
 64: Assigned 234 prompts for this worker.
 34: Assigned 235 prompts for this worker.
 72: Assigned 234 prompts for this worker.
121: Assigned 234 prompts for this worker.
 48: Assigned 234 prompts for this worker.
  2: Assigned 235 prompts for this worker.
 41: Assigned 235 prompts for this worker.
 10: Assigned 235 prompts for this worker.
 65: Assigned 234 prompts for this worker.
 56: Assigned 234 prompts for this worker.
104: Assigned 234 prompts for this worker.
 89: Assigned 234 prompts for this worker.
 97: Assigned 234 prompts for this worker.
  1: Assigned 235 prompts for this worker.
122: Assigned 234 prompts for this worker.
  9: Assigned 235 prompts for this worker.
 73: Assigned 234 prompts for this worker.
 50: Assigned 234 prompts for this worker.
 49: Assigned 234 prompts for this worker.
 19: Assigned 235 prompts for this worker.
 57: Assigned 234 prompts for this worker.
 36: Assigned 235 prompts for this worker.
 58: Assigned 234 prompts for this worker.
 96: Assigned 234 prompts for this worker.
 66: Assigned 234 prompts for this worker.
 42: Assigned 235 prompts for this worker.
 98: Assigned 234 prompts for this worker.
 91: Assigned 234 prompts for this worker.
 80: Assigned 234 prompts for this worker.
 74: Assigned 234 prompts for this worker.
113: Assigned 234 prompts for this worker.
 59: Assigned 234 prompts for this worker.
  3: Assigned 235 prompts for this worker.
 67: Assigned 234 prompts for this worker.
 11: Assigned 235 prompts for this worker.
 16: Assigned 235 prompts for this worker.
 35: Assigned 235 prompts for this worker.
 92: Assigned 234 prompts for this worker.
 24: Assigned 235 prompts for this worker.
 75: Assigned 234 prompts for this worker.
 51: Assigned 234 prompts for this worker.
 81: Assigned 234 prompts for this worker.
114: Assigned 234 prompts for this worker.
 60: Assigned 234 prompts for this worker.
 12: Assigned 235 prompts for this worker.
 17: Assigned 235 prompts for this worker.
123: Assigned 234 prompts for this worker.
 18: Assigned 235 prompts for this worker.
 90: Assigned 234 prompts for this worker.
 77: Assigned 234 prompts for this worker.
105: Assigned 234 prompts for this worker.
 43: Assigned 235 prompts for this worker.
 76: Assigned 234 prompts for this worker.
 52: Assigned 234 prompts for this worker.
 99: Assigned 234 prompts for this worker.
 13: Assigned 235 prompts for this worker.
115: Assigned 234 prompts for this worker.
  4: Assigned 235 prompts for this worker.
 14: Assigned 235 prompts for this worker.
 20: Assigned 235 prompts for this worker.
 27: Assigned 235 prompts for this worker.
 82: Assigned 234 prompts for this worker.
116: Assigned 234 prompts for this worker.
  7: Assigned 235 prompts for this worker.
 79: Assigned 234 prompts for this worker.
125: Assigned 234 prompts for this worker.
106: Assigned 234 prompts for this worker.
 22: Assigned 235 prompts for this worker.
 37: Assigned 235 prompts for this worker.
 95: Assigned 234 prompts for this worker.
100: Assigned 234 prompts for this worker.
 26: Assigned 235 prompts for this worker.
 78: Assigned 234 prompts for this worker.
 55: Assigned 234 prompts for this worker.
107: Assigned 234 prompts for this worker.
 38: Assigned 235 prompts for this worker.
  6: Assigned 235 prompts for this worker.
124: Assigned 234 prompts for this worker.
108: Assigned 234 prompts for this worker.
 15: Assigned 235 prompts for this worker.
 23: Assigned 235 prompts for this worker.
 93: Assigned 234 prompts for this worker.
 68: Assigned 234 prompts for this worker.
 83: Assigned 234 prompts for this worker.
118: Assigned 234 prompts for this worker.
  5: Assigned 235 prompts for this worker.
126: Assigned 234 prompts for this worker.
109: Assigned 234 prompts for this worker.
 44: Assigned 235 prompts for this worker.
 21: Assigned 235 prompts for this worker.
 39: Assigned 235 prompts for this worker.
 94: Assigned 234 prompts for this worker.
101: Assigned 234 prompts for this worker.
 25: Assigned 235 prompts for this worker.
 85: Assigned 234 prompts for this worker.
 53: Assigned 234 prompts for this worker.
111: Assigned 234 prompts for this worker.
 45: Assigned 235 prompts for this worker.
 28: Assigned 235 prompts for this worker.
 84: Assigned 234 prompts for this worker.
 54: Assigned 234 prompts for this worker.
119: Assigned 234 prompts for this worker.
 86: Assigned 234 prompts for this worker.
117: Assigned 234 prompts for this worker.
127: Assigned 234 prompts for this worker.
 61: Assigned 234 prompts for this worker.
102: Assigned 234 prompts for this worker.
 69: Assigned 234 prompts for this worker.
110: Assigned 234 prompts for this worker.
 46: Assigned 235 prompts for this worker.
103: Assigned 234 prompts for this worker.
 29: Assigned 235 prompts for this worker.
 70: Assigned 234 prompts for this worker.
 47: Assigned 235 prompts for this worker.
 71: Assigned 234 prompts for this worker.
 87: Assigned 234 prompts for this worker.
 63: Assigned 234 prompts for this worker.
 62: Assigned 234 prompts for this worker.
 30: Assigned 235 prompts for this worker.
 31: Assigned 235 prompts for this worker.
  0: Calculating FID activations:   0%|          | 0/8 [00:00<?, ?it/s]Calculating FID activations:  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]Calculating FID activations:  62%|██████▎   | 5/8 [00:00<00:00,  7.78it/s]Calculating FID activations: 100%|██████████| 8/8 [00:01<00:00,  7.32it/s]
  0: Computed feature activations of size torch.Size([235, 2048])
  0: :::MLLOG {"namespace": "", "time_ms": 1715319621372, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 89.66760600391763, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 3072000, "metric": "FID"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1715319625978, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.17026467621326447, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 3072000, "metric": "CLIP"}}
  6: Computed feature activations of size torch.Size([235, 2048])
  7: Computed feature activations of size torch.Size([235, 2048])
 68: Computed feature activations of size torch.Size([234, 2048])
 74: Computed feature activations of size torch.Size([234, 2048])
 11: Computed feature activations of size torch.Size([235, 2048])
 19: Computed feature activations of size torch.Size([235, 2048])
 30: Computed feature activations of size torch.Size([235, 2048])
109: Computed feature activations of size torch.Size([234, 2048])
 91: Computed feature activations of size torch.Size([234, 2048])
 84: Computed feature activations of size torch.Size([234, 2048])
  1: Computed feature activations of size torch.Size([235, 2048])
 37: Computed feature activations of size torch.Size([235, 2048])
115: Computed feature activations of size torch.Size([234, 2048])
  2: Computed feature activations of size torch.Size([235, 2048])
121: Computed feature activations of size torch.Size([234, 2048])
 42: Computed feature activations of size torch.Size([235, 2048])
  3: Computed feature activations of size torch.Size([235, 2048])
 49: Computed feature activations of size torch.Size([234, 2048])
 97: Computed feature activations of size torch.Size([234, 2048])
 65: Computed feature activations of size torch.Size([234, 2048])
 60: Computed feature activations of size torch.Size([234, 2048])
 67: Computed feature activations of size torch.Size([234, 2048])
 66: Computed feature activations of size torch.Size([234, 2048])
108: Computed feature activations of size torch.Size([234, 2048])
 71: Computed feature activations of size torch.Size([234, 2048])
111: Computed feature activations of size torch.Size([234, 2048])
 38: Computed feature activations of size torch.Size([235, 2048])
 73: Computed feature activations of size torch.Size([234, 2048])
105: Computed feature activations of size torch.Size([234, 2048])
 10: Computed feature activations of size torch.Size([235, 2048])
 17: Computed feature activations of size torch.Size([235, 2048])
 87: Computed feature activations of size torch.Size([234, 2048])
 75: Computed feature activations of size torch.Size([234, 2048])
 12: Computed feature activations of size torch.Size([235, 2048])
 20: Computed feature activations of size torch.Size([235, 2048])
 85: Computed feature activations of size torch.Size([234, 2048])
 77: Computed feature activations of size torch.Size([234, 2048])
 13: Computed feature activations of size torch.Size([235, 2048])
 92: Computed feature activations of size torch.Size([234, 2048])
 28: Computed feature activations of size torch.Size([235, 2048])
 70: Computed feature activations of size torch.Size([234, 2048])
 81: Computed feature activations of size torch.Size([234, 2048])
118: Computed feature activations of size torch.Size([234, 2048])
  5: Computed feature activations of size torch.Size([235, 2048])
 78: Computed feature activations of size torch.Size([234, 2048])
 15: Computed feature activations of size torch.Size([235, 2048])
 44: Computed feature activations of size torch.Size([235, 2048])
 94: Computed feature activations of size torch.Size([234, 2048])
 26: Computed feature activations of size torch.Size([235, 2048])
 82: Computed feature activations of size torch.Size([234, 2048])
119: Computed feature activations of size torch.Size([234, 2048])
 46: Computed feature activations of size torch.Size([235, 2048])
 58: Computed feature activations of size torch.Size([234, 2048])
116: Computed feature activations of size torch.Size([234, 2048])
122: Computed feature activations of size torch.Size([234, 2048])
 51: Computed feature activations of size torch.Size([234, 2048])
 41: Computed feature activations of size torch.Size([235, 2048])
 16: Computed feature activations of size torch.Size([235, 2048])
 63: Computed feature activations of size torch.Size([234, 2048])
100: Computed feature activations of size torch.Size([234, 2048])
113: Computed feature activations of size torch.Size([234, 2048])
  4: Computed feature activations of size torch.Size([235, 2048])
123: Computed feature activations of size torch.Size([234, 2048])
 55: Computed feature activations of size torch.Size([234, 2048])
 45: Computed feature activations of size torch.Size([235, 2048])
 95: Computed feature activations of size torch.Size([234, 2048])
 57: Computed feature activations of size torch.Size([234, 2048])
102: Computed feature activations of size torch.Size([234, 2048])
 83: Computed feature activations of size torch.Size([234, 2048])
125: Computed feature activations of size torch.Size([234, 2048])
 53: Computed feature activations of size torch.Size([234, 2048])
 14: Computed feature activations of size torch.Size([235, 2048])
 88: Computed feature activations of size torch.Size([234, 2048])
 61: Computed feature activations of size torch.Size([234, 2048])
 98: Computed feature activations of size torch.Size([234, 2048])
 79: Computed feature activations of size torch.Size([234, 2048])
126: Computed feature activations of size torch.Size([234, 2048])
 36: Computed feature activations of size torch.Size([235, 2048])
 56: Computed feature activations of size torch.Size([234, 2048])
103: Computed feature activations of size torch.Size([234, 2048])
120: Computed feature activations of size torch.Size([234, 2048])
 52: Computed feature activations of size torch.Size([234, 2048])
 39: Computed feature activations of size torch.Size([235, 2048])
 90: Computed feature activations of size torch.Size([234, 2048])
 99: Computed feature activations of size torch.Size([234, 2048])
 80: Computed feature activations of size torch.Size([234, 2048])
104: Computed feature activations of size torch.Size([234, 2048])
  8: Computed feature activations of size torch.Size([235, 2048])
 40: Computed feature activations of size torch.Size([235, 2048])
 33: Computed feature activations of size torch.Size([235, 2048])
101: Computed feature activations of size torch.Size([234, 2048])
 31: Computed feature activations of size torch.Size([235, 2048])
 86: Computed feature activations of size torch.Size([234, 2048])
117: Computed feature activations of size torch.Size([234, 2048])
 22: Computed feature activations of size torch.Size([235, 2048])
 32: Computed feature activations of size torch.Size([235, 2048])
 59: Computed feature activations of size torch.Size([234, 2048])
 64: Computed feature activations of size torch.Size([234, 2048])
 76: Computed feature activations of size torch.Size([234, 2048])
 35: Computed feature activations of size torch.Size([235, 2048])
 69: Computed feature activations of size torch.Size([234, 2048])
112: Computed feature activations of size torch.Size([234, 2048])
106: Computed feature activations of size torch.Size([234, 2048])
 21: Computed feature activations of size torch.Size([235, 2048])
 34: Computed feature activations of size torch.Size([235, 2048])
 89: Computed feature activations of size torch.Size([234, 2048])
 62: Computed feature activations of size torch.Size([234, 2048])
107: Computed feature activations of size torch.Size([234, 2048])
  9: Computed feature activations of size torch.Size([235, 2048])
 23: Computed feature activations of size torch.Size([235, 2048])
110: Computed feature activations of size torch.Size([234, 2048])
 93: Computed feature activations of size torch.Size([234, 2048])
124: Computed feature activations of size torch.Size([234, 2048])
 47: Computed feature activations of size torch.Size([235, 2048])
114: Computed feature activations of size torch.Size([234, 2048])
 25: Computed feature activations of size torch.Size([235, 2048])
 18: Computed feature activations of size torch.Size([235, 2048])
 27: Computed feature activations of size torch.Size([235, 2048])
 50: Computed feature activations of size torch.Size([234, 2048])
 43: Computed feature activations of size torch.Size([235, 2048])
127: Computed feature activations of size torch.Size([234, 2048])
 54: Computed feature activations of size torch.Size([234, 2048])
 48: Computed feature activations of size torch.Size([234, 2048])
 96: Computed feature activations of size torch.Size([234, 2048])
 24: Computed feature activations of size torch.Size([235, 2048])
 72: Computed feature activations of size torch.Size([234, 2048])
 29: Computed feature activations of size torch.Size([235, 2048])
  0: :::MLLOG {"namespace": "", "time_ms": 1715319142617, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", "lineno": 186, "status": "success", "step_num": 3000}}
  2: ENDING TIMING RUN AT 2024-05-10 05:40:32 AM
  2: RESULT,stable_diffusion,953,ubuntu,2024-05-10 05:24:39 AM
  0: ENDING TIMING RUN AT 2024-05-10 05:40:32 AM
  0: RESULT,stable_diffusion,953,ubuntu,2024-05-10 05:24:39 AM
  6: ENDING TIMING RUN AT 2024-05-10 05:40:32 AM
  6: RESULT,stable_diffusion,953,ubuntu,2024-05-10 05:24:39 AM
  4: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  4: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 11: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 11: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
  1: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  1: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
123: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
123: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
  3: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  3: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 38: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 38: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 16: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 16: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 27: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 27: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 54: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 54: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
  5: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  5: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 64: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 64: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
127: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 57: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 57: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
127: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
  9: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  9: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 20: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 20: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 32: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 32: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 29: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 29: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
  7: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
  7: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 84: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 84: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 48: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 48: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 41: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 41: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
125: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
125: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 63: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 63: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 22: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 22: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 68: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 68: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 12: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 12: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
102: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
102: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 80: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 80: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 36: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 36: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 31: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 31: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 79: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
 79: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
118: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
118: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
121: ENDING TIMING RUN AT 2024-05-10 05:40:33 AM
121: RESULT,stable_diffusion,954,ubuntu,2024-05-10 05:24:39 AM
 61: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 61: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 15: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 15: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 47: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 47: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 66: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 66: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
111: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
111: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 24: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 24: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 18: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 18: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
120: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
120: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 51: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 51: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
112: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
112: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
  8: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
  8: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 40: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 40: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 35: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 35: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 75: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 75: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 85: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 85: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 93: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 93: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 98: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 98: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 70: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 70: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 30: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 30: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
122: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
122: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 56: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 56: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 19: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 19: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
109: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
109: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 43: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 43: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 73: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 73: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
116: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
116: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 14: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 14: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 39: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 39: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 49: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 49: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
100: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
100: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 82: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 82: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 95: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 95: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 69: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 69: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 21: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 21: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 26: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 26: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
107: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
107: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
124: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
124: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 59: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 59: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 77: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 77: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 44: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 44: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 13: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 13: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 55: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 55: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 34: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 34: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 96: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 96: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 65: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 65: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
104: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
104: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 81: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 81: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
115: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
115: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 92: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 92: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 17: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 17: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 25: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 25: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
126: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
126: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 72: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 72: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 53: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 53: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 10: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 10: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 45: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 45: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
103: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
103: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
117: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
117: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 58: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 58: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 86: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 86: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 67: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 67: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 37: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 37: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
108: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
108: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 78: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 78: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 94: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 94: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 28: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 28: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 23: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 23: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
114: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
114: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 42: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 42: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 52: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 52: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 60: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 60: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 33: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 33: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 97: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 97: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 87: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 87: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 71: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 71: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
110: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
110: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 89: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 89: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 74: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 74: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 46: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 46: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 62: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 62: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
119: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
119: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
 50: ENDING TIMING RUN AT 2024-05-10 05:40:34 AM
 50: RESULT,stable_diffusion,955,ubuntu,2024-05-10 05:24:39 AM
101: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
101: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
105: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
105: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 83: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 83: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 91: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 91: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 76: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 76: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
113: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
113: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 88: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 88: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
106: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
106: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 99: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 99: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
 90: ENDING TIMING RUN AT 2024-05-10 05:40:35 AM
 90: RESULT,stable_diffusion,956,ubuntu,2024-05-10 05:24:39 AM
