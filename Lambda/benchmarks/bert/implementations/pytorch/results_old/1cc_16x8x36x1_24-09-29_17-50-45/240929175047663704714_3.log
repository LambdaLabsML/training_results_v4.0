+ echo 'Beginning trial 3 of 3'
Beginning trial 3 of 3
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] '\''unknown'\'' 1CC'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-bert:latest 137 16 calvin-training-node-[016-031] 'unknown' 1CC
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
Clearing cache on calvin-training-node-020
Clearing cache on calvin-training-node-021
Clearing cache on calvin-training-node-023
Clearing cache on calvin-training-node-031
Clearing cache on calvin-training-node-027
Clearing cache on calvin-training-node-028
Clearing cache on calvin-training-node-029
Clearing cache on calvin-training-node-025
Clearing cache on calvin-training-node-022
Clearing cache on calvin-training-node-017
Clearing cache on calvin-training-node-018
Clearing cache on calvin-training-node-030
Clearing cache on calvin-training-node-026
Clearing cache on calvin-training-node-019
Clearing cache on calvin-training-node-024
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
Clearing cache on calvin-training-node-016
vm.drop_caches = 3
+ srun --ntasks=16 --container-name=language_model_137 python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
srun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 16 with the number of requested nodes 16. Ignoring --ntasks-per-node.
:::MLLOG {"namespace": "", "time_ms": 1727632757965, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632757972, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758032, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758043, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758047, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758045, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758060, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758071, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758093, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758102, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758097, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758113, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758411, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632758720, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632759053, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1727632759416, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --container-name=language_model_137 --container-mounts=/home/ubuntu/ml-1cc/data/mlperf/bert/packed_data:/workspace/data_phase2,/home/ubuntu/ml-1cc/data/mlperf/bert/phase1:/workspace/phase1,/home/ubuntu/ml-1cc/data/mlperf/bert/hdf5/eval_varlength:/workspace/evaldata,./results/1cc_16x8x36x1_24-09-29_17-50-45:/results,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --container-workdir=/workspace/bert slurm2pytorch ./run_and_time.sh
106: Run vars: id 137 gpus 8 mparams ''
106: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
 59: Run vars: id 137 gpus 8 mparams ''
 59: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
123: Run vars: id 137 gpus 8 mparams ''
123: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
124: Run vars: id 137 gpus 8 mparams ''
124: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
126: Run vars: id 137 gpus 8 mparams ''
126: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
 82: Run vars: id 137 gpus 8 mparams ''
 82: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
127: Run vars: id 137 gpus 8 mparams ''
127: STARTING TIMING RUN AT 2024-09-29 05:59:27 PM
108: Run vars: id 137 gpus 8 mparams ''
108: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
111: Run vars: id 137 gpus 8 mparams ''
111: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 86: Run vars: id 137 gpus 8 mparams ''
 86: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 84: Run vars: id 137 gpus 8 mparams ''
 84: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
113: Run vars: id 137 gpus 8 mparams ''
113: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
107: Run vars: id 137 gpus 8 mparams ''
 21: Run vars: id 137 gpus 8 mparams ''
107: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 75: Run vars: id 137 gpus 8 mparams ''
 21: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 75: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 54: Run vars: id 137 gpus 8 mparams ''
 54: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 81: Run vars: id 137 gpus 8 mparams ''
 81: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 57: Run vars: id 137 gpus 8 mparams ''
 79: Run vars: id 137 gpus 8 mparams ''
116: Run vars: id 137 gpus 8 mparams ''
 19: Run vars: id 137 gpus 8 mparams ''
 79: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 57: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 26: Run vars: id 137 gpus 8 mparams ''
 19: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
116: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 26: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 77: Run vars: id 137 gpus 8 mparams ''
 67: Run vars: id 137 gpus 8 mparams ''
 77: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
102: Run vars: id 137 gpus 8 mparams ''
 52: Run vars: id 137 gpus 8 mparams ''
 60: Run vars: id 137 gpus 8 mparams ''
 67: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 49: Run vars: id 137 gpus 8 mparams ''
 52: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
102: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 49: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 60: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 90: Run vars: id 137 gpus 8 mparams ''
 45: Run vars: id 137 gpus 8 mparams ''
 90: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 45: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 96: Run vars: id 137 gpus 8 mparams ''
 76: Run vars: id 137 gpus 8 mparams ''
 35: Run vars: id 137 gpus 8 mparams ''
 96: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 66: Run vars: id 137 gpus 8 mparams ''
 76: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 80: Run vars: id 137 gpus 8 mparams ''
 35: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 66: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 87: Run vars: id 137 gpus 8 mparams ''
 80: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 87: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 39: Run vars: id 137 gpus 8 mparams ''
 39: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 92: Run vars: id 137 gpus 8 mparams ''
 92: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
101: Run vars: id 137 gpus 8 mparams ''
 44: Run vars: id 137 gpus 8 mparams ''
101: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 44: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
125: Run vars: id 137 gpus 8 mparams ''
 97: Run vars: id 137 gpus 8 mparams ''
 31: Run vars: id 137 gpus 8 mparams ''
125: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 31: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 97: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 85: Run vars: id 137 gpus 8 mparams ''
 41: Run vars: id 137 gpus 8 mparams ''
 85: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 41: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 58: Run vars: id 137 gpus 8 mparams ''
 32: Run vars: id 137 gpus 8 mparams ''
 64: Run vars: id 137 gpus 8 mparams ''
 58: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 32: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 64: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 36: Run vars: id 137 gpus 8 mparams ''
 95: Run vars: id 137 gpus 8 mparams ''
 36: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 95: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 14: Run vars: id 137 gpus 8 mparams ''
 33: Run vars: id 137 gpus 8 mparams ''
109: Run vars: id 137 gpus 8 mparams ''
 33: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 14: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
109: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
110: Run vars: id 137 gpus 8 mparams ''
118: Run vars: id 137 gpus 8 mparams ''
110: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
118: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 11: Run vars: id 137 gpus 8 mparams ''
 11: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 22: Run vars: id 137 gpus 8 mparams ''
 22: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 51: Run vars: id 137 gpus 8 mparams ''
117: Run vars: id 137 gpus 8 mparams ''
105: Run vars: id 137 gpus 8 mparams ''
 51: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
117: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 55: Run vars: id 137 gpus 8 mparams ''
105: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 53: Run vars: id 137 gpus 8 mparams ''
 55: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 53: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 37: Run vars: id 137 gpus 8 mparams ''
 37: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 93: Run vars: id 137 gpus 8 mparams ''
 56: Run vars: id 137 gpus 8 mparams ''
 93: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 63: Run vars: id 137 gpus 8 mparams ''
  9: Run vars: id 137 gpus 8 mparams ''
 62: Run vars: id 137 gpus 8 mparams ''
 56: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  9: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 63: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 62: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 40: Run vars: id 137 gpus 8 mparams ''
 40: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  6: Run vars: id 137 gpus 8 mparams ''
 47: Run vars: id 137 gpus 8 mparams ''
 30: Run vars: id 137 gpus 8 mparams ''
 47: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 30: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 29: Run vars: id 137 gpus 8 mparams ''
 29: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 91: Run vars: id 137 gpus 8 mparams ''
 91: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 27: Run vars: id 137 gpus 8 mparams ''
103: Run vars: id 137 gpus 8 mparams ''
 25: Run vars: id 137 gpus 8 mparams ''
  6: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  5: Run vars: id 137 gpus 8 mparams ''
  5: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
103: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 27: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 73: Run vars: id 137 gpus 8 mparams ''
 25: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 73: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  2: Run vars: id 137 gpus 8 mparams ''
 68: Run vars: id 137 gpus 8 mparams ''
  2: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 72: Run vars: id 137 gpus 8 mparams ''
 68: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 72: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 18: Run vars: id 137 gpus 8 mparams ''
 28: Run vars: id 137 gpus 8 mparams ''
 18: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 28: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 74: Run vars: id 137 gpus 8 mparams ''
119: Run vars: id 137 gpus 8 mparams ''
 78: Run vars: id 137 gpus 8 mparams ''
 20: Run vars: id 137 gpus 8 mparams ''
 74: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
119: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 78: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
104: Run vars: id 137 gpus 8 mparams ''
 65: Run vars: id 137 gpus 8 mparams ''
 20: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
104: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 65: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  1: Run vars: id 137 gpus 8 mparams ''
 99: Run vars: id 137 gpus 8 mparams ''
  7: Run vars: id 137 gpus 8 mparams ''
 98: Run vars: id 137 gpus 8 mparams ''
  1: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 99: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  7: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 98: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
112: Run vars: id 137 gpus 8 mparams ''
114: Run vars: id 137 gpus 8 mparams ''
 94: Run vars: id 137 gpus 8 mparams ''
112: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 38: Run vars: id 137 gpus 8 mparams ''
114: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 94: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 38: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  3: Run vars: id 137 gpus 8 mparams ''
  3: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
115: Run vars: id 137 gpus 8 mparams ''
 89: Run vars: id 137 gpus 8 mparams ''
 34: Run vars: id 137 gpus 8 mparams ''
115: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 89: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 34: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 24: Run vars: id 137 gpus 8 mparams ''
  8: Run vars: id 137 gpus 8 mparams ''
 24: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 88: Run vars: id 137 gpus 8 mparams ''
  4: Run vars: id 137 gpus 8 mparams ''
 48: Run vars: id 137 gpus 8 mparams ''
  8: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 88: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  4: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 48: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 71: Run vars: id 137 gpus 8 mparams ''
 71: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
121: Run vars: id 137 gpus 8 mparams ''
121: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 50: Run vars: id 137 gpus 8 mparams ''
 15: Run vars: id 137 gpus 8 mparams ''
 50: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 15: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 83: Run vars: id 137 gpus 8 mparams ''
 61: Run vars: id 137 gpus 8 mparams ''
 83: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 61: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 10: Run vars: id 137 gpus 8 mparams ''
 43: Run vars: id 137 gpus 8 mparams ''
 10: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 43: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 23: Run vars: id 137 gpus 8 mparams ''
 23: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 17: Run vars: id 137 gpus 8 mparams ''
 17: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 46: Run vars: id 137 gpus 8 mparams ''
 46: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
122: Run vars: id 137 gpus 8 mparams ''
120: Run vars: id 137 gpus 8 mparams ''
122: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
120: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 69: Run vars: id 137 gpus 8 mparams ''
 69: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 13: Run vars: id 137 gpus 8 mparams ''
 13: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 12: Run vars: id 137 gpus 8 mparams ''
 12: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
  0: Run vars: id 137 gpus 8 mparams ''
100: Run vars: id 137 gpus 8 mparams ''
  0: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
100: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 42: Run vars: id 137 gpus 8 mparams ''
 42: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 70: Run vars: id 137 gpus 8 mparams ''
 70: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
 16: Run vars: id 137 gpus 8 mparams ''
 16: STARTING TIMING RUN AT 2024-09-29 05:59:28 PM
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
113: [W929 17:59:30.511065646 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W929 17:59:30.511071406 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
113:   warnings.warn(msg, DeprecatedFeatureWarning)
116: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
116:   warnings.warn(msg, DeprecatedFeatureWarning)
106: [W929 17:59:30.224038081 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W929 17:59:30.224049806 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W929 17:59:30.158558337 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
106:   warnings.warn(msg, DeprecatedFeatureWarning)
107: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
107:   warnings.warn(msg, DeprecatedFeatureWarning)
 82: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 82:   warnings.warn(msg, DeprecatedFeatureWarning)
 81: [W929 17:59:30.181493973 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 81:   warnings.warn(msg, DeprecatedFeatureWarning)
 85: [W929 17:59:30.192880418 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W929 17:59:30.815381683 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 85:   warnings.warn(msg, DeprecatedFeatureWarning)
108: [W929 17:59:30.264813585 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 59:   warnings.warn(msg, DeprecatedFeatureWarning)
108: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
108:   warnings.warn(msg, DeprecatedFeatureWarning)
 57: [W929 17:59:30.829827276 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:59:30.035679703 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:59:30.035680378 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 57:   warnings.warn(msg, DeprecatedFeatureWarning)
123: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
123:   warnings.warn(msg, DeprecatedFeatureWarning)
124: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
124:   warnings.warn(msg, DeprecatedFeatureWarning)
117: [W929 17:59:30.583362092 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
117:   warnings.warn(msg, DeprecatedFeatureWarning)
 86: [W929 17:59:30.226297142 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 86:   warnings.warn(msg, DeprecatedFeatureWarning)
125: [W929 17:59:30.067137558 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W929 17:59:30.390629968 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W929 17:59:30.390619919 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:59:30.390630067 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
125:   warnings.warn(msg, DeprecatedFeatureWarning)
126: [W929 17:59:30.071200521 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 75:   warnings.warn(msg, DeprecatedFeatureWarning)
 77: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 77:   warnings.warn(msg, DeprecatedFeatureWarning)
 79: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 79:   warnings.warn(msg, DeprecatedFeatureWarning)
126: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
126:   warnings.warn(msg, DeprecatedFeatureWarning)
 60: [W929 17:59:30.883180108 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 72: [W929 17:59:30.410432265 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W929 17:59:30.633460660 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 60:   warnings.warn(msg, DeprecatedFeatureWarning)
 72: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 72:   warnings.warn(msg, DeprecatedFeatureWarning)
119: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
119:   warnings.warn(msg, DeprecatedFeatureWarning)
 90: [W929 17:59:30.014858356 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W929 17:59:30.014858118 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 90:   warnings.warn(msg, DeprecatedFeatureWarning)
 92: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 92:   warnings.warn(msg, DeprecatedFeatureWarning)
 58: [W929 17:59:30.912647673 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 58:   warnings.warn(msg, DeprecatedFeatureWarning)
 63: [W929 17:59:30.916634024 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W929 17:59:30.386131067 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 63:   warnings.warn(msg, DeprecatedFeatureWarning)
 67: [W929 17:59:30.386132707 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:59:30.126830680 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 66:   warnings.warn(msg, DeprecatedFeatureWarning)
 67: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 67:   warnings.warn(msg, DeprecatedFeatureWarning)
127: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
127:   warnings.warn(msg, DeprecatedFeatureWarning)
 44: [W929 17:59:30.915831294 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W929 17:59:30.915829484 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W929 17:59:30.915831479 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 44:   warnings.warn(msg, DeprecatedFeatureWarning)
 45: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 45:   warnings.warn(msg, DeprecatedFeatureWarning)
 47: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 47:   warnings.warn(msg, DeprecatedFeatureWarning)
 89: [W929 17:59:30.060180733 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 49: [W929 17:59:30.444633781 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: [W929 17:59:30.444631701 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: [W929 17:59:30.444633715 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 89: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 89:   warnings.warn(msg, DeprecatedFeatureWarning)
 49: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 49:   warnings.warn(msg, DeprecatedFeatureWarning)
 52: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 52:   warnings.warn(msg, DeprecatedFeatureWarning)
 54: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 54:   warnings.warn(msg, DeprecatedFeatureWarning)
 91: [W929 17:59:30.080531066 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 91:   warnings.warn(msg, DeprecatedFeatureWarning)
 80: [W929 17:59:30.363047155 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W929 17:59:30.966572902 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 80: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 80:   warnings.warn(msg, DeprecatedFeatureWarning)
 41: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 41:   warnings.warn(msg, DeprecatedFeatureWarning)
 83: [W929 17:59:30.373166773 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: [W929 17:59:30.460291341 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 83:   warnings.warn(msg, DeprecatedFeatureWarning)
 64: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 64:   warnings.warn(msg, DeprecatedFeatureWarning)
 84: [W929 17:59:30.382998399 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 84:   warnings.warn(msg, DeprecatedFeatureWarning)
121: [W929 17:59:30.215909911 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:59:30.538241073 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [W929 17:59:30.324799845 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W929 17:59:30.324801335 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:59:30.324803676 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: [W929 17:59:30.674880289 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W929 17:59:30.674878391 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W929 17:59:30.674883969 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:59:30.674870199 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
121:   warnings.warn(msg, DeprecatedFeatureWarning)
 76: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 76:   warnings.warn(msg, DeprecatedFeatureWarning)
 32: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 32:   warnings.warn(msg, DeprecatedFeatureWarning)
 35: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 35:   warnings.warn(msg, DeprecatedFeatureWarning)
 39: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 39:   warnings.warn(msg, DeprecatedFeatureWarning)
 96: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 96:   warnings.warn(msg, DeprecatedFeatureWarning)
 97: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 97:   warnings.warn(msg, DeprecatedFeatureWarning)
 98: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 98:   warnings.warn(msg, DeprecatedFeatureWarning)
102: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
102:   warnings.warn(msg, DeprecatedFeatureWarning)
 73: [W929 17:59:30.546121435 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W929 17:59:30.680606310 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
101:   warnings.warn(msg, DeprecatedFeatureWarning)
 73: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 73:   warnings.warn(msg, DeprecatedFeatureWarning)
 33: [W929 17:59:30.337467651 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 33:   warnings.warn(msg, DeprecatedFeatureWarning)
115: [W929 17:59:30.778148290 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
115:   warnings.warn(msg, DeprecatedFeatureWarning)
 87: [W929 17:59:30.417547998 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 87:   warnings.warn(msg, DeprecatedFeatureWarning)
 37: [W929 17:59:30.354331380 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W929 17:59:30.965738967 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W929 17:59:30.965734644 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W929 17:59:30.965739102 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W929 17:59:30.965743956 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:59:30.965734753 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W929 17:59:30.250890598 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 37:   warnings.warn(msg, DeprecatedFeatureWarning)
122: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
122:   warnings.warn(msg, DeprecatedFeatureWarning)
 18: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 18:   warnings.warn(msg, DeprecatedFeatureWarning)
 19: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 19:   warnings.warn(msg, DeprecatedFeatureWarning)
 20: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 20:   warnings.warn(msg, DeprecatedFeatureWarning)
 21: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 21:   warnings.warn(msg, DeprecatedFeatureWarning)
 22: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 22:   warnings.warn(msg, DeprecatedFeatureWarning)
 78: [W929 17:59:30.582101415 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W929 17:59:30.501990930 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 78:   warnings.warn(msg, DeprecatedFeatureWarning)
103: [W929 17:59:30.720723545 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
105:   warnings.warn(msg, DeprecatedFeatureWarning)
 51: [W929 17:59:30.555117542 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
103: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
103:   warnings.warn(msg, DeprecatedFeatureWarning)
 51: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 51:   warnings.warn(msg, DeprecatedFeatureWarning)
 40: [W929 17:59:30.047388010 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W929 17:59:30.381537396 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 40: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 40:   warnings.warn(msg, DeprecatedFeatureWarning)
 34: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 34:   warnings.warn(msg, DeprecatedFeatureWarning)
 74: [W929 17:59:30.600329871 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W929 17:59:30.386348019 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: [W929 17:59:30.568124233 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 74:   warnings.warn(msg, DeprecatedFeatureWarning)
 36: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 36:   warnings.warn(msg, DeprecatedFeatureWarning)
 55: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 55:   warnings.warn(msg, DeprecatedFeatureWarning)
 61: [W929 17:59:30.082974016 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W929 17:59:30.063169731 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:59:30.063174197 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 61:   warnings.warn(msg, DeprecatedFeatureWarning)
 99: [W929 17:59:30.748773337 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 99:   warnings.warn(msg, DeprecatedFeatureWarning)
  9: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  9:   warnings.warn(msg, DeprecatedFeatureWarning)
 14: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 14:   warnings.warn(msg, DeprecatedFeatureWarning)
 62: [W929 17:59:30.092802792 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: [W929 17:59:30.540554259 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 62:   warnings.warn(msg, DeprecatedFeatureWarning)
 65: [W929 17:59:30.564698806 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
104:   warnings.warn(msg, DeprecatedFeatureWarning)
 65: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 65:   warnings.warn(msg, DeprecatedFeatureWarning)
 11: [W929 17:59:30.079466538 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W929 17:59:30.851047805 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 11:   warnings.warn(msg, DeprecatedFeatureWarning)
 15: [W929 17:59:30.083725064 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W929 17:59:30.552619409 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
118:   warnings.warn(msg, DeprecatedFeatureWarning)
120: [W929 17:59:30.313984847 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:59:30.555690314 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
109:   warnings.warn(msg, DeprecatedFeatureWarning)
 15: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 15:   warnings.warn(msg, DeprecatedFeatureWarning)
 23: [W929 17:59:30.032692047 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
120: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
120:   warnings.warn(msg, DeprecatedFeatureWarning)
111: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
111:   warnings.warn(msg, DeprecatedFeatureWarning)
 23: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 23:   warnings.warn(msg, DeprecatedFeatureWarning)
 93: [W929 17:59:30.228733357 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W929 17:59:30.586002969 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 93:   warnings.warn(msg, DeprecatedFeatureWarning)
 53: [W929 17:59:30.615349784 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:59:30.616375223 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 69:   warnings.warn(msg, DeprecatedFeatureWarning)
 53: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 53:   warnings.warn(msg, DeprecatedFeatureWarning)
 50: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 50:   warnings.warn(msg, DeprecatedFeatureWarning)
 46: [W929 17:59:30.109878553 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W929 17:59:30.443042963 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 46:   warnings.warn(msg, DeprecatedFeatureWarning)
 38: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 38:   warnings.warn(msg, DeprecatedFeatureWarning)
 56: [W929 17:59:30.135692969 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W929 17:59:30.603360886 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 56: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 56:   warnings.warn(msg, DeprecatedFeatureWarning)
 71: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 71:   warnings.warn(msg, DeprecatedFeatureWarning)
 68: [W929 17:59:30.607741582 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W929 17:59:30.889754605 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 68:   warnings.warn(msg, DeprecatedFeatureWarning)
114: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
114:   warnings.warn(msg, DeprecatedFeatureWarning)
112: [W929 17:59:30.893664108 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: [W929 17:59:30.807162829 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: [W929 17:59:30.640233562 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W929 17:59:30.258768426 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W929 17:59:30.595027773 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
100: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
100:   warnings.warn(msg, DeprecatedFeatureWarning)
112: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
112:   warnings.warn(msg, DeprecatedFeatureWarning)
 48: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 48:   warnings.warn(msg, DeprecatedFeatureWarning)
 95: [W929 17:59:30.261924378 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 94:   warnings.warn(msg, DeprecatedFeatureWarning)
 17: [W929 17:59:30.073797498 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
110:   warnings.warn(msg, DeprecatedFeatureWarning)
 95: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 95:   warnings.warn(msg, DeprecatedFeatureWarning)
 17: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 17:   warnings.warn(msg, DeprecatedFeatureWarning)
 43: [W929 17:59:30.136075343 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 43:   warnings.warn(msg, DeprecatedFeatureWarning)
 42: [W929 17:59:30.152697628 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 42:   warnings.warn(msg, DeprecatedFeatureWarning)
 88: [W929 17:59:30.287638098 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W929 17:59:30.156852461 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 88:   warnings.warn(msg, DeprecatedFeatureWarning)
 10: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 10:   warnings.warn(msg, DeprecatedFeatureWarning)
 70: [W929 17:59:30.658331363 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W929 17:59:30.858022636 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: [W929 17:59:30.858024786 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W929 17:59:30.858012509 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W929 17:59:30.858003920 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W929 17:59:30.858014814 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:59:30.858003224 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W929 17:59:30.858025032 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:59:30.858016759 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 70:   warnings.warn(msg, DeprecatedFeatureWarning)
  8: [W929 17:59:30.174313556 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 26:   warnings.warn(msg, DeprecatedFeatureWarning)
 27: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 27:   warnings.warn(msg, DeprecatedFeatureWarning)
 28: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 28:   warnings.warn(msg, DeprecatedFeatureWarning)
 29: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 29:   warnings.warn(msg, DeprecatedFeatureWarning)
 30: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 30:   warnings.warn(msg, DeprecatedFeatureWarning)
 24: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 24:   warnings.warn(msg, DeprecatedFeatureWarning)
 25: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 25:   warnings.warn(msg, DeprecatedFeatureWarning)
 31: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 31:   warnings.warn(msg, DeprecatedFeatureWarning)
  8: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  8:   warnings.warn(msg, DeprecatedFeatureWarning)
 16: [W929 17:59:30.124602081 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 16: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 16:   warnings.warn(msg, DeprecatedFeatureWarning)
 13: [W929 17:59:30.226793656 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 13:   warnings.warn(msg, DeprecatedFeatureWarning)
 12: [W929 17:59:30.240776620 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 12:   warnings.warn(msg, DeprecatedFeatureWarning)
  0: [W929 17:59:30.272343793 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W929 17:59:30.272362016 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [W929 17:59:30.272352023 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W929 17:59:30.272357523 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W929 17:59:30.272338466 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:59:30.272350178 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W929 17:59:30.272353202 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:59:30.277410650 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  0: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  0:   warnings.warn(msg, DeprecatedFeatureWarning)
  1: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  1:   warnings.warn(msg, DeprecatedFeatureWarning)
  2: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  2:   warnings.warn(msg, DeprecatedFeatureWarning)
  3: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  3:   warnings.warn(msg, DeprecatedFeatureWarning)
  5: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  5:   warnings.warn(msg, DeprecatedFeatureWarning)
  6: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  6:   warnings.warn(msg, DeprecatedFeatureWarning)
  7: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  7:   warnings.warn(msg, DeprecatedFeatureWarning)
  4: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  4:   warnings.warn(msg, DeprecatedFeatureWarning)
 49: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 49:   from jax.experimental.maps import xmap
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 99:   from jax.experimental.maps import xmap
 72: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 72:   from jax.experimental.maps import xmap
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 80:   from jax.experimental.maps import xmap
108: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
108:   from jax.experimental.maps import xmap
 87: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 87:   from jax.experimental.maps import xmap
 71: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 71:   from jax.experimental.maps import xmap
117: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
117:   from jax.experimental.maps import xmap
 88: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 88:   from jax.experimental.maps import xmap
105: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
105:   from jax.experimental.maps import xmap
102: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
102:   from jax.experimental.maps import xmap
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 41:   from jax.experimental.maps import xmap
 89: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 89:   from jax.experimental.maps import xmap
 39: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 39:   from jax.experimental.maps import xmap
 49: 2024-09-29 17:59:38.438730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 60:   from jax.experimental.maps import xmap
 61: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 61:   from jax.experimental.maps import xmap
 59: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 59:   from jax.experimental.maps import xmap
114: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
114:   from jax.experimental.maps import xmap
 97: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 97:   from jax.experimental.maps import xmap
 44: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 44:   from jax.experimental.maps import xmap
 33: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 33:   from jax.experimental.maps import xmap
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 70:   from jax.experimental.maps import xmap
 77: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 77:   from jax.experimental.maps import xmap
115: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
115:   from jax.experimental.maps import xmap
 90: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 90:   from jax.experimental.maps import xmap
125: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
125:   from jax.experimental.maps import xmap
126: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
126:   from jax.experimental.maps import xmap
 78: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 78:   from jax.experimental.maps import xmap
 63: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 63:   from jax.experimental.maps import xmap
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 83:   from jax.experimental.maps import xmap
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 57:   from jax.experimental.maps import xmap
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 62:   from jax.experimental.maps import xmap
109: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
109:   from jax.experimental.maps import xmap
 86: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 86:   from jax.experimental.maps import xmap
 46: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 46:   from jax.experimental.maps import xmap
 52: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 52:   from jax.experimental.maps import xmap
 74: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 74:   from jax.experimental.maps import xmap
 58: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 58:   from jax.experimental.maps import xmap
 82: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 82:   from jax.experimental.maps import xmap
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
101:   from jax.experimental.maps import xmap
113: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
113:   from jax.experimental.maps import xmap
104: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
104:   from jax.experimental.maps import xmap
 73: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 73:   from jax.experimental.maps import xmap
 54: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 54:   from jax.experimental.maps import xmap
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
122:   from jax.experimental.maps import xmap
 81: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 81:   from jax.experimental.maps import xmap
 49: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 49:   from jax import xla_computation as _xla_computation
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 51:   from jax.experimental.maps import xmap
 66: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 66:   from jax.experimental.maps import xmap
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 56:   from jax.experimental.maps import xmap
121: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
121:   from jax.experimental.maps import xmap
 91: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 91:   from jax.experimental.maps import xmap
119: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
119:   from jax.experimental.maps import xmap
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 84:   from jax.experimental.maps import xmap
 85: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 85:   from jax.experimental.maps import xmap
127: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
127:   from jax.experimental.maps import xmap
 34: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 34:   from jax.experimental.maps import xmap
 55: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 55:   from jax.experimental.maps import xmap
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
106:   from jax.experimental.maps import xmap
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 94:   from jax.experimental.maps import xmap
 64: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 64:   from jax.experimental.maps import xmap
116: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
116:   from jax.experimental.maps import xmap
 69: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 69:   from jax.experimental.maps import xmap
 75: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 75:   from jax.experimental.maps import xmap
 42: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 42:   from jax.experimental.maps import xmap
 68: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 68:   from jax.experimental.maps import xmap
 79: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 79:   from jax.experimental.maps import xmap
123: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
123:   from jax.experimental.maps import xmap
 45: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 45:   from jax.experimental.maps import xmap
 43: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 43:   from jax.experimental.maps import xmap
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
120:   from jax.experimental.maps import xmap
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
118:   from jax.experimental.maps import xmap
 92: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 92:   from jax.experimental.maps import xmap
124: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
124:   from jax.experimental.maps import xmap
112: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
112:   from jax.experimental.maps import xmap
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 65:   from jax.experimental.maps import xmap
 37: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 37:   from jax.experimental.maps import xmap
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 47:   from jax.experimental.maps import xmap
 95: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 95:   from jax.experimental.maps import xmap
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 76:   from jax.experimental.maps import xmap
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
107:   from jax.experimental.maps import xmap
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 93:   from jax.experimental.maps import xmap
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 40:   from jax.experimental.maps import xmap
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 32:   from jax.experimental.maps import xmap
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
110:   from jax.experimental.maps import xmap
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 53:   from jax.experimental.maps import xmap
 67: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 67:   from jax.experimental.maps import xmap
103: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
103:   from jax.experimental.maps import xmap
 48: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 48:   from jax.experimental.maps import xmap
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: 2024-09-29 17:59:39.061617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 35:   from jax.experimental.maps import xmap
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
111:   from jax.experimental.maps import xmap
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 50:   from jax.experimental.maps import xmap
 36: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 36:   from jax.experimental.maps import xmap
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 38:   from jax.experimental.maps import xmap
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 96:   from jax.experimental.maps import xmap
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
100:   from jax.experimental.maps import xmap
 98: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 98:   from jax.experimental.maps import xmap
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  9:   from jax.experimental.maps import xmap
 13: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 13:   from jax.experimental.maps import xmap
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 15:   from jax.experimental.maps import xmap
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 99:   from jax import xla_computation as _xla_computation
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: 2024-09-29 17:59:39.410188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
102: 2024-09-29 17:59:39.436779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: 2024-09-29 17:59:39.485403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 77: 2024-09-29 17:59:39.485394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: 2024-09-29 17:59:39.536161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 87: 2024-09-29 17:59:39.536161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 83: 2024-09-29 17:59:39.546950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 86: 2024-09-29 17:59:39.550896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 10:   from jax.experimental.maps import xmap
 97: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 97:   from jax import xla_computation as _xla_computation
 82: 2024-09-29 17:59:39.586790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: 2024-09-29 17:59:39.589818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
102: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
102:   from jax import xla_computation as _xla_computation
 14: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 14:   from jax.experimental.maps import xmap
101: 2024-09-29 17:59:39.617218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: 2024-09-29 17:59:39.651858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
105: 2024-09-29 17:59:39.648669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: 2024-09-29 17:59:39.648669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: 2024-09-29 17:59:39.662699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 11:   from jax.experimental.maps import xmap
 78: 2024-09-29 17:59:39.680378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 51: 2024-09-29 17:59:39.675729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
113: 2024-09-29 17:59:39.687191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
115: 2024-09-29 17:59:39.687191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
117: 2024-09-29 17:59:39.687191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 74: 2024-09-29 17:59:39.699821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 25:   from jax.experimental.maps import xmap
 27: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 27:   from jax.experimental.maps import xmap
114: 2024-09-29 17:59:39.721610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 70: 2024-09-29 17:59:39.729020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 71: 2024-09-29 17:59:39.729015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 24: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 24:   from jax.experimental.maps import xmap
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: 2024-09-29 17:59:39.756490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: 2024-09-29 17:59:39.758994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 52:   from jax import xla_computation as _xla_computation
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: 2024-09-29 17:59:39.770000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 30:   from jax.experimental.maps import xmap
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
101: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
101:   from jax import xla_computation as _xla_computation
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 12:   from jax.experimental.maps import xmap
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: 2024-09-29 17:59:39.790743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 79: 2024-09-29 17:59:39.797996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: 2024-09-29 17:59:39.798560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 49: [W929 17:59:39.687973346 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: 2024-09-29 17:59:39.800524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 89: 2024-09-29 17:59:39.800524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 90: 2024-09-29 17:59:39.800524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 91: 2024-09-29 17:59:39.800524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 49: :::MLLOG {"namespace": "", "time_ms": 1727632779804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 69: 2024-09-29 17:59:39.812480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: 2024-09-29 17:59:39.818065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 72: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 72:   from jax import xla_computation as _xla_computation
 77: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 77:   from jax import xla_computation as _xla_computation
 68: 2024-09-29 17:59:39.821312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 73:   from jax import xla_computation as _xla_computation
 94: 2024-09-29 17:59:39.828938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  0: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  0:   from jax.experimental.maps import xmap
  1: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  1:   from jax.experimental.maps import xmap
 65: 2024-09-29 17:59:39.838548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
119: 2024-09-29 17:59:39.838637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 99: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 51: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 51:   from jax import xla_computation as _xla_computation
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: 2024-09-29 17:59:39.864496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 42: 2024-09-29 17:59:39.864496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 44: 2024-09-29 17:59:39.864497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 46: 2024-09-29 17:59:39.864497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 78: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 78:   from jax import xla_computation as _xla_computation
 80: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 80:   from jax import xla_computation as _xla_computation
 82: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 82:   from jax import xla_computation as _xla_computation
 83: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 83:   from jax import xla_computation as _xla_computation
 86: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 86:   from jax import xla_computation as _xla_computation
 87: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 87:   from jax import xla_computation as _xla_computation
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: 2024-09-29 17:59:39.876170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 74:   from jax import xla_computation as _xla_computation
106: 2024-09-29 17:59:39.883149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 97: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  8:   from jax.experimental.maps import xmap
 33: 2024-09-29 17:59:39.898994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 34: 2024-09-29 17:59:39.899002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 37: 2024-09-29 17:59:39.899008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 39: 2024-09-29 17:59:39.899004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: 2024-09-29 17:59:39.916613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: 2024-09-29 17:59:39.924553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 81:   from jax import xla_computation as _xla_computation
 75: 2024-09-29 17:59:39.939245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 55: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 55:   from jax import xla_computation as _xla_computation
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 47: 2024-09-29 17:59:39.943207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
118: 2024-09-29 17:59:39.945985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 56: 2024-09-29 17:59:39.949110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 57: 2024-09-29 17:59:39.949115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 58: 2024-09-29 17:59:39.949115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 59: 2024-09-29 17:59:39.949115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 60: 2024-09-29 17:59:39.949117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 61: 2024-09-29 17:59:39.949113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 62: 2024-09-29 17:59:39.949114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 63: 2024-09-29 17:59:39.949110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 79:   from jax import xla_computation as _xla_computation
112: 2024-09-29 17:59:39.965821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 43: 2024-09-29 17:59:39.969170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  4:   from jax.experimental.maps import xmap
  5: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  5:   from jax.experimental.maps import xmap
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
104:   from jax import xla_computation as _xla_computation
105: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
105:   from jax import xla_computation as _xla_computation
108: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
108:   from jax import xla_computation as _xla_computation
 54: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 54:   from jax import xla_computation as _xla_computation
 84: 2024-09-29 17:59:39.979719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
109: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
109:   from jax import xla_computation as _xla_computation
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 28:   from jax.experimental.maps import xmap
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 16: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 16:   from jax.experimental.maps import xmap
 18: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 18:   from jax.experimental.maps import xmap
 21: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 21:   from jax.experimental.maps import xmap
 22: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 22:   from jax.experimental.maps import xmap
 23: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 23:   from jax.experimental.maps import xmap
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 17:   from jax.experimental.maps import xmap
 26: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 26:   from jax.experimental.maps import xmap
121: 2024-09-29 17:59:40.020821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 20: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 20:   from jax.experimental.maps import xmap
122: 2024-09-29 17:59:40.020821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
124: 2024-09-29 17:59:40.020821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
125: 2024-09-29 17:59:40.020820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
126: 2024-09-29 17:59:40.020821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
127: 2024-09-29 17:59:40.020819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
113: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
113:   from jax import xla_computation as _xla_computation
114: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
114:   from jax import xla_computation as _xla_computation
115: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
115:   from jax import xla_computation as _xla_computation
116: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
116:   from jax import xla_computation as _xla_computation
117: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
117:   from jax import xla_computation as _xla_computation
119: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
119:   from jax import xla_computation as _xla_computation
 76: 2024-09-29 17:59:40.024557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 92: 2024-09-29 17:59:40.018617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
107: 2024-09-29 17:59:40.021443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  7:   from jax.experimental.maps import xmap
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  2:   from jax.experimental.maps import xmap
 29: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 29:   from jax.experimental.maps import xmap
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: 2024-09-29 17:59:40.050431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
123: 2024-09-29 17:59:40.052571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: 2024-09-29 17:59:40.051581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 64:   from jax import xla_computation as _xla_computation
 65: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 65:   from jax import xla_computation as _xla_computation
 68: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 68:   from jax import xla_computation as _xla_computation
 69: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 69:   from jax import xla_computation as _xla_computation
 70: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 70:   from jax import xla_computation as _xla_computation
 71: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 71:   from jax import xla_computation as _xla_computation
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
106:   from jax import xla_computation as _xla_computation
 19: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 19:   from jax.experimental.maps import xmap
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
 31:   from jax.experimental.maps import xmap
 93: 2024-09-29 17:59:40.076258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 52: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: 2024-09-29 17:59:40.100668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 85:   from jax import xla_computation as _xla_computation
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 53: 2024-09-29 17:59:40.106517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 66: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 66:   from jax import xla_computation as _xla_computation
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: 2024-09-29 17:59:40.117437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 75: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 75:   from jax import xla_computation as _xla_computation
  3: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  3:   from jax.experimental.maps import xmap
 35: 2024-09-29 17:59:40.125587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  6: /usr/local/lib/python3.10/dist-packages/transformer_engine/jax/sharding.py:16: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD device-parallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.
  6:   from jax.experimental.maps import xmap
103: 2024-09-29 17:59:40.124432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
118: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
118:   from jax import xla_computation as _xla_computation
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 88:   from jax import xla_computation as _xla_computation
 89: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 89:   from jax import xla_computation as _xla_computation
 90: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 90:   from jax import xla_computation as _xla_computation
 91: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 91:   from jax import xla_computation as _xla_computation
 94: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 94:   from jax import xla_computation as _xla_computation
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: 2024-09-29 17:59:40.139885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
112:   from jax import xla_computation as _xla_computation
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: 2024-09-29 17:59:40.158997: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 67: 2024-09-29 17:59:40.156828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: 2024-09-29 17:59:40.159301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 48: 2024-09-29 17:59:40.160765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 84:   from jax import xla_computation as _xla_computation
111: 2024-09-29 17:59:40.169535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: 2024-09-29 17:59:40.181363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
100: 2024-09-29 17:59:40.195205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 41:   from jax import xla_computation as _xla_computation
 42: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 42:   from jax import xla_computation as _xla_computation
 43: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 43:   from jax import xla_computation as _xla_computation
 44: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 44:   from jax import xla_computation as _xla_computation
 45: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 45:   from jax import xla_computation as _xla_computation
 46: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 46:   from jax import xla_computation as _xla_computation
 47: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 47:   from jax import xla_computation as _xla_computation
107: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
107:   from jax import xla_computation as _xla_computation
 76: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 76:   from jax import xla_computation as _xla_computation
 92: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 92:   from jax import xla_computation as _xla_computation
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: 2024-09-29 17:59:40.221336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 98: 2024-09-29 17:59:40.225661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 33:   from jax import xla_computation as _xla_computation
 34: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 34:   from jax import xla_computation as _xla_computation
 37: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 37:   from jax import xla_computation as _xla_computation
 39: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 39:   from jax import xla_computation as _xla_computation
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 95:   from jax import xla_computation as _xla_computation
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 93: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 93:   from jax import xla_computation as _xla_computation
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 32:   from jax import xla_computation as _xla_computation
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 53:   from jax import xla_computation as _xla_computation
 35: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 35:   from jax import xla_computation as _xla_computation
 40: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 40:   from jax import xla_computation as _xla_computation
 56: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 56:   from jax import xla_computation as _xla_computation
 57: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 57:   from jax import xla_computation as _xla_computation
 58: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 58:   from jax import xla_computation as _xla_computation
 59: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 59:   from jax import xla_computation as _xla_computation
 60: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 60:   from jax import xla_computation as _xla_computation
 61: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 61:   from jax import xla_computation as _xla_computation
 62: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 62:   from jax import xla_computation as _xla_computation
 63: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 63:   from jax import xla_computation as _xla_computation
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
103:   from jax import xla_computation as _xla_computation
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
110:   from jax import xla_computation as _xla_computation
 72: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 77: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 79: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 36: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 36:   from jax import xla_computation as _xla_computation
 67: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 67:   from jax import xla_computation as _xla_computation
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 78: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
111:   from jax import xla_computation as _xla_computation
 38: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 38:   from jax import xla_computation as _xla_computation
120: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
120:   from jax import xla_computation as _xla_computation
121: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
121:   from jax import xla_computation as _xla_computation
122: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
122:   from jax import xla_computation as _xla_computation
123: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
123:   from jax import xla_computation as _xla_computation
124: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
124:   from jax import xla_computation as _xla_computation
125: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
125:   from jax import xla_computation as _xla_computation
126: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
126:   from jax import xla_computation as _xla_computation
127: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
127:   from jax import xla_computation as _xla_computation
 50: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 50:   from jax import xla_computation as _xla_computation
 48: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 48:   from jax import xla_computation as _xla_computation
 54: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
100: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
100:   from jax import xla_computation as _xla_computation
 80: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 82: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 83: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 86: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 87: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 96:   from jax import xla_computation as _xla_computation
 98: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 98:   from jax import xla_computation as _xla_computation
 52: [W929 17:59:40.307550233 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 97: [W929 17:59:40.476458349 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 99: [W929 17:59:40.476458272 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
101: [W929 17:59:40.476460106 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
102: [W929 17:59:40.476456937 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 52: :::MLLOG {"namespace": "", "time_ms": 1727632780423, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
101: :::MLLOG {"namespace": "", "time_ms": 1727632780428, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
102: :::MLLOG {"namespace": "", "time_ms": 1727632780428, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 97: :::MLLOG {"namespace": "", "time_ms": 1727632780428, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 99: :::MLLOG {"namespace": "", "time_ms": 1727632780428, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: [W929 17:59:40.360077215 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 51: :::MLLOG {"namespace": "", "time_ms": 1727632780475, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
104: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
108: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
105: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
115: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
116: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
117: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
114: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
119: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 65: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 68: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 69: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 71: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: [W929 17:59:40.471183183 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 55: :::MLLOG {"namespace": "", "time_ms": 1727632780587, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
107: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 76: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 70: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 89: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 90: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 91: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 94: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 53: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 95: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 54: [W929 17:59:40.585163574 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 54: :::MLLOG {"namespace": "", "time_ms": 1727632780701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 42: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 44: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 45: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 46: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 92: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 34: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 37: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 39: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
100: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 40: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 50: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 38: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 43: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 35: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 32: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 96: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 58: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 59: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 61: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 62: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 63: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
121: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
122: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
125: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
126: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
127: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
120: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: [W929 17:59:40.857706328 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 73: [W929 17:59:40.857709630 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 74: [W929 17:59:40.857743747 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 75: [W929 17:59:40.857731520 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 76: [W929 17:59:40.857742561 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 77: [W929 17:59:40.857699773 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 78: [W929 17:59:40.857746327 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 79: [W929 17:59:40.857698505 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 73: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 77: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 72: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 79: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 75: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 76: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 78: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 74: :::MLLOG {"namespace": "", "time_ms": 1727632780948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: [W929 17:59:40.752675158 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 81: [W929 17:59:40.752714788 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: [W929 17:59:40.752675173 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 83: [W929 17:59:40.752675297 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 84: [W929 17:59:40.752716940 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 85: [W929 17:59:40.752727563 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: [W929 17:59:40.752656997 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 87: [W929 17:59:40.752667940 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 86: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 87: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 80: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 81: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 82: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 83: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 84: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 85: :::MLLOG {"namespace": "", "time_ms": 1727632780985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 53: [W929 17:59:40.874486007 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 53: :::MLLOG {"namespace": "", "time_ms": 1727632780990, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: 2024-09-29 17:59:40.992977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  9: 2024-09-29 17:59:40.992945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 10: 2024-09-29 17:59:40.992951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 11: 2024-09-29 17:59:40.992944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 12: 2024-09-29 17:59:40.992944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 13: 2024-09-29 17:59:40.992944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 14: 2024-09-29 17:59:40.992945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 15: 2024-09-29 17:59:40.992944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
103: [W929 17:59:41.061938486 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: :::MLLOG {"namespace": "", "time_ms": 1727632781014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: [W929 17:59:41.122142170 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: :::MLLOG {"namespace": "", "time_ms": 1727632781074, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
104: [W929 17:59:41.921957803 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
105: [W929 17:59:41.922010099 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
106: [W929 17:59:41.922023091 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
107: [W929 17:59:41.922010092 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
108: [W929 17:59:41.921958149 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
109: [W929 17:59:41.922032946 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
110: [W929 17:59:41.921998520 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 50: [W929 17:59:41.972077773 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
104: :::MLLOG {"namespace": "", "time_ms": 1727632781089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
108: :::MLLOG {"namespace": "", "time_ms": 1727632781089, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
107: :::MLLOG {"namespace": "", "time_ms": 1727632781090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
105: :::MLLOG {"namespace": "", "time_ms": 1727632781090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
106: :::MLLOG {"namespace": "", "time_ms": 1727632781090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
109: :::MLLOG {"namespace": "", "time_ms": 1727632781090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
110: :::MLLOG {"namespace": "", "time_ms": 1727632781090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 50: :::MLLOG {"namespace": "", "time_ms": 1727632781088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 48: [W929 17:59:41.980612484 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
111: [W929 17:59:41.936673038 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 48: :::MLLOG {"namespace": "", "time_ms": 1727632781096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
111: :::MLLOG {"namespace": "", "time_ms": 1727632781104, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 96: [W929 17:59:41.163659510 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 98: [W929 17:59:41.164395051 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 96: :::MLLOG {"namespace": "", "time_ms": 1727632781116, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 98: :::MLLOG {"namespace": "", "time_ms": 1727632781117, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: [W929 17:59:41.286714929 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: [W929 17:59:41.286682768 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
114: [W929 17:59:41.286731354 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
115: [W929 17:59:41.286682700 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
116: [W929 17:59:41.286682710 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
117: [W929 17:59:41.286682563 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
118: [W929 17:59:41.286727492 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
119: [W929 17:59:41.286728153 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
113: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
115: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
116: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
117: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
118: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
119: :::MLLOG {"namespace": "", "time_ms": 1727632781151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
112: :::MLLOG {"namespace": "", "time_ms": 1727632781152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
114: :::MLLOG {"namespace": "", "time_ms": 1727632781152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 64: [W929 17:59:41.024296227 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 65: [W929 17:59:41.024296042 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 66: [W929 17:59:41.024313666 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 67: [W929 17:59:41.024309919 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 68: [W929 17:59:41.024275837 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 69: [W929 17:59:41.024275899 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 70: [W929 17:59:41.024334627 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 71: [W929 17:59:41.024275709 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 64: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 65: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 68: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 69: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 71: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 67: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 70: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 66: :::MLLOG {"namespace": "", "time_ms": 1727632781173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: [W929 17:59:41.743132585 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 91: [W929 17:59:41.743132498 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: [W929 17:59:41.743130012 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 88: [W929 17:59:41.743164175 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 92: [W929 17:59:41.743182208 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 93: [W929 17:59:41.743161197 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 95: [W929 17:59:41.743161791 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 90: [W929 17:59:41.743377623 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 94: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 89: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 90: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 91: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 88: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 93: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 95: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 92: :::MLLOG {"namespace": "", "time_ms": 1727632781243, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: [W929 17:59:41.686873279 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 41: [W929 17:59:41.686814646 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: [W929 17:59:41.686818959 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 43: [W929 17:59:41.686862521 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 44: [W929 17:59:41.686815212 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 45: [W929 17:59:41.686828114 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 46: [W929 17:59:41.686821908 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 47: [W929 17:59:41.686847726 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 42: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 44: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 45: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 46: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 41: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 47: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 43: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 40: :::MLLOG {"namespace": "", "time_ms": 1727632781322, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 33: [W929 17:59:41.078119441 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 34: [W929 17:59:41.078119550 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 37: [W929 17:59:41.078119818 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 39: [W929 17:59:41.078122027 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 32: [W929 17:59:41.078152027 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 35: [W929 17:59:41.078152237 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 36: [W929 17:59:41.078151107 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 38: [W929 17:59:41.078151629 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 33: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 34: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 37: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 39: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 32: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 35: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 36: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 38: :::MLLOG {"namespace": "", "time_ms": 1727632781384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  8:   from jax import xla_computation as _xla_computation
  9: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  9:   from jax import xla_computation as _xla_computation
 10: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 10:   from jax import xla_computation as _xla_computation
 11: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 11:   from jax import xla_computation as _xla_computation
 12: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 12:   from jax import xla_computation as _xla_computation
 13: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 13:   from jax import xla_computation as _xla_computation
 14: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 14:   from jax import xla_computation as _xla_computation
 15: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 15:   from jax import xla_computation as _xla_computation
 56: [W929 17:59:41.855201651 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 57: [W929 17:59:41.855146057 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: [W929 17:59:41.855144164 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 59: [W929 17:59:41.855141181 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 60: [W929 17:59:41.855172047 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 61: [W929 17:59:41.855144429 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 62: [W929 17:59:41.855144335 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 63: [W929 17:59:41.855144395 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 58: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 59: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 57: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 61: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 62: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 63: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 60: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 56: :::MLLOG {"namespace": "", "time_ms": 1727632781470, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
120: [W929 17:59:41.121179326 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: [W929 17:59:41.121127936 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
122: [W929 17:59:41.121127839 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
123: [W929 17:59:41.121165489 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
125: [W929 17:59:41.121128931 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
126: [W929 17:59:41.121124939 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
127: [W929 17:59:41.121126796 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
124: [W929 17:59:41.121289753 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
121: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
122: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
124: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
125: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
126: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
127: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
120: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
123: :::MLLOG {"namespace": "", "time_ms": 1727632781531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: 2024-09-29 17:59:41.698320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 25: 2024-09-29 17:59:41.698319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 26: 2024-09-29 17:59:41.698328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 27: 2024-09-29 17:59:41.698319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 28: 2024-09-29 17:59:41.698320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 29: 2024-09-29 17:59:41.698319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 30: 2024-09-29 17:59:41.698320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 31: 2024-09-29 17:59:41.698331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 10: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 11: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 12: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 13: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 15: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 14: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: 2024-09-29 17:59:42.021052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  1: 2024-09-29 17:59:42.021050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  2: 2024-09-29 17:59:42.021052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  3: 2024-09-29 17:59:42.021047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  4: 2024-09-29 17:59:42.021047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  5: 2024-09-29 17:59:42.021047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  6: 2024-09-29 17:59:42.021055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  7: 2024-09-29 17:59:42.021047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 24: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 24:   from jax import xla_computation as _xla_computation
 25: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 25:   from jax import xla_computation as _xla_computation
 26: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 26:   from jax import xla_computation as _xla_computation
 27: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 27:   from jax import xla_computation as _xla_computation
 28: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 28:   from jax import xla_computation as _xla_computation
 29: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 29:   from jax import xla_computation as _xla_computation
 30: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 30:   from jax import xla_computation as _xla_computation
 31: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 31:   from jax import xla_computation as _xla_computation
 16: 2024-09-29 17:59:42.142973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 17: 2024-09-29 17:59:42.142964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 18: 2024-09-29 17:59:42.142972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 19: 2024-09-29 17:59:42.142951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 20: 2024-09-29 17:59:42.142968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 21: 2024-09-29 17:59:42.142942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 22: 2024-09-29 17:59:42.142948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
 23: 2024-09-29 17:59:42.142942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  0:   from jax import xla_computation as _xla_computation
  1: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  1:   from jax import xla_computation as _xla_computation
  2: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  2:   from jax import xla_computation as _xla_computation
  3: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  3:   from jax import xla_computation as _xla_computation
  4: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  4:   from jax import xla_computation as _xla_computation
  5: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  5:   from jax import xla_computation as _xla_computation
  6: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  6:   from jax import xla_computation as _xla_computation
  7: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
  7:   from jax import xla_computation as _xla_computation
 16: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 16:   from jax import xla_computation as _xla_computation
 17: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 17:   from jax import xla_computation as _xla_computation
 18: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 18:   from jax import xla_computation as _xla_computation
 19: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 19:   from jax import xla_computation as _xla_computation
 20: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 20:   from jax import xla_computation as _xla_computation
 21: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 21:   from jax import xla_computation as _xla_computation
 22: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 22:   from jax import xla_computation as _xla_computation
 23: /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.
 23:   from jax import xla_computation as _xla_computation
 24: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 25: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 26: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 27: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 28: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 29: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 30: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 31: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  8: [W929 17:59:42.147919055 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: [W929 17:59:42.147834887 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 10: [W929 17:59:42.147845566 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 11: [W929 17:59:42.147834955 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 12: [W929 17:59:42.147860457 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 13: [W929 17:59:42.147860704 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 14: [W929 17:59:42.147878106 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 15: [W929 17:59:42.147865541 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  9: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 10: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 11: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 12: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 13: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 15: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 14: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  8: :::MLLOG {"namespace": "", "time_ms": 1727632782784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  1: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  2: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  4: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  5: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  6: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  7: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  3: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 16: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 17: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 18: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 19: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 20: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 21: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 22: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
 23: <frozen importlib._bootstrap>:671: ImportWarning: _SixMetaPathImporter.exec_module() not found; falling back to load_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: [W929 17:59:43.616104423 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 27: [W929 17:59:43.616119635 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 28: [W929 17:59:43.616107897 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 29: [W929 17:59:43.616139536 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 30: [W929 17:59:43.616104034 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 24: [W929 17:59:43.616146789 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 26: [W929 17:59:43.616162994 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 31: [W929 17:59:43.616142746 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 25: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 27: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 28: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 30: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 24: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 31: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 26: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 29: :::MLLOG {"namespace": "", "time_ms": 1727632783566, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: [W929 17:59:44.409093294 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  1: [W929 17:59:44.409132855 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  2: [W929 17:59:44.409093246 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  3: [W929 17:59:44.409108299 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  4: [W929 17:59:44.409071112 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: [W929 17:59:44.409068703 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  6: [W929 17:59:44.409142979 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  7: [W929 17:59:44.409071075 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
  5: :::MLLOG {"namespace": "", "time_ms": 1727632784112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  7: :::MLLOG {"namespace": "", "time_ms": 1727632784112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  4: :::MLLOG {"namespace": "", "time_ms": 1727632784112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632784113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  1: :::MLLOG {"namespace": "", "time_ms": 1727632784113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  2: :::MLLOG {"namespace": "", "time_ms": 1727632784113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  3: :::MLLOG {"namespace": "", "time_ms": 1727632784113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
  6: :::MLLOG {"namespace": "", "time_ms": 1727632784113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 48: [W929 17:59:44.006593710 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 87: [W929 17:59:44.896585220 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 82: [W929 17:59:44.900630521 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 49: [W929 17:59:44.022068788 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 81: [W929 17:59:44.955296235 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
104: [W929 17:59:44.035440644 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 83: [W929 17:59:44.979721430 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 84: [W929 17:59:44.983622023 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 85: [W929 17:59:44.988946232 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
110: [W929 17:59:44.065742040 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 65: [W929 17:59:44.089862866 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 96: [W929 17:59:44.289397168 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 64: [W929 17:59:44.096419091 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
105: [W929 17:59:44.077396203 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 71: [W929 17:59:44.101058740 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
108: [W929 17:59:44.082022411 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
109: [W929 17:59:44.095207002 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
117: [W929 17:59:44.399750140 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 92: [W929 17:59:44.765235496 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
114: [W929 17:59:44.403004519 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
106: [W929 17:59:44.102538561 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 88: [W929 17:59:44.775964504 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 70: [W929 17:59:44.135510399 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
112: [W929 17:59:44.419133711 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [W929 17:59:44.138006613 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 66: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
111: [W929 17:59:44.122467039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
116: [W929 17:59:44.437060818 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 16: [W929 17:59:44.611630891 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 17: [W929 17:59:44.611635429 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 18: [W929 17:59:44.611635510 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 19: [W929 17:59:44.611664278 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 20: [W929 17:59:44.611601800 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 21: [W929 17:59:44.611601817 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 22: [W929 17:59:44.611601725 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 23: [W929 17:59:44.611601863 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 82: using fp8 FMHA
 87: using fp8 FMHA
 67: [W929 17:59:44.158738622 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
119: [W929 17:59:44.443888902 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 20: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 21: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 22: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 23: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 16: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 17: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 18: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 19: :::MLLOG {"namespace": "", "time_ms": 1727632784304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1270}}
 69: [W929 17:59:44.175612807 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 95: [W929 17:59:44.829014684 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
107: [W929 17:59:44.164378380 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 90: [W929 17:59:44.838166374 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
115: [W929 17:59:44.477474584 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 47: [W929 17:59:44.711735895 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 45: [W929 17:59:44.713250065 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 68: [W929 17:59:44.203115679 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 94: [W929 17:59:44.850583340 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
118: [W929 17:59:44.489105057 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 52: [W929 17:59:44.235720105 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 49: using fp8 FMHA
 93: [W929 17:59:44.859582353 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 99: [W929 17:59:44.412275279 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 91: [W929 17:59:44.862842355 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
113: [W929 17:59:44.501722848 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 81: using fp8 FMHA
 83: using fp8 FMHA
 89: [W929 17:59:44.882783357 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 16: [W929 17:59:44.699417061 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 42: [W929 17:59:44.760430576 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 84: using fp8 FMHA
 43: [W929 17:59:44.767279847 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 85: using fp8 FMHA
102: [W929 17:59:44.453044086 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 65: using fp8 FMHA
 41: [W929 17:59:44.783023845 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [W929 17:59:44.783198679 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [W929 17:59:44.783316215 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 46: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 40: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 32: [W929 17:59:44.116095963 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 44: [W929 17:59:44.789565136 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [W929 17:59:44.470965342 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
101: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 51: [W929 17:59:44.314330676 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 97: [W929 17:59:44.482437231 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
114: using fp8 FMHA
117: using fp8 FMHA
 34: [W929 17:59:44.150143993 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
110: using fp8 FMHA
 71: using fp8 FMHA
 92: using fp8 FMHA
108: using fp8 FMHA
105: using fp8 FMHA
 63: [W929 17:59:44.875962643 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: using fp8 FMHA
 63: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 48: using fp8 FMHA
 37: [W929 17:59:44.197453076 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 56: [W929 17:59:44.887604541 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
106: using fp8 FMHA
 36: [W929 17:59:44.203951704 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
116: using fp8 FMHA
 57: [W929 17:59:44.895690557 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 62: [W929 17:59:44.897005552 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
111: using fp8 FMHA
 39: [W929 17:59:44.210017067 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 70: using fp8 FMHA
119: using fp8 FMHA
 52: using fp8 FMHA
 66: using fp8 FMHA
 45: using fp8 FMHA
 47: using fp8 FMHA
 55: [W929 17:59:44.413270221 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 69: using fp8 FMHA
107: using fp8 FMHA
 35: [W929 17:59:44.235983237 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 59: [W929 17:59:44.924447313 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 67: using fp8 FMHA
 68: using fp8 FMHA
 33: [W929 17:59:44.248221172 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
121: [W929 17:59:44.144958743 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 60: [W929 17:59:44.940366825 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 90: using fp8 FMHA
125: [W929 17:59:44.150303826 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 99: using fp8 FMHA
 95: using fp8 FMHA
 42: using fp8 FMHA
 61: [W929 17:59:44.946529925 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
123: [W929 17:59:44.154259528 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: using fp8 FMHA
123: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
104: using fp8 FMHA
 38: [W929 17:59:44.265420625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: using fp8 FMHA
 38: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 91: using fp8 FMHA
 54: [W929 17:59:44.452969426 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 43: using fp8 FMHA
113: using fp8 FMHA
101: using fp8 FMHA
118: using fp8 FMHA
 89: using fp8 FMHA
 93: using fp8 FMHA
 94: using fp8 FMHA
120: [W929 17:59:44.183982655 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 58: [W929 17:59:44.983860532 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 96: using fp8 FMHA
 97: using fp8 FMHA
 51: using fp8 FMHA
 64: using fp8 FMHA
124: [W929 17:59:44.192939208 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 24: [W929 17:59:44.657405926 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
127: [W929 17:59:44.209806690 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
122: [W929 17:59:44.211690566 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 29: [W929 17:59:44.680976674 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 34: using fp8 FMHA
 44: using fp8 FMHA
 46: using fp8 FMHA
 41: using fp8 FMHA
 31: [W929 17:59:44.703715785 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
126: [W929 17:59:44.246247701 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
112: using fp8 FMHA
 88: using fp8 FMHA
 63: using fp8 FMHA
 53: [W929 17:59:44.560147658 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 55: using fp8 FMHA
 50: [W929 17:59:44.580594031 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 28: [W929 17:59:44.764031775 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 37: using fp8 FMHA
 39: using fp8 FMHA
 30: [W929 17:59:44.767937190 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 26: [W929 17:59:44.773738363 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 57: using fp8 FMHA
 36: using fp8 FMHA
 27: [W929 17:59:44.775166680 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
103: [W929 17:59:44.773415946 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 25: [W929 17:59:44.776232854 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
121: using fp8 FMHA
125: using fp8 FMHA
 62: using fp8 FMHA
123: using fp8 FMHA
 33: using fp8 FMHA
 54: using fp8 FMHA
 59: using fp8 FMHA
 35: using fp8 FMHA
 60: using fp8 FMHA
 61: using fp8 FMHA
100: [W929 17:59:44.806937932 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 38: using fp8 FMHA
 32: using fp8 FMHA
 98: [W929 17:59:44.849125888 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 58: using fp8 FMHA
127: using fp8 FMHA
122: using fp8 FMHA
  9: [W929 17:59:44.186024509 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
124: using fp8 FMHA
 40: using fp8 FMHA
 10: [W929 17:59:44.200430710 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
  8: [W929 17:59:44.212134842 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 53: using fp8 FMHA
 14: [W929 17:59:44.215837871 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 12: [W929 17:59:44.234499243 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
126: using fp8 FMHA
 29: using fp8 FMHA
 50: using fp8 FMHA
 31: using fp8 FMHA
 56: using fp8 FMHA
103: using fp8 FMHA
 28: using fp8 FMHA
 30: using fp8 FMHA
 26: using fp8 FMHA
 27: using fp8 FMHA
 13: [W929 17:59:44.291079885 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 11: [W929 17:59:44.292126262 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 25: using fp8 FMHA
 15: [W929 17:59:44.300593779 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
100: using fp8 FMHA
120: using fp8 FMHA
 98: using fp8 FMHA
 24: using fp8 FMHA
  9: using fp8 FMHA
 10: using fp8 FMHA
 72: [W929 17:59:45.937739438 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 74: [W929 17:59:45.961944709 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
 78: [W929 17:59:45.966293153 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [W929 17:59:45.818888376 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 80: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
 79: [W929 17:59:45.981205536 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 14: using fp8 FMHA
 77: [W929 17:59:45.987091700 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 12: using fp8 FMHA
 73: [W929 17:59:45.990751672 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 86: [W929 17:59:45.845844679 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 76: [W929 17:59:45.030180653 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 75: [W929 17:59:45.034762401 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 11: using fp8 FMHA
 13: using fp8 FMHA
 15: using fp8 FMHA
  2: [W929 17:59:45.519033032 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
  1: [W929 17:59:45.539937433 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
  5: [W929 17:59:45.548239415 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 73: using fp8 FMHA
 74: using fp8 FMHA
 77: using fp8 FMHA
 78: using fp8 FMHA
  8: using fp8 FMHA
  6: [W929 17:59:45.598037827 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 86: using fp8 FMHA
  7: [W929 17:59:45.607182899 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
  4: [W929 17:59:45.612702198 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
  3: [W929 17:59:45.616225680 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 79: using fp8 FMHA
 76: using fp8 FMHA
 75: using fp8 FMHA
  2: using fp8 FMHA
  5: using fp8 FMHA
  1: using fp8 FMHA
 72: using fp8 FMHA
  7: using fp8 FMHA
 80: using fp8 FMHA
  4: using fp8 FMHA
  6: using fp8 FMHA
  3: using fp8 FMHA
 21: [W929 17:59:45.881804575 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: device: cuda:5 n_gpu: 128, distributed training: True, 16-bits training: True
 22: [W929 17:59:45.906728516 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: device: cuda:6 n_gpu: 128, distributed training: True, 16-bits training: True
 19: [W929 17:59:45.965650981 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: device: cuda:3 n_gpu: 128, distributed training: True, 16-bits training: True
 20: [W929 17:59:45.966520305 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: device: cuda:4 n_gpu: 128, distributed training: True, 16-bits training: True
 17: [W929 17:59:45.978967746 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: device: cuda:1 n_gpu: 128, distributed training: True, 16-bits training: True
 23: [W929 17:59:45.986487840 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: device: cuda:7 n_gpu: 128, distributed training: True, 16-bits training: True
 18: [W929 17:59:45.994928884 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: device: cuda:2 n_gpu: 128, distributed training: True, 16-bits training: True
  0: [W929 17:59:45.984273785 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: device: cuda:0 n_gpu: 128, distributed training: True, 16-bits training: True
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1277}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785681, "event_type": "POINT_IN_TIME", "key": "seed", "value": 7191, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1279}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 9216, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1281}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 36, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1283}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1285}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1287}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 740.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1289}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632785682, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1291}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=7191, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
  0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_p
  0: g=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0))
 16: using fp8 FMHA
 21: using fp8 FMHA
 22: using fp8 FMHA
 20: using fp8 FMHA
 17: using fp8 FMHA
 23: using fp8 FMHA
 19: using fp8 FMHA
 18: using fp8 FMHA
  0: using fp8 FMHA
 82: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 82:   self._overflow_buf = torch.cuda.IntTensor([0])
 83: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 83:   self._overflow_buf = torch.cuda.IntTensor([0])
 87: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 87:   self._overflow_buf = torch.cuda.IntTensor([0])
 82: [rank82]:[W929 17:59:46.604677298 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:59:46.604677393 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:59:46.604655602 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 17:59:46.605174773 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 17:59:46.605178740 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 17:59:46.605161309 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
113:   self._overflow_buf = torch.cuda.IntTensor([0])
115: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
115:   self._overflow_buf = torch.cuda.IntTensor([0])
116: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
116:   self._overflow_buf = torch.cuda.IntTensor([0])
117: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
117:   self._overflow_buf = torch.cuda.IntTensor([0])
113: [rank113]:[W929 17:59:46.050474228 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:59:46.050460746 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:59:46.050454448 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 17:59:46.050454304 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 17:59:46.050810362 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 17:59:46.050805407 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 17:59:46.050831804 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 17:59:46.050830698 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 84:   self._overflow_buf = torch.cuda.IntTensor([0])
 84: [rank84]:[W929 17:59:46.685514486 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 17:59:46.686011086 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 81:   self._overflow_buf = torch.cuda.IntTensor([0])
 81: [rank81]:[W929 17:59:46.688991695 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 17:59:46.689467356 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 85:   self._overflow_buf = torch.cuda.IntTensor([0])
 85: [rank85]:[W929 17:59:46.694548018 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 17:59:46.695019539 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 49:   self._overflow_buf = torch.cuda.IntTensor([0])
 51: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 51:   self._overflow_buf = torch.cuda.IntTensor([0])
 52: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 52:   self._overflow_buf = torch.cuda.IntTensor([0])
 55: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 55:   self._overflow_buf = torch.cuda.IntTensor([0])
 49: [rank49]:[W929 17:59:46.829610349 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:59:46.829610295 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:59:46.829608267 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:59:46.829616277 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 17:59:46.830204879 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 17:59:46.830201565 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 17:59:46.830196070 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 17:59:46.830198333 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 64:   self._overflow_buf = torch.cuda.IntTensor([0])
 65: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 65:   self._overflow_buf = torch.cuda.IntTensor([0])
 68: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 68:   self._overflow_buf = torch.cuda.IntTensor([0])
 69: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 69:   self._overflow_buf = torch.cuda.IntTensor([0])
 71: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 71:   self._overflow_buf = torch.cuda.IntTensor([0])
 64: [rank64]:[W929 17:59:46.817022002 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:59:46.816973624 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:59:46.816929669 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:59:46.816929556 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:59:46.816929488 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 17:59:46.817459302 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 17:59:46.817456281 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 17:59:46.817474317 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: [rank64]:[W929 17:59:46.817576281 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 17:59:46.817561123 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
119:   self._overflow_buf = torch.cuda.IntTensor([0])
119: [rank119]:[W929 17:59:46.126776860 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 17:59:46.127145297 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
118:   self._overflow_buf = torch.cuda.IntTensor([0])
118: [rank118]:[W929 17:59:46.132662693 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 17:59:46.133024782 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
114:   self._overflow_buf = torch.cuda.IntTensor([0])
114: [rank114]:[W929 17:59:46.137347725 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 17:59:46.137704733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
112:   self._overflow_buf = torch.cuda.IntTensor([0])
112: [rank112]:[W929 17:59:46.138986068 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
112: [rank112]:[W929 17:59:46.139335312 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 97:   self._overflow_buf = torch.cuda.IntTensor([0])
 99: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 99:   self._overflow_buf = torch.cuda.IntTensor([0])
101: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
101:   self._overflow_buf = torch.cuda.IntTensor([0])
102: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
102:   self._overflow_buf = torch.cuda.IntTensor([0])
 97: [rank97]:[W929 17:59:46.057733236 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:59:46.057733161 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:59:46.057735078 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:59:46.057735157 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 17:59:46.058144226 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 17:59:46.058144048 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 17:59:46.058150851 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 17:59:46.058145979 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 54:   self._overflow_buf = torch.cuda.IntTensor([0])
 53: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 53:   self._overflow_buf = torch.cuda.IntTensor([0])
 54: [rank54]:[W929 17:59:47.909067124 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:59:47.909540228 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 17:59:47.909618773 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 17:59:47.910089399 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 48:   self._overflow_buf = torch.cuda.IntTensor([0])
 48: [rank48]:[W929 17:59:47.920887829 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 17:59:47.921440810 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 70:   self._overflow_buf = torch.cuda.IntTensor([0])
 70: [rank70]:[W929 17:59:47.903778290 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 66:   self._overflow_buf = torch.cuda.IntTensor([0])
 70: [rank70]:[W929 17:59:47.904312082 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [rank66]:[W929 17:59:47.904958405 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 67:   self._overflow_buf = torch.cuda.IntTensor([0])
 66: [rank66]:[W929 17:59:47.905476532 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:59:47.906021857 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 17:59:47.906630497 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
104:   self._overflow_buf = torch.cuda.IntTensor([0])
 50: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 50:   self._overflow_buf = torch.cuda.IntTensor([0])
108: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
108:   self._overflow_buf = torch.cuda.IntTensor([0])
104: [rank104]:[W929 17:59:47.923676758 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:59:47.923666501 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:59:47.971971760 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 17:59:47.924099286 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 17:59:47.972547464 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 17:59:47.924102416 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 96:   self._overflow_buf = torch.cuda.IntTensor([0])
 96: [rank96]:[W929 17:59:47.143583232 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 17:59:47.144013617 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
103:   self._overflow_buf = torch.cuda.IntTensor([0])
103: [rank103]:[W929 17:59:47.158769634 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 17:59:47.159171963 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
100:   self._overflow_buf = torch.cuda.IntTensor([0])
100: [rank100]:[W929 17:59:47.167388661 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 98:   self._overflow_buf = torch.cuda.IntTensor([0])
100: [rank100]:[W929 17:59:47.167777168 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:59:47.168493487 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 17:59:47.168902925 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 41:   self._overflow_buf = torch.cuda.IntTensor([0])
 42: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 42:   self._overflow_buf = torch.cuda.IntTensor([0])
 44: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 44:   self._overflow_buf = torch.cuda.IntTensor([0])
 45: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 45:   self._overflow_buf = torch.cuda.IntTensor([0])
 46: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 46:   self._overflow_buf = torch.cuda.IntTensor([0])
 41: [rank41]:[W929 17:59:47.493889082 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:59:47.493883251 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:59:47.493886267 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:59:47.493913044 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:59:47.493886194 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 17:59:47.494449306 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 17:59:47.494481173 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 17:59:47.494478917 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 17:59:47.494494552 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 17:59:47.494493365 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 57:   self._overflow_buf = torch.cuda.IntTensor([0])
 58: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 58:   self._overflow_buf = torch.cuda.IntTensor([0])
 59: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 59:   self._overflow_buf = torch.cuda.IntTensor([0])
 61: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 61:   self._overflow_buf = torch.cuda.IntTensor([0])
 62: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 62:   self._overflow_buf = torch.cuda.IntTensor([0])
 63: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 63:   self._overflow_buf = torch.cuda.IntTensor([0])
 57: [rank57]:[W929 17:59:47.540575753 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:59:47.540575663 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:59:47.540559884 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:59:47.540597129 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 17:59:47.540590138 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:59:47.540596778 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 17:59:47.541080889 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 17:59:47.541065695 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 17:59:47.541094522 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 17:59:47.541097463 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 17:59:47.541098828 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 17:59:47.541109507 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 33:   self._overflow_buf = torch.cuda.IntTensor([0])
 33: [rank33]:[W929 17:59:47.866063973 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 17:59:47.866671079 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 34:   self._overflow_buf = torch.cuda.IntTensor([0])
 34: [rank34]:[W929 17:59:47.867338658 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 17:59:47.867931863 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 37:   self._overflow_buf = torch.cuda.IntTensor([0])
 37: [rank37]:[W929 17:59:47.873745113 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 17:59:47.874362364 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 39:   self._overflow_buf = torch.cuda.IntTensor([0])
 39: [rank39]:[W929 17:59:47.876333585 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 17:59:47.876940281 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
105:   self._overflow_buf = torch.cuda.IntTensor([0])
105: [rank105]:[W929 17:59:47.021017470 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 17:59:47.021458930 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
107:   self._overflow_buf = torch.cuda.IntTensor([0])
110: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
110:   self._overflow_buf = torch.cuda.IntTensor([0])
110: [rank110]:[W929 17:59:47.023798671 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:59:47.023966693 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 17:59:47.024210012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 17:59:47.024371707 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
111:   self._overflow_buf = torch.cuda.IntTensor([0])
109: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
109:   self._overflow_buf = torch.cuda.IntTensor([0])
106: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
106:   self._overflow_buf = torch.cuda.IntTensor([0])
111: [rank111]:[W929 17:59:47.026892656 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:59:47.026983510 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:59:47.027020495 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 17:59:47.027284986 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
109: [rank109]:[W929 17:59:47.027374291 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 17:59:47.027438750 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 43:   self._overflow_buf = torch.cuda.IntTensor([0])
 43: [rank43]:[W929 17:59:47.574303874 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: [rank43]:[W929 17:59:47.574930732 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 47:   self._overflow_buf = torch.cuda.IntTensor([0])
 47: [rank47]:[W929 17:59:47.576498636 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 17:59:47.577081943 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 89:   self._overflow_buf = torch.cuda.IntTensor([0])
 90: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 90:   self._overflow_buf = torch.cuda.IntTensor([0])
 91: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 91:   self._overflow_buf = torch.cuda.IntTensor([0])
 94: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 94:   self._overflow_buf = torch.cuda.IntTensor([0])
 89: [rank89]:[W929 17:59:47.712666087 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:59:47.712666071 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:59:47.712665993 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:59:47.712684706 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 17:59:47.713125881 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 17:59:47.713126909 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 17:59:47.713113530 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 17:59:47.713109255 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 40:   self._overflow_buf = torch.cuda.IntTensor([0])
 40: [rank40]:[W929 17:59:47.586417417 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 17:59:47.587009189 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 60:   self._overflow_buf = torch.cuda.IntTensor([0])
 60: [rank60]:[W929 17:59:47.617603717 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 17:59:47.618132753 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 56:   self._overflow_buf = torch.cuda.IntTensor([0])
 56: [rank56]:[W929 17:59:47.625644364 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 17:59:47.626196582 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 38:   self._overflow_buf = torch.cuda.IntTensor([0])
 38: [rank38]:[W929 17:59:47.946981677 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 17:59:47.947577915 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 36:   self._overflow_buf = torch.cuda.IntTensor([0])
 35: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 35:   self._overflow_buf = torch.cuda.IntTensor([0])
 36: [rank36]:[W929 17:59:47.949070256 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:59:47.949301706 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 17:59:47.949691460 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 17:59:47.949924229 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 32:   self._overflow_buf = torch.cuda.IntTensor([0])
 32: [rank32]:[W929 17:59:47.951058504 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 17:59:47.951664985 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
121:   self._overflow_buf = torch.cuda.IntTensor([0])
122: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
122:   self._overflow_buf = torch.cuda.IntTensor([0])
125: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
125:   self._overflow_buf = torch.cuda.IntTensor([0])
126: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
126:   self._overflow_buf = torch.cuda.IntTensor([0])
127: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
127:   self._overflow_buf = torch.cuda.IntTensor([0])
121: [rank121]:[W929 17:59:47.846433478 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:59:47.846443566 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:59:47.846469102 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:59:47.846481437 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:59:47.846484859 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 17:59:47.846764480 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 17:59:47.846763894 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 17:59:47.846784541 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 17:59:47.846801865 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
125: [rank125]:[W929 17:59:47.846825361 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 88:   self._overflow_buf = torch.cuda.IntTensor([0])
 88: [rank88]:[W929 17:59:47.792431369 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 17:59:47.792922246 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 93:   self._overflow_buf = torch.cuda.IntTensor([0])
 92: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 92:   self._overflow_buf = torch.cuda.IntTensor([0])
 93: [rank93]:[W929 17:59:47.803536733 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 17:59:47.803987330 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:59:47.804211878 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 17:59:47.804625980 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 95:   self._overflow_buf = torch.cuda.IntTensor([0])
 95: [rank95]:[W929 17:59:47.806153683 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 17:59:47.806613246 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
124:   self._overflow_buf = torch.cuda.IntTensor([0])
124: [rank124]:[W929 17:59:47.928882721 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
124: [rank124]:[W929 17:59:47.929199851 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
123:   self._overflow_buf = torch.cuda.IntTensor([0])
123: [rank123]:[W929 17:59:47.931278351 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
123: [rank123]:[W929 17:59:47.931731183 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
120:   self._overflow_buf = torch.cuda.IntTensor([0])
120: [rank120]:[W929 17:59:47.944502921 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: [rank120]:[W929 17:59:47.944856683 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 25:   self._overflow_buf = torch.cuda.IntTensor([0])
 27: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 27:   self._overflow_buf = torch.cuda.IntTensor([0])
 28: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 28:   self._overflow_buf = torch.cuda.IntTensor([0])
 30: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 30:   self._overflow_buf = torch.cuda.IntTensor([0])
 25: [rank25]:[W929 17:59:47.616814840 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:59:47.616814927 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:59:47.616815776 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:59:47.616815857 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 17:59:47.617447726 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 17:59:47.617463534 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 17:59:47.617479208 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 17:59:47.617481138 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  9:   self._overflow_buf = torch.cuda.IntTensor([0])
 10: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 10:   self._overflow_buf = torch.cuda.IntTensor([0])
 11: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 11:   self._overflow_buf = torch.cuda.IntTensor([0])
 12: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 12:   self._overflow_buf = torch.cuda.IntTensor([0])
 13: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 13:   self._overflow_buf = torch.cuda.IntTensor([0])
 15: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 15:   self._overflow_buf = torch.cuda.IntTensor([0])
  9: [rank9]:[W929 17:59:47.945536525 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:59:47.945536444 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:59:47.945536538 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:59:47.945563899 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:59:47.945563641 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:59:47.945563771 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 17:59:47.946260182 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 10: [rank10]:[W929 17:59:47.946261723 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 11: [rank11]:[W929 17:59:47.946260889 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 17:59:47.946374368 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 17:59:47.946347751 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 17:59:47.946374411 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 86:   self._overflow_buf = torch.cuda.IntTensor([0])
 86: [rank86]:[W929 17:59:47.359612230 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 17:59:47.360043749 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 80:   self._overflow_buf = torch.cuda.IntTensor([0])
 80: [rank80]:[W929 17:59:47.364844206 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 17:59:47.365293437 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 26:   self._overflow_buf = torch.cuda.IntTensor([0])
 26: [rank26]:[W929 17:59:47.690330451 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 17:59:47.691000038 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 31:   self._overflow_buf = torch.cuda.IntTensor([0])
 29: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 29:   self._overflow_buf = torch.cuda.IntTensor([0])
 31: [rank31]:[W929 17:59:47.698969183 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:59:47.699483021 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 17:59:47.699630080 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 17:59:47.700146725 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 14:   self._overflow_buf = torch.cuda.IntTensor([0])
 24: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 24:   self._overflow_buf = torch.cuda.IntTensor([0])
 24: [rank24]:[W929 17:59:47.705226900 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 17:59:47.705868827 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:59:47.018540668 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 17:59:47.019252526 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  8:   self._overflow_buf = torch.cuda.IntTensor([0])
  8: [rank8]:[W929 17:59:47.025842922 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 17:59:47.026573261 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 72:   self._overflow_buf = torch.cuda.IntTensor([0])
 73: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 73:   self._overflow_buf = torch.cuda.IntTensor([0])
 77: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 77:   self._overflow_buf = torch.cuda.IntTensor([0])
 79: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 79:   self._overflow_buf = torch.cuda.IntTensor([0])
 77: [rank77]:[W929 17:59:48.063812468 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:59:48.063812094 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:59:48.063855973 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 17:59:48.063858788 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 17:59:48.064319915 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 17:59:48.064330679 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 17:59:48.064389084 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 17:59:48.064389876 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 78:   self._overflow_buf = torch.cuda.IntTensor([0])
 78: [rank78]:[W929 17:59:48.145463835 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 74:   self._overflow_buf = torch.cuda.IntTensor([0])
 78: [rank78]:[W929 17:59:48.145941789 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:59:48.146529128 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 17:59:48.147077088 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 75:   self._overflow_buf = torch.cuda.IntTensor([0])
 75: [rank75]:[W929 17:59:48.149349315 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 76:   self._overflow_buf = torch.cuda.IntTensor([0])
 75: [rank75]:[W929 17:59:48.149862853 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:59:48.150475359 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 17:59:48.150946171 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 16:   self._overflow_buf = torch.cuda.IntTensor([0])
 17: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 17:   self._overflow_buf = torch.cuda.IntTensor([0])
 18: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 18:   self._overflow_buf = torch.cuda.IntTensor([0])
 20: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 20:   self._overflow_buf = torch.cuda.IntTensor([0])
 21: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 21:   self._overflow_buf = torch.cuda.IntTensor([0])
 22: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 22:   self._overflow_buf = torch.cuda.IntTensor([0])
 23: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 23:   self._overflow_buf = torch.cuda.IntTensor([0])
 16: [rank16]:[W929 17:59:48.606908651 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:59:48.606908812 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:59:48.606908722 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:59:48.606885119 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:59:48.606885196 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:59:48.606885211 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:59:48.606903112 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 17:59:48.607678685 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 17:59:48.607683918 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 20: [rank20]:[W929 17:59:48.607679557 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 17:59:48.607667456 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 17:59:48.607667480 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 17:59:48.607698353 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 17:59:48.607780436 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 19:   self._overflow_buf = torch.cuda.IntTensor([0])
 19: [rank19]:[W929 17:59:48.681940367 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 17:59:48.682593326 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788437, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/word_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788437, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/position_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788437, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/token_type_embeddings"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/embeddings/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788438, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788439, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788440, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788441, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788442, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788443, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788444, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788445, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788446, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788447, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788448, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788449, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788450, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788451, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788452, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788453, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788454, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "bert/pooler/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/output_bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/kernel"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/dense/bias"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_weights"}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788455, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 887, "tensor": "cls/seq_relationship/output_bias"}}
  2: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  2:   self._overflow_buf = torch.cuda.IntTensor([0])
  4: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  4:   self._overflow_buf = torch.cuda.IntTensor([0])
  5: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  5:   self._overflow_buf = torch.cuda.IntTensor([0])
  7: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  7:   self._overflow_buf = torch.cuda.IntTensor([0])
  4: [rank4]:[W929 17:59:48.031118953 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:59:48.031119036 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:59:48.031119100 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:59:48.031154821 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 17:59:48.031934078 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 17:59:48.031936468 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 17:59:48.031929398 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 17:59:48.032113472 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: :::MLLOG {"namespace": "", "time_ms": 1727632788729, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 917}}
  0: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  0:   self._overflow_buf = torch.cuda.IntTensor([0])
  0: [rank0]:[W929 17:59:48.035518367 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 17:59:48.036578658 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: NCCL version 2.21.5+cuda12.4
  3: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  3:   self._overflow_buf = torch.cuda.IntTensor([0])
  3: [rank3]:[W929 17:59:48.108280414 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 17:59:48.109145459 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  1:   self._overflow_buf = torch.cuda.IntTensor([0])
  1: [rank1]:[W929 17:59:48.109736979 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  6:   self._overflow_buf = torch.cuda.IntTensor([0])
  6: [rank6]:[W929 17:59:48.110347121 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 17:59:48.110606337 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 17:59:48.111270619 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 33: [rank33]:[W929 18:00:04.584580244 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  1: [rank1]:[W929 18:00:04.179476802 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 17: [rank17]:[W929 18:00:04.193965506 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  2: [rank2]:[W929 18:00:04.179531311 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 34: [rank34]:[W929 18:00:04.584651110 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 25: [rank25]:[W929 18:00:04.935641717 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  3: [rank3]:[W929 18:00:04.179615546 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 35: [rank35]:[W929 18:00:04.584869970 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  4: [rank4]:[W929 18:00:04.179577904 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 36: [rank36]:[W929 18:00:04.584865268 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  5: [rank5]:[W929 18:00:04.179691376 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  7: [rank7]:[W929 18:00:04.179783919 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 73: [rank73]:[W929 18:00:04.799243491 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 41: [rank41]:[W929 18:00:04.252246600 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  0: [rank0]:[W929 18:00:04.179806692 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 89: [rank89]:[W929 18:00:04.383513149 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  9: [rank9]:[W929 18:00:04.249139012 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  6: [rank6]:[W929 18:00:04.179833148 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 27: [rank27]:[W929 18:00:04.935739642 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 26: [rank26]:[W929 18:00:04.935824925 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 28: [rank28]:[W929 18:00:04.935813747 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 57: [rank57]:[W929 18:00:04.272222355 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 30: [rank30]:[W929 18:00:04.935939432 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 29: [rank29]:[W929 18:00:04.936060420 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 49: [rank49]:[W929 18:00:04.766154874 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 65: [rank65]:[W929 18:00:04.738614334 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 97: [rank97]:[W929 18:00:04.933732694 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: [rank32]:[W929 18:00:04.585082140 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
105: [rank105]:[W929 18:00:04.718251731 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 18: [rank18]:[W929 18:00:04.194043789 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 37: [rank37]:[W929 18:00:04.584918065 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
106: [rank106]:[W929 18:00:04.718323851 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
121: [rank121]:[W929 18:00:04.477874806 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 58: [rank58]:[W929 18:00:04.272300476 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 19: [rank19]:[W929 18:00:04.194229390 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 74: [rank74]:[W929 18:00:04.799359330 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 38: [rank38]:[W929 18:00:04.585036339 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
113: [rank113]:[W929 18:00:04.020286403 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 90: [rank90]:[W929 18:00:04.383542494 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
107: [rank107]:[W929 18:00:04.718380145 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 59: [rank59]:[W929 18:00:04.272337578 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: NCCL version 2.21.5+cuda12.4
 20: [rank20]:[W929 18:00:04.194159076 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 75: [rank75]:[W929 18:00:04.799424689 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 39: [rank39]:[W929 18:00:04.584988866 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: [rank24]:[W929 18:00:04.936115231 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 50: [rank50]:[W929 18:00:04.766364686 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 42: [rank42]:[W929 18:00:04.252336295 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 91: [rank91]:[W929 18:00:04.383578672 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 81: [rank81]:[W929 18:00:04.651719084 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
108: [rank108]:[W929 18:00:04.718303552 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 60: [rank60]:[W929 18:00:04.272524569 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 21: [rank21]:[W929 18:00:04.194224401 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 76: [rank76]:[W929 18:00:04.799464379 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 51: [rank51]:[W929 18:00:04.766256870 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 44: [rank44]:[W929 18:00:04.252400367 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 98: [rank98]:[W929 18:00:04.933927678 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 92: [rank92]:[W929 18:00:04.383790229 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: NCCL version 2.21.5+cuda12.4
 10: [rank10]:[W929 18:00:04.249194039 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: [rank104]:[W929 18:00:04.718504751 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 61: [rank61]:[W929 18:00:04.272545009 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 22: [rank22]:[W929 18:00:04.194284000 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 77: [rank77]:[W929 18:00:04.799376059 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 31: [rank31]:[W929 18:00:04.936131200 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 52: [rank52]:[W929 18:00:04.766317998 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 43: [rank43]:[W929 18:00:04.252480545 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 64: [rank64]:[W929 18:00:04.739003049 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 99: [rank99]:[W929 18:00:04.933832895 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
104: NCCL version 2.21.5+cuda12.4
 11: [rank11]:[W929 18:00:04.249209965 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: NCCL version 2.21.5+cuda12.4
109: [rank109]:[W929 18:00:04.718555913 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 62: [rank62]:[W929 18:00:04.272550018 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 23: [rank23]:[W929 18:00:04.194328807 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 79: [rank79]:[W929 18:00:04.799525341 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: [rank48]:[W929 18:00:04.766620293 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 45: [rank45]:[W929 18:00:04.252452839 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 66: [rank66]:[W929 18:00:04.738875647 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
101: [rank101]:[W929 18:00:04.934013970 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 12: [rank12]:[W929 18:00:04.249316190 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 82: [rank82]:[W929 18:00:04.651742131 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 63: [rank63]:[W929 18:00:04.272579475 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 16: [rank16]:[W929 18:00:04.194343995 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 72: [rank72]:[W929 18:00:04.799536280 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 53: [rank53]:[W929 18:00:04.766517537 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 46: [rank46]:[W929 18:00:04.252500548 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 67: [rank67]:[W929 18:00:04.738875621 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
102: [rank102]:[W929 18:00:04.934032707 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 13: [rank13]:[W929 18:00:04.249415523 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 83: [rank83]:[W929 18:00:04.651753319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 78: [rank78]:[W929 18:00:04.799597997 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 54: [rank54]:[W929 18:00:04.766582482 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 40: [rank40]:[W929 18:00:04.252696022 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 68: [rank68]:[W929 18:00:04.738796816 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
100: [rank100]:[W929 18:00:04.934054738 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 88: [rank88]:[W929 18:00:04.384043830 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 15: [rank15]:[W929 18:00:04.249474654 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 80: [rank80]:[W929 18:00:04.652007568 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 55: [rank55]:[W929 18:00:04.766503943 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 47: [rank47]:[W929 18:00:04.252681364 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 69: [rank69]:[W929 18:00:04.738900982 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 14: [rank14]:[W929 18:00:04.249538625 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 84: [rank84]:[W929 18:00:04.651907785 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 71: [rank71]:[W929 18:00:04.738991761 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
  8: [rank8]:[W929 18:00:04.249598617 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 85: [rank85]:[W929 18:00:04.652018581 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 70: [rank70]:[W929 18:00:04.739086789 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 86: [rank86]:[W929 18:00:04.651934991 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: [rank96]:[W929 18:00:04.934197339 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 93: [rank93]:[W929 18:00:04.383889900 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 87: [rank87]:[W929 18:00:04.651990757 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
122: [rank122]:[W929 18:00:04.477939728 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 94: [rank94]:[W929 18:00:04.383860825 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 48: NCCL version 2.21.5+cuda12.4
123: [rank123]:[W929 18:00:04.478129226 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
103: [rank103]:[W929 18:00:04.934209389 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 95: [rank95]:[W929 18:00:04.383959966 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 32: NCCL version 2.21.5+cuda12.4
124: [rank124]:[W929 18:00:04.478144232 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 96: NCCL version 2.21.5+cuda12.4
125: [rank125]:[W929 18:00:04.478125355 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
115: [rank115]:[W929 18:00:04.020404333 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
126: [rank126]:[W929 18:00:04.478177378 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
114: [rank114]:[W929 18:00:04.020482608 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
127: [rank127]:[W929 18:00:04.478216078 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
116: [rank116]:[W929 18:00:04.020441870 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
117: [rank117]:[W929 18:00:04.020554866 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
118: [rank118]:[W929 18:00:04.020721079 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
119: [rank119]:[W929 18:00:04.020777434 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 24: NCCL version 2.21.5+cuda12.4
112: [rank112]:[W929 18:00:04.020793723 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
110: [rank110]:[W929 18:00:04.718587882 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
111: [rank111]:[W929 18:00:04.718602143 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: [rank56]:[W929 18:00:04.272709384 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
 56: NCCL version 2.21.5+cuda12.4
 88: NCCL version 2.21.5+cuda12.4
 80: NCCL version 2.21.5+cuda12.4
120: [rank120]:[W929 18:00:04.478344376 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
120: NCCL version 2.21.5+cuda12.4
112: NCCL version 2.21.5+cuda12.4
  8: NCCL version 2.21.5+cuda12.4
 40: NCCL version 2.21.5+cuda12.4
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
 56: Torch distributed is available.
 56: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
112: Torch distributed is available.
112: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
 40: Torch distributed is available.
 40: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
109: Torch distributed is available.
109: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812852, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 956}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812852, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 957}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812852, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.6, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 959}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812852, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.7, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 960}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812852, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 961}}
 66: Torch distributed is available.
 66: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812943, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812943, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632812944, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": -200000.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
  9: Torch distributed is available.
  9: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
  4: Torch distributed is available.
  4: Torch distributed is initialized.
  5: Torch distributed is available.
  5: Torch distributed is initialized.
  7: Torch distributed is available.
  7: Torch distributed is initialized.
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  2: Torch distributed is available.
  2: Torch distributed is initialized.
  3: Torch distributed is available.
  3: Torch distributed is initialized.
  6: Torch distributed is available.
  6: Torch distributed is initialized.
  1: Torch distributed is available.
  1: Torch distributed is initialized.
  2: Enabling make_graphed_callables for encoder!!
  7: Enabling make_graphed_callables for encoder!!
  4: Enabling make_graphed_callables for encoder!!
  5: Enabling make_graphed_callables for encoder!!
  1: Enabling make_graphed_callables for encoder!!
  0: Enabling make_graphed_callables for encoder!!
  3: Enabling make_graphed_callables for encoder!!
  6: Enabling make_graphed_callables for encoder!!
  5: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  7: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  2: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  3: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  0: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  1: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  6: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  4: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: Torch distributed is available.
112: Torch distributed is initialized.
113: Torch distributed is available.
113: Torch distributed is initialized.
115: Torch distributed is available.
115: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
117: Torch distributed is available.
117: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
119: Torch distributed is available.
119: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
 12: Torch distributed is available.
 12: Torch distributed is initialized.
 13: Torch distributed is available.
 13: Torch distributed is initialized.
  8: Torch distributed is available.
  8: Torch distributed is initialized.
  9: Torch distributed is available.
  9: Torch distributed is initialized.
 10: Torch distributed is available.
 10: Torch distributed is initialized.
 11: Torch distributed is available.
 11: Torch distributed is initialized.
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 15: Torch distributed is available.
 15: Torch distributed is initialized.
 56: Torch distributed is available.
 56: Torch distributed is initialized.
 57: Torch distributed is available.
 57: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
 59: Torch distributed is available.
 59: Torch distributed is initialized.
 60: Torch distributed is available.
 60: Torch distributed is initialized.
 61: Torch distributed is available.
 61: Torch distributed is initialized.
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 63: Torch distributed is available.
 63: Torch distributed is initialized.
 96: Torch distributed is available.
 96: Torch distributed is initialized.
 97: Torch distributed is available.
 97: Torch distributed is initialized.
 98: Torch distributed is available.
 98: Torch distributed is initialized.
 99: Torch distributed is available.
 99: Torch distributed is initialized.
100: Torch distributed is available.
100: Torch distributed is initialized.
101: Torch distributed is available.
101: Torch distributed is initialized.
102: Torch distributed is available.
102: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
 80: Torch distributed is available.
 80: Torch distributed is initialized.
 81: Torch distributed is available.
 81: Torch distributed is initialized.
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 84: Torch distributed is available.
 84: Torch distributed is initialized.
 85: Torch distributed is available.
 85: Torch distributed is initialized.
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
 88: Torch distributed is available.
 88: Torch distributed is initialized.
 89: Torch distributed is available.
 89: Torch distributed is initialized.
 90: Torch distributed is available.
 90: Torch distributed is initialized.
 91: Torch distributed is available.
 91: Torch distributed is initialized.
 92: Torch distributed is available.
 92: Torch distributed is initialized.
 93: Torch distributed is available.
 93: Torch distributed is initialized.
 94: Torch distributed is available.
 94: Torch distributed is initialized.
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 48: Torch distributed is available.
 48: Torch distributed is initialized.
 49: Torch distributed is available.
 49: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 53: Torch distributed is available.
 53: Torch distributed is initialized.
 54: Torch distributed is available.
 54: Torch distributed is initialized.
 55: Torch distributed is available.
 55: Torch distributed is initialized.
 24: Torch distributed is available.
 24: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
 26: Torch distributed is available.
 26: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 31: Torch distributed is available.
 31: Torch distributed is initialized.
 72: Torch distributed is available.
 72: Torch distributed is initialized.
 73: Torch distributed is available.
 73: Torch distributed is initialized.
 74: Torch distributed is available.
 74: Torch distributed is initialized.
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 76: Torch distributed is available.
 76: Torch distributed is initialized.
 77: Torch distributed is available.
 77: Torch distributed is initialized.
 78: Torch distributed is available.
 78: Torch distributed is initialized.
 79: Torch distributed is available.
 79: Torch distributed is initialized.
104: Torch distributed is available.
104: Torch distributed is initialized.
105: Torch distributed is available.
105: Torch distributed is initialized.
106: Torch distributed is available.
106: Torch distributed is initialized.
107: Torch distributed is available.
107: Torch distributed is initialized.
108: Torch distributed is available.
108: Torch distributed is initialized.
109: Torch distributed is available.
109: Torch distributed is initialized.
110: Torch distributed is available.
110: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
 16: Torch distributed is available.
 16: Torch distributed is initialized.
 17: Torch distributed is available.
 17: Torch distributed is initialized.
 18: Torch distributed is available.
 18: Torch distributed is initialized.
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 20: Torch distributed is available.
 20: Torch distributed is initialized.
 21: Torch distributed is available.
 21: Torch distributed is initialized.
 22: Torch distributed is available.
 22: Torch distributed is initialized.
 23: Torch distributed is available.
 23: Torch distributed is initialized.
 42: Torch distributed is available.
 42: Torch distributed is initialized.
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
 45: Torch distributed is available.
 45: Torch distributed is initialized.
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
 40: Torch distributed is available.
 40: Torch distributed is initialized.
 47: Torch distributed is available.
 47: Torch distributed is initialized.
 39: Torch distributed is available.
 39: Torch distributed is initialized.
 32: Torch distributed is available.
 32: Torch distributed is initialized.
 33: Torch distributed is available.
 33: Torch distributed is initialized.
 34: Torch distributed is available.
 34: Torch distributed is initialized.
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 36: Torch distributed is available.
 36: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
 38: Torch distributed is available.
 38: Torch distributed is initialized.
120: Torch distributed is available.
120: Torch distributed is initialized.
121: Torch distributed is available.
121: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
124: Torch distributed is available.
124: Torch distributed is initialized.
125: Torch distributed is available.
125: Torch distributed is initialized.
126: Torch distributed is available.
126: Torch distributed is initialized.
127: Torch distributed is available.
127: Torch distributed is initialized.
 64: Torch distributed is available.
 64: Torch distributed is initialized.
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 67: Torch distributed is available.
 67: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
 70: Torch distributed is available.
 70: Torch distributed is initialized.
 71: Torch distributed is available.
 71: Torch distributed is initialized.
113: Enabling make_graphed_callables for encoder!!
115: Enabling make_graphed_callables for encoder!!
117: Enabling make_graphed_callables for encoder!!
116: Enabling make_graphed_callables for encoder!!
112: Enabling make_graphed_callables for encoder!!
114: Enabling make_graphed_callables for encoder!!
118: Enabling make_graphed_callables for encoder!!
119: Enabling make_graphed_callables for encoder!!
 57: Enabling make_graphed_callables for encoder!!
 58: Enabling make_graphed_callables for encoder!!
 59: Enabling make_graphed_callables for encoder!!
 63: Enabling make_graphed_callables for encoder!!
 61: Enabling make_graphed_callables for encoder!!
 62: Enabling make_graphed_callables for encoder!!
  9: Enabling make_graphed_callables for encoder!!
 11: Enabling make_graphed_callables for encoder!!
 12: Enabling make_graphed_callables for encoder!!
 13: Enabling make_graphed_callables for encoder!!
 15: Enabling make_graphed_callables for encoder!!
 10: Enabling make_graphed_callables for encoder!!
 83: Enabling make_graphed_callables for encoder!!
 86: Enabling make_graphed_callables for encoder!!
 87: Enabling make_graphed_callables for encoder!!
 80: Enabling make_graphed_callables for encoder!!
 82: Enabling make_graphed_callables for encoder!!
 99: Enabling make_graphed_callables for encoder!!
101: Enabling make_graphed_callables for encoder!!
102: Enabling make_graphed_callables for encoder!!
 97: Enabling make_graphed_callables for encoder!!
 56: Enabling make_graphed_callables for encoder!!
 60: Enabling make_graphed_callables for encoder!!
  8: Enabling make_graphed_callables for encoder!!
 84: Enabling make_graphed_callables for encoder!!
 85: Enabling make_graphed_callables for encoder!!
 98: Enabling make_graphed_callables for encoder!!
 96: Enabling make_graphed_callables for encoder!!
 14: Enabling make_graphed_callables for encoder!!
100: Enabling make_graphed_callables for encoder!!
 49: Enabling make_graphed_callables for encoder!!
 89: Enabling make_graphed_callables for encoder!!
 51: Enabling make_graphed_callables for encoder!!
 52: Enabling make_graphed_callables for encoder!!
 55: Enabling make_graphed_callables for encoder!!
 90: Enabling make_graphed_callables for encoder!!
 91: Enabling make_graphed_callables for encoder!!
 94: Enabling make_graphed_callables for encoder!!
108: Enabling make_graphed_callables for encoder!!
104: Enabling make_graphed_callables for encoder!!
 81: Enabling make_graphed_callables for encoder!!
 72: Enabling make_graphed_callables for encoder!!
 73: Enabling make_graphed_callables for encoder!!
 77: Enabling make_graphed_callables for encoder!!
 79: Enabling make_graphed_callables for encoder!!
 27: Enabling make_graphed_callables for encoder!!
103: Enabling make_graphed_callables for encoder!!
 50: Enabling make_graphed_callables for encoder!!
 88: Enabling make_graphed_callables for encoder!!
 53: Enabling make_graphed_callables for encoder!!
 48: Enabling make_graphed_callables for encoder!!
 93: Enabling make_graphed_callables for encoder!!
 92: Enabling make_graphed_callables for encoder!!
 54: Enabling make_graphed_callables for encoder!!
 28: Enabling make_graphed_callables for encoder!!
 30: Enabling make_graphed_callables for encoder!!
 95: Enabling make_graphed_callables for encoder!!
 25: Enabling make_graphed_callables for encoder!!
 41: Enabling make_graphed_callables for encoder!!
 45: Enabling make_graphed_callables for encoder!!
 46: Enabling make_graphed_callables for encoder!!
 76: Enabling make_graphed_callables for encoder!!
 44: Enabling make_graphed_callables for encoder!!
 74: Enabling make_graphed_callables for encoder!!
 78: Enabling make_graphed_callables for encoder!!
106: Enabling make_graphed_callables for encoder!!
111: Enabling make_graphed_callables for encoder!!
105: Enabling make_graphed_callables for encoder!!
110: Enabling make_graphed_callables for encoder!!
 42: Enabling make_graphed_callables for encoder!!
 24: Enabling make_graphed_callables for encoder!!
 26: Enabling make_graphed_callables for encoder!!
 75: Enabling make_graphed_callables for encoder!!
 17: Enabling make_graphed_callables for encoder!!
 18: Enabling make_graphed_callables for encoder!!
 20: Enabling make_graphed_callables for encoder!!
 21: Enabling make_graphed_callables for encoder!!
 23: Enabling make_graphed_callables for encoder!!
 16: Enabling make_graphed_callables for encoder!!
 31: Enabling make_graphed_callables for encoder!!
 33: Enabling make_graphed_callables for encoder!!
 37: Enabling make_graphed_callables for encoder!!
 39: Enabling make_graphed_callables for encoder!!
 43: Enabling make_graphed_callables for encoder!!
 47: Enabling make_graphed_callables for encoder!!
 40: Enabling make_graphed_callables for encoder!!
107: Enabling make_graphed_callables for encoder!!
109: Enabling make_graphed_callables for encoder!!
121: Enabling make_graphed_callables for encoder!!
122: Enabling make_graphed_callables for encoder!!
125: Enabling make_graphed_callables for encoder!!
126: Enabling make_graphed_callables for encoder!!
 29: Enabling make_graphed_callables for encoder!!
 22: Enabling make_graphed_callables for encoder!!
 19: Enabling make_graphed_callables for encoder!!
 36: Enabling make_graphed_callables for encoder!!
 32: Enabling make_graphed_callables for encoder!!
 35: Enabling make_graphed_callables for encoder!!
 34: Enabling make_graphed_callables for encoder!!
 38: Enabling make_graphed_callables for encoder!!
 64: Enabling make_graphed_callables for encoder!!
 68: Enabling make_graphed_callables for encoder!!
 65: Enabling make_graphed_callables for encoder!!
127: Enabling make_graphed_callables for encoder!!
124: Enabling make_graphed_callables for encoder!!
 67: Enabling make_graphed_callables for encoder!!
120: Enabling make_graphed_callables for encoder!!
 66: Enabling make_graphed_callables for encoder!!
 70: Enabling make_graphed_callables for encoder!!
123: Enabling make_graphed_callables for encoder!!
 69: Enabling make_graphed_callables for encoder!!
 71: Enabling make_graphed_callables for encoder!!
119: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
116: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
115: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
112: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
113: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
117: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
118: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
114: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 59: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 58: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 63: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 97: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
101: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
100: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 56: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 57: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 61: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
103: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
102: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 60: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 99: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 62: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 51: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 98: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 82: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 84: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 52: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 83: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 80: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 87: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 86: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
104: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 89: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 96: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 85: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 50: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 81: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 94: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 91: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 53: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 54: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 49: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
107: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 88: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
108: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
111: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
106: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 55: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 48: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 90: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 92: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 93: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 95: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
105: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
110: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
109: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 37: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 32: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 33: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 72: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 77: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 79: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 34: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  9: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 10: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 39: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 11: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 12: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 35: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 78: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 74: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 36: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 38: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 15: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 14: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 42: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 46: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 40: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
  8: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 68: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 64: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 69: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 13: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 41: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 66: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 73: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 45: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 76: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 44: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 43: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 67: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
126: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
124: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 47: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
121: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 65: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 70: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 71: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
125: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
127: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
120: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
123: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
122: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 30: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 28: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 29: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 25: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 27: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 26: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 24: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 31: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 16: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 18: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 20: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 19: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 22: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 17: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 21: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 23: <frozen importlib._bootstrap>:914: ImportWarning: _SixMetaPathImporter.find_spec() not found; falling back to find_module()
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632848176, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632848176, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632848212, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1639, "epoch_num": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632848213, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1641, "first_epoch_num": 1, "epoch_count": 1}}
  0: parsed args:
  0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=36, eval_batch_size=16, learning_rate=0.002, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=740.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=7191, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_ch
  0: eckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_ag_p
  0: g=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=128, device=device(type='cuda', index=0), resume_step=0)
  0: epoch: 1
  0: :::MLLOG {"namespace": "", "time_ms": 1727632848215, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_02301", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1676}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632851862, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 42909.86501109167}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 156506}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632851862, "event_type": "INTERVAL_START", "key": "eval_start", "value": 156506, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 156506}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632852683, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 156506, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 156506}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632852683, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3636297881603241, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 156506}}
  0: {'global_steps': 17, 'eval_loss': 4.226527690887451, 'eval_mlm_accuracy': 0.3636297881603241}
 25: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 25:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 44: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 44:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 30: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 30:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 49:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 18: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 18:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 65:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 41: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 41:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 42:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 46:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 89: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 89:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 10: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 35:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 51: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 51:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 69:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 68:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 64:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 75: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 75:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 27: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 27:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 71:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 94:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 59:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 37:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 39:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 45:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 55: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 55:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 66: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 66:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
102: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
102:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 82: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 82:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 61:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 57: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 57:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 56: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 56:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 63:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 97: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 97:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 11: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 83: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 83:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 86: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 86:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 77: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 77:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
105: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
105:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 62: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 62:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 33: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 33:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 72: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 72:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 34: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 34:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
125: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
125:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 38: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 38:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
110: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
110:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 78: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 78:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
101: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
101:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
121: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
121:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
122: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
122:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 16: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 16:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 58: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 58:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 54: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 54:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 98: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 98:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 88: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 88:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  2:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
106:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 74: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 74:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 25: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 25:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 90:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 95: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 95:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 81: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 81:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 96: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 96:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 28: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 28:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
126: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
126:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 40: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 40:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 70: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 70:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 20: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 20:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 29: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 29:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
107: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
107:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 79: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 79:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 22: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 22:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 99: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 99:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 12: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 80: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 80:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 44: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 44:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 67:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 17: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 17:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 84: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 84:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 24:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 48: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 48:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  9: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
117: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
117:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 47: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 47:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 26: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 26:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 73: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 73:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
104: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
104:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 49: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 49:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 18: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 18:   warnings.warn(
 30: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 30:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 43:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 13: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 13:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 87:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 50: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 50:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 19: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 19:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 65: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 65:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 37: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 37:   warnings.warn(
 41: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 41:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 53:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
127: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
127:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 39: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 39:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
113:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 14: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 35: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 35:   warnings.warn(
115: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
115:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
116: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
116:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 69: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 69:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 21:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
108: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
108:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
100: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
100:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
120: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
120:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
103: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
103:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 61: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 61:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 31:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
111: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
111:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 42: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 42:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 85:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 59: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 59:   warnings.warn(
102: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
102:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 76:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 68: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 68:   warnings.warn(
 86: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 86:   warnings.warn(
 10: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 10:   warnings.warn(
 57: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 57:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
109:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 36: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 36:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 52: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 52:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 46: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 46:   warnings.warn(
 89: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 89:   warnings.warn(
 51: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 51:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 91:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 94: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 94:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 32:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
112: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
112:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 64: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 64:   warnings.warn(
 82: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 82:   warnings.warn(
 75: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 75:   warnings.warn(
 83: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 83:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
124:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
114: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
114:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
118: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
118:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 60: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 60:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 71: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 71:   warnings.warn(
 27: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 27:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 23:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
123: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
123:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 63: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 63:   warnings.warn(
 55: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 55:   warnings.warn(
 97: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 97:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
119:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 45: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 45:   warnings.warn(
 66: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 66:   warnings.warn(
105: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
105:   warnings.warn(
 11: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 11:   warnings.warn(
 56: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 56:   warnings.warn(
 77: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 77:   warnings.warn(
 38: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 38:   warnings.warn(
125: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
125:   warnings.warn(
 34: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 34:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 92:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 15: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 15:   warnings.warn(
 62: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 62:   warnings.warn(
 16: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 16:   warnings.warn(
 33: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 33:   warnings.warn(
101: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
101:   warnings.warn(
 72: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 72:   warnings.warn(
121: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
121:   warnings.warn(
 78: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 78:   warnings.warn(
122: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
122:   warnings.warn(
126: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
126:   warnings.warn(
 20: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 20:   warnings.warn(
 58: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 58:   warnings.warn(
 90: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 90:   warnings.warn(
 40: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 40:   warnings.warn(
 88: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 88:   warnings.warn(
 81: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 81:   warnings.warn(
110: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
110:   warnings.warn(
 17: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 17:   warnings.warn(
 96: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 96:   warnings.warn(
 74: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 74:   warnings.warn(
106: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
106:   warnings.warn(
 22: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 22:   warnings.warn(
 98: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 98:   warnings.warn(
  7: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  7:   warnings.warn(
 95: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 95:   warnings.warn(
 54: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 54:   warnings.warn(
113: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
113:   warnings.warn(
 67: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 67:   warnings.warn(
 19: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 19:   warnings.warn(
 70: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 70:   warnings.warn(
 99: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 99:   warnings.warn(
116: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
116:   warnings.warn(
 73: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 73:   warnings.warn(
  4: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  4:   warnings.warn(
 80: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 80:   warnings.warn(
 47: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 47:   warnings.warn(
 12: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 12:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
 93:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 24: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 24:   warnings.warn(
120: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
120:   warnings.warn(
117: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
117:   warnings.warn(
115: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
115:   warnings.warn(
127: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
127:   warnings.warn(
 28: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 28:   warnings.warn(
111: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
111:   warnings.warn(
 79: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 79:   warnings.warn(
 21: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 21:   warnings.warn(
 43: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 43:   warnings.warn(
 48: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 48:   warnings.warn(
  9: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  9:   warnings.warn(
104: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
104:   warnings.warn(
 29: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 29:   warnings.warn(
 84: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 84:   warnings.warn(
107: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
107:   warnings.warn(
 26: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 26:   warnings.warn(
 50: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 50:   warnings.warn(
112: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
112:   warnings.warn(
103: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
103:   warnings.warn(
  8: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  8:   warnings.warn(
  3: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  3:   warnings.warn(
  5: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  5:   warnings.warn(
108: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
108:   warnings.warn(
  6: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  6:   warnings.warn(
 14: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 14:   warnings.warn(
 36: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 36:   warnings.warn(
118: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
118:   warnings.warn(
100: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
100:   warnings.warn(
 91: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 91:   warnings.warn(
 52: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 52:   warnings.warn(
114: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
114:   warnings.warn(
 53: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 53:   warnings.warn(
109: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
109:   warnings.warn(
 31: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 31:   warnings.warn(
123: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
123:   warnings.warn(
124: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
124:   warnings.warn(
 76: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 76:   warnings.warn(
 87: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 87:   warnings.warn(
 60: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 60:   warnings.warn(
 23: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 23:   warnings.warn(
 85: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 85:   warnings.warn(
119: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
119:   warnings.warn(
 32: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 32:   warnings.warn(
  0: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  0:   warnings.warn(
  1: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  1:   warnings.warn(
 92: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 92:   warnings.warn(
 93: /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 93:   warnings.warn(
  0: :::MLLOG {"namespace": "", "time_ms": 1727632853951, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 70690.19414317817}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 304181}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632853951, "event_type": "INTERVAL_START", "key": "eval_start", "value": 304181, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 304181}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632854042, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 304181, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 304181}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632854042, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.36996275186538696, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 304181}}
  0: {'global_steps': 33, 'eval_loss': 4.199822425842285, 'eval_mlm_accuracy': 0.36996275186538696}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632855309, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108594.96368032282}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 451715}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632855310, "event_type": "INTERVAL_START", "key": "eval_start", "value": 451715, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 451715}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632855403, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 451715, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 451715}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632855404, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3706749677658081, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 451715}}
  0: {'global_steps': 49, 'eval_loss': 4.145687103271484, 'eval_mlm_accuracy': 0.3706749677658081}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632856752, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108390.92780713308}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 608062}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632856752, "event_type": "INTERVAL_START", "key": "eval_start", "value": 608062, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 608062}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632856867, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 608062, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 608062}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632856868, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3738064467906952, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 608062}}
  0: {'global_steps': 66, 'eval_loss': 4.108675956726074, 'eval_mlm_accuracy': 0.3738064467906952}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632858141, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 106113.1930031228}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 755477}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632858141, "event_type": "INTERVAL_START", "key": "eval_start", "value": 755477, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 755477}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632858231, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 755477, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 755477}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632858232, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3811318576335907, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 755477}}
  0: {'global_steps': 82, 'eval_loss': 4.046879768371582, 'eval_mlm_accuracy': 0.3811318576335907}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632859501, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108220.42188853478}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 902592}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632859501, "event_type": "INTERVAL_START", "key": "eval_start", "value": 902592, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 902592}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632859593, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 902592, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 902592}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632859593, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.39340782165527344, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 902592}}
  0: {'global_steps': 98, 'eval_loss': 3.930182695388794, 'eval_mlm_accuracy': 0.39340782165527344}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632860859, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108473.06844567125}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1049989}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632860860, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1049989, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1049989}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632860952, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1049989, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1049989}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632860952, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4033813178539276, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1049989}}
  0: {'global_steps': 114, 'eval_loss': 3.836350679397583, 'eval_mlm_accuracy': 0.4033813178539276}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632862309, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107613.3751785382}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1205964}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632862309, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1205964, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1205964}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632862402, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1205964, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1205964}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632862402, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.41512253880500793, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1205964}}
  0: {'global_steps': 131, 'eval_loss': 3.7291548252105713, 'eval_mlm_accuracy': 0.41512253880500793}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632863672, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107934.95018095481}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1353152}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632863673, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1353152, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1353152}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632863762, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1353152, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1353152}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632863762, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4308755695819855, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1353152}}
  0: {'global_steps': 147, 'eval_loss': 3.6007680892944336, 'eval_mlm_accuracy': 0.4308755695819855}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632865029, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108739.92090029737}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1500688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632865029, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1500688, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1500688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632865124, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1500688, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1500688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632865125, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.443371057510376, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1500688}}
  0: {'global_steps': 163, 'eval_loss': 3.4366087913513184, 'eval_mlm_accuracy': 0.443371057510376}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632866473, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108877.4138680777}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1657841}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632866473, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1657841, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1657841}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632866562, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1657841, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1657841}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632866562, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4767405688762665, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1657841}}
  0: {'global_steps': 180, 'eval_loss': 3.146932363510132, 'eval_mlm_accuracy': 0.4767405688762665}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632867831, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108275.49721588982}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1804917}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632867831, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1804917, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1804917}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632867922, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1804917, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1804917}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632867923, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.528854489326477, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1804917}}
  0: {'global_steps': 196, 'eval_loss': 2.731980323791504, 'eval_mlm_accuracy': 0.528854489326477}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632869192, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108101.35692704907}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 1952077}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632869193, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1952077, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 1952077}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632869295, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1952077, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 1952077}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632869295, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5793548226356506, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 1952077}}
  0: {'global_steps': 212, 'eval_loss': 2.325108766555786, 'eval_mlm_accuracy': 0.5793548226356506}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632870562, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107576.49945621191}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2099376}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632870562, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2099376, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2099376}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632870651, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2099376, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2099376}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632870652, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6273401379585266, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2099376}}
  0: {'global_steps': 228, 'eval_loss': 1.9655756950378418, 'eval_mlm_accuracy': 0.6273401379585266}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632871999, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108653.67989930832}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2255591}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632872000, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2255591, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2255591}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632872089, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2255591, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2255591}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632872089, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6736091375350952, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2255591}}
  0: {'global_steps': 245, 'eval_loss': 1.6187117099761963, 'eval_mlm_accuracy': 0.6736091375350952}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632873357, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108849.37715816557}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2403398}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632873357, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2403398, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2403398}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632873455, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2403398, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2403398}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632873456, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6955620050430298, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2403398}}
  0: {'global_steps': 261, 'eval_loss': 1.4678831100463867, 'eval_mlm_accuracy': 0.6955620050430298}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632874725, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107666.92860782932}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2550684}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632874725, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2550684, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2550684}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632874814, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2550684, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2550684}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632874815, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7053370475769043, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2550684}}
  0: {'global_steps': 277, 'eval_loss': 1.3988330364227295, 'eval_mlm_accuracy': 0.7053370475769043}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632876083, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108226.09904453137}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2697668}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632876084, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2697668, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2697668}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632876174, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2697668, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2697668}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632876174, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.708793044090271, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2697668}}
  0: {'global_steps': 293, 'eval_loss': 1.3820236921310425, 'eval_mlm_accuracy': 0.708793044090271}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632877523, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108745.32149083969}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 2854195}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632877523, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2854195, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 2854195}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632877614, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2854195, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 2854195}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632877614, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7103576064109802, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 2854195}}
  0: {'global_steps': 310, 'eval_loss': 1.3747265338897705, 'eval_mlm_accuracy': 0.7103576064109802}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632878883, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108274.48589727363}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3001451}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632878883, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3001451, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3001451}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632878983, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3001451, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3001451}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632878983, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7128632664680481, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3001451}}
  0: {'global_steps': 326, 'eval_loss': 1.357366919517517, 'eval_mlm_accuracy': 0.7128632664680481}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632880251, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 107713.66462917472}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3148794}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632880251, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3148794, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3148794}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632880340, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3148794, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3148794}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632880340, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7162469029426575, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3148794}}
  0: {'global_steps': 342, 'eval_loss': 1.3301811218261719, 'eval_mlm_accuracy': 0.7162469029426575}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632881690, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108837.15966076202}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3305476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632881690, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3305476, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3305476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632881809, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3305476, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3305476}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632881809, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7169124484062195, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3305476}}
  0: {'global_steps': 359, 'eval_loss': 1.3260023593902588, 'eval_mlm_accuracy': 0.7169124484062195}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632883078, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 106518.92381138513}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3453352}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632883079, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3453352, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3453352}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632883168, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3453352, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3453352}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632883168, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7175779938697815, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3453352}}
  0: {'global_steps': 375, 'eval_loss': 1.3200032711029053, 'eval_mlm_accuracy': 0.7175779938697815}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632884434, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108643.27531934278}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3600678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632884435, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3600678, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3600678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632884525, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3600678, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3600678}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632884525, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7186217904090881, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3600678}}
  0: {'global_steps': 391, 'eval_loss': 1.3162542581558228, 'eval_mlm_accuracy': 0.7186217904090881}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632885798, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108021.07054079651}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3747930}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632885798, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3747930, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3747930}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632885891, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3747930, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3747930}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632885891, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7188926935195923, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3747930}}
  0: {'global_steps': 407, 'eval_loss': 1.3141717910766602, 'eval_mlm_accuracy': 0.7188926935195923}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632887240, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108348.5706213102}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 3904236}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632887241, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3904236, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 3904236}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632887330, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3904236, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 3904236}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632887330, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7192242741584778, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 3904236}}
  0: {'global_steps': 424, 'eval_loss': 1.3105324506759644, 'eval_mlm_accuracy': 0.7192242741584778}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632888600, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108572.93423504927}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4051822}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632888600, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4051822, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4051822}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632888690, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4051822, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4051822}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632888690, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.719691276550293, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4051822}}
  0: {'global_steps': 440, 'eval_loss': 1.3096507787704468, 'eval_mlm_accuracy': 0.719691276550293}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632889959, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108604.51953514974}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4199505}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632889960, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4199505, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4199505}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632890058, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4199505, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4199505}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632890058, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7199855446815491, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4199505}}
  0: {'global_steps': 456, 'eval_loss': 1.3057141304016113, 'eval_mlm_accuracy': 0.7199855446815491}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632891408, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 108129.00872022401}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1850, "epoch_num": 4356132}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632891408, "event_type": "INTERVAL_START", "key": "eval_start", "value": 4356132, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1856, "epoch_num": 4356132}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632891500, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 4356132, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1862, "epoch_num": 4356132}}
  0: :::MLLOG {"namespace": "", "time_ms": 1727632891500, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7202680706977844, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1866, "epoch_num": 4356132}}
  0: {'global_steps': 473, 'eval_loss': 1.302546739578247, 'eval_mlm_accuracy': 0.7202680706977844}
  0: 0.720268 > 0.720000, Target MLM Accuracy reached at 473
  0: Training runs 0.9844068686167399 mins sustained_training_time 0
  1: [rank1]: Traceback (most recent call last):
  1: [rank1]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  1: [rank1]:     args, final_loss, train_time_raw = main()
  1: [rank1]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  1: [rank1]:     next_eval_step = eval_steps[eval_count]
  1: [rank1]: IndexError: list index out of range
  2: [rank2]: Traceback (most recent call last):
  2: [rank2]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  2: [rank2]:     args, final_loss, train_time_raw = main()
  2: [rank2]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  2: [rank2]:     next_eval_step = eval_steps[eval_count]
  2: [rank2]: IndexError: list index out of range
  7: [rank7]: Traceback (most recent call last):
  7: [rank7]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  7: [rank7]:     args, final_loss, train_time_raw = main()
  7: [rank7]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  7: [rank7]:     next_eval_step = eval_steps[eval_count]
  7: [rank7]: IndexError: list index out of range
  3: [rank3]: Traceback (most recent call last):
  3: [rank3]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  3: [rank3]:     args, final_loss, train_time_raw = main()
  3: [rank3]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  3: [rank3]:     next_eval_step = eval_steps[eval_count]
  3: [rank3]: IndexError: list index out of range
  6: [rank6]: Traceback (most recent call last):
  6: [rank6]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  6: [rank6]:     args, final_loss, train_time_raw = main()
  6: [rank6]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  6: [rank6]:     next_eval_step = eval_steps[eval_count]
  6: [rank6]: IndexError: list index out of range
  4: [rank4]: Traceback (most recent call last):
  4: [rank4]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  4: [rank4]:     args, final_loss, train_time_raw = main()
  4: [rank4]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  4: [rank4]:     next_eval_step = eval_steps[eval_count]
  4: [rank4]: IndexError: list index out of range
  5: [rank5]: Traceback (most recent call last):
  5: [rank5]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  5: [rank5]:     args, final_loss, train_time_raw = main()
  5: [rank5]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  5: [rank5]:     next_eval_step = eval_steps[eval_count]
  5: [rank5]: IndexError: list index out of range
  0: [rank0]: Traceback (most recent call last):
  0: [rank0]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  0: [rank0]:     args, final_loss, train_time_raw = main()
  0: [rank0]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  0: [rank0]:     next_eval_step = eval_steps[eval_count]
  0: [rank0]: IndexError: list index out of range
 35: [rank35]: Traceback (most recent call last):
 35: [rank35]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 35: [rank35]:     args, final_loss, train_time_raw = main()
 35: [rank35]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 35: [rank35]:     next_eval_step = eval_steps[eval_count]
 35: [rank35]: IndexError: list index out of range
 27: [rank27]: Traceback (most recent call last):
 27: [rank27]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 27: [rank27]:     args, final_loss, train_time_raw = main()
 27: [rank27]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 27: [rank27]:     next_eval_step = eval_steps[eval_count]
 27: [rank27]: IndexError: list index out of range
 97: [rank97]: Traceback (most recent call last):
 97: [rank97]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 97: [rank97]:     args, final_loss, train_time_raw = main()
 97: [rank97]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 97: [rank97]:     next_eval_step = eval_steps[eval_count]
 97: [rank97]: IndexError: list index out of range
121: [rank121]: Traceback (most recent call last):
121: [rank121]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
121: [rank121]:     args, final_loss, train_time_raw = main()
121: [rank121]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
121: [rank121]:     next_eval_step = eval_steps[eval_count]
121: [rank121]: IndexError: list index out of range
 58: [rank58]: Traceback (most recent call last):
 58: [rank58]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 58: [rank58]:     args, final_loss, train_time_raw = main()
 58: [rank58]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 58: [rank58]:     next_eval_step = eval_steps[eval_count]
 58: [rank58]: IndexError: list index out of range
 89: [rank89]: Traceback (most recent call last):
 89: [rank89]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 89: [rank89]:     args, final_loss, train_time_raw = main()
 89: [rank89]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 89: [rank89]:     next_eval_step = eval_steps[eval_count]
 89: [rank89]: IndexError: list index out of range
113: [rank113]: Traceback (most recent call last):
113: [rank113]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
113: [rank113]:     args, final_loss, train_time_raw = main()
113: [rank113]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
113: [rank113]:     next_eval_step = eval_steps[eval_count]
113: [rank113]: IndexError: list index out of range
 73: [rank73]: Traceback (most recent call last):
 73: [rank73]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 73: [rank73]:     args, final_loss, train_time_raw = main()
 73: [rank73]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 73: [rank73]:     next_eval_step = eval_steps[eval_count]
 73: [rank73]: IndexError: list index out of range
 49: [rank49]: Traceback (most recent call last):
 49: [rank49]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 49: [rank49]:     args, final_loss, train_time_raw = main()
 49: [rank49]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 49: [rank49]:     next_eval_step = eval_steps[eval_count]
 49: [rank49]: IndexError: list index out of range
 65: [rank65]: Traceback (most recent call last):
 65: [rank65]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 65: [rank65]:     args, final_loss, train_time_raw = main()
 65: [rank65]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 65: [rank65]:     next_eval_step = eval_steps[eval_count]
 65: [rank65]: IndexError: list index out of range
105: [rank105]: Traceback (most recent call last):
105: [rank105]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
105: [rank105]:     args, final_loss, train_time_raw = main()
105: [rank105]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
105: [rank105]:     next_eval_step = eval_steps[eval_count]
105: [rank105]: IndexError: list index out of range
 17: [rank17]: Traceback (most recent call last):
 17: [rank17]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 17: [rank17]:     args, final_loss, train_time_raw = main()
 17: [rank17]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 17: [rank17]:     next_eval_step = eval_steps[eval_count]
 17: [rank17]: IndexError: list index out of range
 81: [rank81]: Traceback (most recent call last):
 81: [rank81]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 81: [rank81]:     args, final_loss, train_time_raw = main()
 81: [rank81]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 81: [rank81]:     next_eval_step = eval_steps[eval_count]
 81: [rank81]: IndexError: list index out of range
 41: [rank41]: Traceback (most recent call last):
 41: [rank41]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 41: [rank41]:     args, final_loss, train_time_raw = main()
 41: [rank41]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 41: [rank41]:     next_eval_step = eval_steps[eval_count]
 41: [rank41]: IndexError: list index out of range
 10: [rank10]: Traceback (most recent call last):
 10: [rank10]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 10: [rank10]:     args, final_loss, train_time_raw = main()
 10: [rank10]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 10: [rank10]:     next_eval_step = eval_steps[eval_count]
 10: [rank10]: IndexError: list index out of range
 26: [rank26]: Traceback (most recent call last):
 26: [rank26]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 26: [rank26]:     args, final_loss, train_time_raw = main()
 26: [rank26]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 26: [rank26]:     next_eval_step = eval_steps[eval_count]
 26: [rank26]: IndexError: list index out of range
 29: [rank29]: Traceback (most recent call last):
 29: [rank29]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 29: [rank29]:     args, final_loss, train_time_raw = main()
 29: [rank29]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 29: [rank29]:     next_eval_step = eval_steps[eval_count]
 29: [rank29]: IndexError: list index out of range
 25: [rank25]: Traceback (most recent call last):
 25: [rank25]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 25: [rank25]:     args, final_loss, train_time_raw = main()
 25: [rank25]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 25: [rank25]:     next_eval_step = eval_steps[eval_count]
 25: [rank25]: IndexError: list index out of range
 24: [rank24]: Traceback (most recent call last):
 24: [rank24]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 24: [rank24]:     args, final_loss, train_time_raw = main()
 24: [rank24]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 24: [rank24]:     next_eval_step = eval_steps[eval_count]
 24: [rank24]: IndexError: list index out of range
 31: [rank31]: Traceback (most recent call last):
 31: [rank31]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 31: [rank31]:     args, final_loss, train_time_raw = main()
 31: [rank31]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 31: [rank31]:     next_eval_step = eval_steps[eval_count]
 31: [rank31]: IndexError: list index out of range
 30: [rank30]: Traceback (most recent call last):
 30: [rank30]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 30: [rank30]:     args, final_loss, train_time_raw = main()
 30: [rank30]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 30: [rank30]:     next_eval_step = eval_steps[eval_count]
 30: [rank30]: IndexError: list index out of range
 28: [rank28]: Traceback (most recent call last):
 28: [rank28]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 28: [rank28]:     args, final_loss, train_time_raw = main()
 28: [rank28]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 28: [rank28]:     next_eval_step = eval_steps[eval_count]
 28: [rank28]: IndexError: list index out of range
 57: [rank57]: Traceback (most recent call last):
 57: [rank57]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 57: [rank57]:     args, final_loss, train_time_raw = main()
 57: [rank57]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 57: [rank57]:     next_eval_step = eval_steps[eval_count]
 57: [rank57]: IndexError: list index out of range
101: [rank101]: Traceback (most recent call last):
101: [rank101]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
101: [rank101]:     args, final_loss, train_time_raw = main()
101: [rank101]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
101: [rank101]:     next_eval_step = eval_steps[eval_count]
101: [rank101]: IndexError: list index out of range
 59: [rank59]: Traceback (most recent call last):
 59: [rank59]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 59: [rank59]:     args, final_loss, train_time_raw = main()
 59: [rank59]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 59: [rank59]:     next_eval_step = eval_steps[eval_count]
 59: [rank59]: IndexError: list index out of range
102: [rank102]: Traceback (most recent call last):
102: [rank102]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
102: [rank102]:     args, final_loss, train_time_raw = main()
102: [rank102]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
102: [rank102]:     next_eval_step = eval_steps[eval_count]
102: [rank102]: IndexError: list index out of range
 60: [rank60]: Traceback (most recent call last):
 60: [rank60]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 60: [rank60]:     args, final_loss, train_time_raw = main()
 60: [rank60]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 60: [rank60]:     next_eval_step = eval_steps[eval_count]
 60: [rank60]: IndexError: list index out of range
 98: [rank98]: Traceback (most recent call last):
 98: [rank98]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 98: [rank98]:     args, final_loss, train_time_raw = main()
 98: [rank98]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 98: [rank98]:     next_eval_step = eval_steps[eval_count]
 98: [rank98]: IndexError: list index out of range
 62: [rank62]: Traceback (most recent call last):
 62: [rank62]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 62: [rank62]:     args, final_loss, train_time_raw = main()
 62: [rank62]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 62: [rank62]:     next_eval_step = eval_steps[eval_count]
 62: [rank62]: IndexError: list index out of range
100: [rank100]: Traceback (most recent call last):
100: [rank100]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
100: [rank100]:     args, final_loss, train_time_raw = main()
100: [rank100]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
100: [rank100]:     next_eval_step = eval_steps[eval_count]
100: [rank100]: IndexError: list index out of range
 63: [rank63]: Traceback (most recent call last):
 63: [rank63]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 63: [rank63]:     args, final_loss, train_time_raw = main()
 63: [rank63]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 63: [rank63]:     next_eval_step = eval_steps[eval_count]
 63: [rank63]: IndexError: list index out of range
 96: [rank96]: Traceback (most recent call last):
 96: [rank96]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 96: [rank96]:     args, final_loss, train_time_raw = main()
 96: [rank96]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 96: [rank96]:     next_eval_step = eval_steps[eval_count]
 96: [rank96]: IndexError: list index out of range
 56: [rank56]: Traceback (most recent call last):
 56: [rank56]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 56: [rank56]:     args, final_loss, train_time_raw = main()
 56: [rank56]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 56: [rank56]:     next_eval_step = eval_steps[eval_count]
 56: [rank56]: IndexError: list index out of range
103: [rank103]: Traceback (most recent call last):
103: [rank103]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
103: [rank103]:     args, final_loss, train_time_raw = main()
103: [rank103]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
103: [rank103]:     next_eval_step = eval_steps[eval_count]
103: [rank103]: IndexError: list index out of range
 61: [rank61]: Traceback (most recent call last):
 61: [rank61]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 61: [rank61]:     args, final_loss, train_time_raw = main()
 61: [rank61]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 61: [rank61]:     next_eval_step = eval_steps[eval_count]
 61: [rank61]: IndexError: list index out of range
 99: [rank99]: Traceback (most recent call last):
 99: [rank99]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 99: [rank99]:     args, final_loss, train_time_raw = main()
 99: [rank99]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 99: [rank99]:     next_eval_step = eval_steps[eval_count]
 99: [rank99]: IndexError: list index out of range
 90: [rank90]: Traceback (most recent call last):
 90: [rank90]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 90: [rank90]:     args, final_loss, train_time_raw = main()
 90: [rank90]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 90: [rank90]:     next_eval_step = eval_steps[eval_count]
 90: [rank90]: IndexError: list index out of range
 94: [rank94]: Traceback (most recent call last):
 94: [rank94]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 94: [rank94]:     args, final_loss, train_time_raw = main()
 94: [rank94]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 94: [rank94]:     next_eval_step = eval_steps[eval_count]
 94: [rank94]: IndexError: list index out of range
 92: [rank92]: Traceback (most recent call last):
 92: [rank92]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 92: [rank92]:     args, final_loss, train_time_raw = main()
 92: [rank92]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 92: [rank92]:     next_eval_step = eval_steps[eval_count]
 92: [rank92]: IndexError: list index out of range
 93: [rank93]: Traceback (most recent call last):
 93: [rank93]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 93: [rank93]:     args, final_loss, train_time_raw = main()
 93: [rank93]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 93: [rank93]:     next_eval_step = eval_steps[eval_count]
 93: [rank93]: IndexError: list index out of range
 91: [rank91]: Traceback (most recent call last):
 91: [rank91]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 91: [rank91]:     args, final_loss, train_time_raw = main()
 91: [rank91]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 91: [rank91]:     next_eval_step = eval_steps[eval_count]
 91: [rank91]: IndexError: list index out of range
 95: [rank95]: Traceback (most recent call last):
 95: [rank95]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 95: [rank95]:     args, final_loss, train_time_raw = main()
 95: [rank95]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 95: [rank95]:     next_eval_step = eval_steps[eval_count]
 95: [rank95]: IndexError: list index out of range
 88: [rank88]: Traceback (most recent call last):
 88: [rank88]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 88: [rank88]:     args, final_loss, train_time_raw = main()
 88: [rank88]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 88: [rank88]:     next_eval_step = eval_steps[eval_count]
 88: [rank88]: IndexError: list index out of range
 36: [rank36]: Traceback (most recent call last):
 36: [rank36]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 36: [rank36]:     args, final_loss, train_time_raw = main()
 36: [rank36]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 36: [rank36]:     next_eval_step = eval_steps[eval_count]
 36: [rank36]: IndexError: list index out of range
 33: [rank33]: Traceback (most recent call last):
 33: [rank33]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 33: [rank33]:     args, final_loss, train_time_raw = main()
 33: [rank33]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 33: [rank33]:     next_eval_step = eval_steps[eval_count]
 33: [rank33]: IndexError: list index out of range
 38: [rank38]: Traceback (most recent call last):
 38: [rank38]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 38: [rank38]:     args, final_loss, train_time_raw = main()
 38: [rank38]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 38: [rank38]:     next_eval_step = eval_steps[eval_count]
 38: [rank38]: IndexError: list index out of range
 34: [rank34]: Traceback (most recent call last):
 34: [rank34]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 34: [rank34]:     args, final_loss, train_time_raw = main()
 34: [rank34]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 34: [rank34]:     next_eval_step = eval_steps[eval_count]
 34: [rank34]: IndexError: list index out of range
 39: [rank39]: Traceback (most recent call last):
 39: [rank39]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 39: [rank39]:     args, final_loss, train_time_raw = main()
 39: [rank39]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 39: [rank39]:     next_eval_step = eval_steps[eval_count]
 39: [rank39]: IndexError: list index out of range
 32: [rank32]: Traceback (most recent call last):
 32: [rank32]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 32: [rank32]:     args, final_loss, train_time_raw = main()
 32: [rank32]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 32: [rank32]:     next_eval_step = eval_steps[eval_count]
 32: [rank32]: IndexError: list index out of range
 66: [rank66]: Traceback (most recent call last):
 66: [rank66]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 66: [rank66]:     args, final_loss, train_time_raw = main()
 66: [rank66]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 66: [rank66]:     next_eval_step = eval_steps[eval_count]
 66: [rank66]: IndexError: list index out of range
 37: [rank37]: Traceback (most recent call last):
 37: [rank37]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 37: [rank37]:     args, final_loss, train_time_raw = main()
 37: [rank37]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 37: [rank37]:     next_eval_step = eval_steps[eval_count]
 37: [rank37]: IndexError: list index out of range
 67: [rank67]: Traceback (most recent call last):
 67: [rank67]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 67: [rank67]:     args, final_loss, train_time_raw = main()
 67: [rank67]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 67: [rank67]:     next_eval_step = eval_steps[eval_count]
 67: [rank67]: IndexError: list index out of range
 42: [rank42]: Traceback (most recent call last):
 42: [rank42]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 42: [rank42]:     args, final_loss, train_time_raw = main()
 42: [rank42]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 42: [rank42]:     next_eval_step = eval_steps[eval_count]
 42: [rank42]: IndexError: list index out of range
 68: [rank68]: Traceback (most recent call last):
 68: [rank68]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 68: [rank68]:     args, final_loss, train_time_raw = main()
 68: [rank68]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 68: [rank68]:     next_eval_step = eval_steps[eval_count]
 68: [rank68]: IndexError: list index out of range
 44: [rank44]: Traceback (most recent call last):
 44: [rank44]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 44: [rank44]:     args, final_loss, train_time_raw = main()
 44: [rank44]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 44: [rank44]:     next_eval_step = eval_steps[eval_count]
 44: [rank44]: IndexError: list index out of range
 69: [rank69]: Traceback (most recent call last):
 69: [rank69]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 69: [rank69]:     args, final_loss, train_time_raw = main()
 69: [rank69]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 69: [rank69]:     next_eval_step = eval_steps[eval_count]
 69: [rank69]: IndexError: list index out of range
 20: [rank20]: Traceback (most recent call last):
 20: [rank20]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 20: [rank20]:     args, final_loss, train_time_raw = main()
 20: [rank20]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 20: [rank20]:     next_eval_step = eval_steps[eval_count]
 20: [rank20]: IndexError: list index out of range
 46: [rank46]: Traceback (most recent call last):
 46: [rank46]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 46: [rank46]:     args, final_loss, train_time_raw = main()
 46: [rank46]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 46: [rank46]:     next_eval_step = eval_steps[eval_count]
 46: [rank46]: IndexError: list index out of range
 64: [rank64]: Traceback (most recent call last):
 64: [rank64]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 64: [rank64]:     args, final_loss, train_time_raw = main()
 64: [rank64]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 64: [rank64]:     next_eval_step = eval_steps[eval_count]
 64: [rank64]: IndexError: list index out of range
108: [rank108]: Traceback (most recent call last):
108: [rank108]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
108: [rank108]:     args, final_loss, train_time_raw = main()
108: [rank108]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
108: [rank108]:     next_eval_step = eval_steps[eval_count]
108: [rank108]: IndexError: list index out of range
 18: [rank18]: Traceback (most recent call last):
 18: [rank18]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 18: [rank18]:     args, final_loss, train_time_raw = main()
 18: [rank18]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 18: [rank18]:     next_eval_step = eval_steps[eval_count]
 18: [rank18]: IndexError: list index out of range
 45: [rank45]: Traceback (most recent call last):
 45: [rank45]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 45: [rank45]:     args, final_loss, train_time_raw = main()
 45: [rank45]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 45: [rank45]:     next_eval_step = eval_steps[eval_count]
 45: [rank45]: IndexError: list index out of range
116: [rank116]: Traceback (most recent call last):
116: [rank116]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
116: [rank116]:     args, final_loss, train_time_raw = main()
116: [rank116]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
116: [rank116]:     next_eval_step = eval_steps[eval_count]
116: [rank116]: IndexError: list index out of range
 71: [rank71]: Traceback (most recent call last):
 71: [rank71]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 71: [rank71]:     args, final_loss, train_time_raw = main()
 71: [rank71]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 71: [rank71]:     next_eval_step = eval_steps[eval_count]
 71: [rank71]: IndexError: list index out of range
106: [rank106]: Traceback (most recent call last):
106: [rank106]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
106: [rank106]:     args, final_loss, train_time_raw = main()
106: [rank106]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
106: [rank106]:     next_eval_step = eval_steps[eval_count]
106: [rank106]: IndexError: list index out of range
 19: [rank19]: Traceback (most recent call last):
 19: [rank19]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 19: [rank19]:     args, final_loss, train_time_raw = main()
 19: [rank19]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 19: [rank19]:     next_eval_step = eval_steps[eval_count]
 19: [rank19]: IndexError: list index out of range
 43: [rank43]: Traceback (most recent call last):
 43: [rank43]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 43: [rank43]:     args, final_loss, train_time_raw = main()
 43: [rank43]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 43: [rank43]:     next_eval_step = eval_steps[eval_count]
 43: [rank43]: IndexError: list index out of range
114: [rank114]: Traceback (most recent call last):
114: [rank114]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
114: [rank114]:     args, final_loss, train_time_raw = main()
114: [rank114]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
114: [rank114]:     next_eval_step = eval_steps[eval_count]
114: [rank114]: IndexError: list index out of range
 70: [rank70]: Traceback (most recent call last):
 70: [rank70]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 70: [rank70]:     args, final_loss, train_time_raw = main()
 70: [rank70]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 70: [rank70]:     next_eval_step = eval_steps[eval_count]
 70: [rank70]: IndexError: list index out of range
109: [rank109]: Traceback (most recent call last):
109: [rank109]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
109: [rank109]:     args, final_loss, train_time_raw = main()
109: [rank109]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
109: [rank109]:     next_eval_step = eval_steps[eval_count]
109: [rank109]: IndexError: list index out of range
 21: [rank21]: Traceback (most recent call last):
 21: [rank21]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 21: [rank21]:     args, final_loss, train_time_raw = main()
 21: [rank21]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 21: [rank21]:     next_eval_step = eval_steps[eval_count]
 21: [rank21]: IndexError: list index out of range
 40: [rank40]: Traceback (most recent call last):
 40: [rank40]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 40: [rank40]:     args, final_loss, train_time_raw = main()
 40: [rank40]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 40: [rank40]:     next_eval_step = eval_steps[eval_count]
 40: [rank40]: IndexError: list index out of range
118: [rank118]: Traceback (most recent call last):
118: [rank118]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
118: [rank118]:     args, final_loss, train_time_raw = main()
118: [rank118]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
118: [rank118]:     next_eval_step = eval_steps[eval_count]
118: [rank118]: IndexError: list index out of range
107: [rank107]: Traceback (most recent call last):
107: [rank107]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
107: [rank107]:     args, final_loss, train_time_raw = main()
107: [rank107]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
107: [rank107]:     next_eval_step = eval_steps[eval_count]
107: [rank107]: IndexError: list index out of range
 16: [rank16]: Traceback (most recent call last):
 16: [rank16]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 16: [rank16]:     args, final_loss, train_time_raw = main()
 16: [rank16]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 16: [rank16]:     next_eval_step = eval_steps[eval_count]
 16: [rank16]: IndexError: list index out of range
 47: [rank47]: Traceback (most recent call last):
 47: [rank47]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 47: [rank47]:     args, final_loss, train_time_raw = main()
 47: [rank47]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 47: [rank47]:     next_eval_step = eval_steps[eval_count]
 47: [rank47]: IndexError: list index out of range
115: [rank115]: Traceback (most recent call last):
115: [rank115]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
115: [rank115]:     args, final_loss, train_time_raw = main()
115: [rank115]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
115: [rank115]:     next_eval_step = eval_steps[eval_count]
115: [rank115]: IndexError: list index out of range
111: [rank111]: Traceback (most recent call last):
111: [rank111]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
111: [rank111]:     args, final_loss, train_time_raw = main()
111: [rank111]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
111: [rank111]:     next_eval_step = eval_steps[eval_count]
111: [rank111]: IndexError: list index out of range
 22: [rank22]: Traceback (most recent call last):
 22: [rank22]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 22: [rank22]:     args, final_loss, train_time_raw = main()
 22: [rank22]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 22: [rank22]:     next_eval_step = eval_steps[eval_count]
 22: [rank22]: IndexError: list index out of range
119: [rank119]: Traceback (most recent call last):
119: [rank119]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
119: [rank119]:     args, final_loss, train_time_raw = main()
119: [rank119]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
119: [rank119]:     next_eval_step = eval_steps[eval_count]
119: [rank119]: IndexError: list index out of range
104: [rank104]: Traceback (most recent call last):
104: [rank104]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
104: [rank104]:     args, final_loss, train_time_raw = main()
104: [rank104]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
104: [rank104]:     next_eval_step = eval_steps[eval_count]
104: [rank104]: IndexError: list index out of range
 23: [rank23]: Traceback (most recent call last):
 23: [rank23]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 23: [rank23]:     args, final_loss, train_time_raw = main()
 23: [rank23]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 23: [rank23]:     next_eval_step = eval_steps[eval_count]
 23: [rank23]: IndexError: list index out of range
112: [rank112]: Traceback (most recent call last):
112: [rank112]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
112: [rank112]:     args, final_loss, train_time_raw = main()
112: [rank112]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
112: [rank112]:     next_eval_step = eval_steps[eval_count]
112: [rank112]: IndexError: list index out of range
110: [rank110]: Traceback (most recent call last):
110: [rank110]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
110: [rank110]:     args, final_loss, train_time_raw = main()
110: [rank110]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
110: [rank110]:     next_eval_step = eval_steps[eval_count]
110: [rank110]: IndexError: list index out of range
117: [rank117]: Traceback (most recent call last):
117: [rank117]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
117: [rank117]:     args, final_loss, train_time_raw = main()
117: [rank117]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
117: [rank117]:     next_eval_step = eval_steps[eval_count]
117: [rank117]: IndexError: list index out of range
 51: [rank51]: Traceback (most recent call last):
 51: [rank51]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 51: [rank51]:     args, final_loss, train_time_raw = main()
 51: [rank51]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 51: [rank51]:     next_eval_step = eval_steps[eval_count]
 51: [rank51]: IndexError: list index out of range
 52: [rank52]: Traceback (most recent call last):
 52: [rank52]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 52: [rank52]:     args, final_loss, train_time_raw = main()
 52: [rank52]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 52: [rank52]:     next_eval_step = eval_steps[eval_count]
 52: [rank52]: IndexError: list index out of range
 11: [rank11]: Traceback (most recent call last):
 11: [rank11]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 11: [rank11]:     args, final_loss, train_time_raw = main()
 11: [rank11]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 11: [rank11]:     next_eval_step = eval_steps[eval_count]
 11: [rank11]: IndexError: list index out of range
 50: [rank50]: Traceback (most recent call last):
 50: [rank50]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 50: [rank50]:     args, final_loss, train_time_raw = main()
 50: [rank50]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 50: [rank50]:     next_eval_step = eval_steps[eval_count]
 50: [rank50]: IndexError: list index out of range
  9: [rank9]: Traceback (most recent call last):
  9: [rank9]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  9: [rank9]:     args, final_loss, train_time_raw = main()
  9: [rank9]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  9: [rank9]:     next_eval_step = eval_steps[eval_count]
  9: [rank9]: IndexError: list index out of range
 75: [rank75]: Traceback (most recent call last):
 75: [rank75]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 75: [rank75]:     args, final_loss, train_time_raw = main()
 75: [rank75]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 75: [rank75]:     next_eval_step = eval_steps[eval_count]
 75: [rank75]: IndexError: list index out of range
 55: [rank55]: Traceback (most recent call last):
 55: [rank55]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 55: [rank55]:     args, final_loss, train_time_raw = main()
 55: [rank55]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 55: [rank55]:     next_eval_step = eval_steps[eval_count]
 55: [rank55]: IndexError: list index out of range
 13: [rank13]: Traceback (most recent call last):
 13: [rank13]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 13: [rank13]:     args, final_loss, train_time_raw = main()
 13: [rank13]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 13: [rank13]:     next_eval_step = eval_steps[eval_count]
 13: [rank13]: IndexError: list index out of range
 74: [rank74]: Traceback (most recent call last):
 74: [rank74]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 74: [rank74]:     args, final_loss, train_time_raw = main()
 74: [rank74]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 74: [rank74]:     next_eval_step = eval_steps[eval_count]
 74: [rank74]: IndexError: list index out of range
 53: [rank53]: Traceback (most recent call last):
 53: [rank53]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 53: [rank53]:     args, final_loss, train_time_raw = main()
 53: [rank53]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 53: [rank53]:     next_eval_step = eval_steps[eval_count]
 53: [rank53]: IndexError: list index out of range
 15: [rank15]: Traceback (most recent call last):
 15: [rank15]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 15: [rank15]:     args, final_loss, train_time_raw = main()
 15: [rank15]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 15: [rank15]:     next_eval_step = eval_steps[eval_count]
 15: [rank15]: IndexError: list index out of range
 76: [rank76]: Traceback (most recent call last):
 76: [rank76]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 76: [rank76]:     args, final_loss, train_time_raw = main()
 76: [rank76]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 76: [rank76]:     next_eval_step = eval_steps[eval_count]
 76: [rank76]: IndexError: list index out of range
 48: [rank48]: Traceback (most recent call last):
 48: [rank48]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 48: [rank48]:     args, final_loss, train_time_raw = main()
 48: [rank48]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 48: [rank48]:     next_eval_step = eval_steps[eval_count]
 48: [rank48]: IndexError: list index out of range
 12: [rank12]: Traceback (most recent call last):
 12: [rank12]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 12: [rank12]:     args, final_loss, train_time_raw = main()
 12: [rank12]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 12: [rank12]:     next_eval_step = eval_steps[eval_count]
 12: [rank12]: IndexError: list index out of range
 77: [rank77]: Traceback (most recent call last):
 77: [rank77]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 77: [rank77]:     args, final_loss, train_time_raw = main()
 77: [rank77]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 77: [rank77]:     next_eval_step = eval_steps[eval_count]
 77: [rank77]: IndexError: list index out of range
 54: [rank54]: Traceback (most recent call last):
 54: [rank54]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 54: [rank54]:     args, final_loss, train_time_raw = main()
 54: [rank54]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 54: [rank54]:     next_eval_step = eval_steps[eval_count]
 54: [rank54]: IndexError: list index out of range
 14: [rank14]: Traceback (most recent call last):
 14: [rank14]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 14: [rank14]:     args, final_loss, train_time_raw = main()
 14: [rank14]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 14: [rank14]:     next_eval_step = eval_steps[eval_count]
 14: [rank14]: IndexError: list index out of range
 78: [rank78]: Traceback (most recent call last):
 78: [rank78]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 78: [rank78]:     args, final_loss, train_time_raw = main()
 78: [rank78]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 78: [rank78]:     next_eval_step = eval_steps[eval_count]
 78: [rank78]: IndexError: list index out of range
  8: [rank8]: Traceback (most recent call last):
  8: [rank8]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
  8: [rank8]:     args, final_loss, train_time_raw = main()
  8: [rank8]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
  8: [rank8]:     next_eval_step = eval_steps[eval_count]
  8: [rank8]: IndexError: list index out of range
 72: [rank72]: Traceback (most recent call last):
 72: [rank72]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 72: [rank72]:     args, final_loss, train_time_raw = main()
 72: [rank72]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 72: [rank72]:     next_eval_step = eval_steps[eval_count]
 72: [rank72]: IndexError: list index out of range
 79: [rank79]: Traceback (most recent call last):
 79: [rank79]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 79: [rank79]:     args, final_loss, train_time_raw = main()
 79: [rank79]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 79: [rank79]:     next_eval_step = eval_steps[eval_count]
 79: [rank79]: IndexError: list index out of range
122: [rank122]: Traceback (most recent call last):
122: [rank122]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
122: [rank122]:     args, final_loss, train_time_raw = main()
122: [rank122]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
122: [rank122]:     next_eval_step = eval_steps[eval_count]
122: [rank122]: IndexError: list index out of range
123: [rank123]: Traceback (most recent call last):
123: [rank123]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
123: [rank123]:     args, final_loss, train_time_raw = main()
123: [rank123]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
123: [rank123]:     next_eval_step = eval_steps[eval_count]
123: [rank123]: IndexError: list index out of range
124: [rank124]: Traceback (most recent call last):
124: [rank124]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
124: [rank124]:     args, final_loss, train_time_raw = main()
124: [rank124]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
124: [rank124]:     next_eval_step = eval_steps[eval_count]
124: [rank124]: IndexError: list index out of range
125: [rank125]: Traceback (most recent call last):
125: [rank125]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
125: [rank125]:     args, final_loss, train_time_raw = main()
125: [rank125]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
125: [rank125]:     next_eval_step = eval_steps[eval_count]
125: [rank125]: IndexError: list index out of range
126: [rank126]: Traceback (most recent call last):
126: [rank126]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
126: [rank126]:     args, final_loss, train_time_raw = main()
126: [rank126]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
126: [rank126]:     next_eval_step = eval_steps[eval_count]
126: [rank126]: IndexError: list index out of range
120: [rank120]: Traceback (most recent call last):
120: [rank120]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
120: [rank120]:     args, final_loss, train_time_raw = main()
120: [rank120]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
120: [rank120]:     next_eval_step = eval_steps[eval_count]
120: [rank120]: IndexError: list index out of range
127: [rank127]: Traceback (most recent call last):
127: [rank127]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
127: [rank127]:     args, final_loss, train_time_raw = main()
127: [rank127]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
127: [rank127]:     next_eval_step = eval_steps[eval_count]
127: [rank127]: IndexError: list index out of range
 82: [rank82]: Traceback (most recent call last):
 82: [rank82]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 82: [rank82]:     args, final_loss, train_time_raw = main()
 82: [rank82]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 82: [rank82]:     next_eval_step = eval_steps[eval_count]
 82: [rank82]: IndexError: list index out of range
 83: [rank83]: Traceback (most recent call last):
 83: [rank83]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 83: [rank83]:     args, final_loss, train_time_raw = main()
 83: [rank83]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 83: [rank83]:     next_eval_step = eval_steps[eval_count]
 83: [rank83]: IndexError: list index out of range
 84: [rank84]: Traceback (most recent call last):
 84: [rank84]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 84: [rank84]:     args, final_loss, train_time_raw = main()
 84: [rank84]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 84: [rank84]:     next_eval_step = eval_steps[eval_count]
 84: [rank84]: IndexError: list index out of range
 86: [rank86]: Traceback (most recent call last):
 86: [rank86]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 86: [rank86]:     args, final_loss, train_time_raw = main()
 86: [rank86]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 86: [rank86]:     next_eval_step = eval_steps[eval_count]
 86: [rank86]: IndexError: list index out of range
 85: [rank85]: Traceback (most recent call last):
 85: [rank85]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 85: [rank85]:     args, final_loss, train_time_raw = main()
 85: [rank85]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 85: [rank85]:     next_eval_step = eval_steps[eval_count]
 85: [rank85]: IndexError: list index out of range
 87: [rank87]: Traceback (most recent call last):
 87: [rank87]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 87: [rank87]:     args, final_loss, train_time_raw = main()
 87: [rank87]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 87: [rank87]:     next_eval_step = eval_steps[eval_count]
 87: [rank87]: IndexError: list index out of range
 80: [rank80]: Traceback (most recent call last):
 80: [rank80]:   File "/workspace/bert/run_pretraining.py", line 2053, in <module>
 80: [rank80]:     args, final_loss, train_time_raw = main()
 80: [rank80]:   File "/workspace/bert/run_pretraining.py", line 1879, in main
 80: [rank80]:     next_eval_step = eval_steps[eval_count]
 80: [rank80]: IndexError: list index out of range
srun: error: calvin-training-node-028: task 96: Exited with exit code 1
srun: error: calvin-training-node-024: task 64: Exited with exit code 1
srun: error: calvin-training-node-026: task 80: Exited with exit code 1
srun: error: calvin-training-node-021: task 40: Exited with exit code 1
srun: error: calvin-training-node-018: task 16: Exited with exit code 1
srun: error: calvin-training-node-016: task 0: Exited with exit code 1
srun: error: calvin-training-node-023: task 56: Exited with exit code 1
srun: error: calvin-training-node-017: tasks 8,11,15: Exited with exit code 1
srun: error: calvin-training-node-020: tasks 32,34-35,37: Exited with exit code 1
srun: error: calvin-training-node-031: tasks 120,122,124-125,127: Exited with exit code 1
srun: error: calvin-training-node-025: tasks 72,74-75,77-78: Exited with exit code 1
srun: error: calvin-training-node-029: tasks 104,107-108,110-111: Exited with exit code 1
srun: error: calvin-training-node-022: tasks 48-53: Exited with exit code 1
srun: error: calvin-training-node-023: tasks 57-59: Exited with exit code 1
srun: error: calvin-training-node-030: tasks 112-116,118-119: Exited with exit code 1
srun: error: calvin-training-node-031: task 121: Exited with exit code 1
srun: error: calvin-training-node-027: tasks 88-91,93-95: Exited with exit code 1
srun: error: calvin-training-node-029: task 109: Exited with exit code 1
srun: error: calvin-training-node-031: task 123: Exited with exit code 1
srun: error: calvin-training-node-022: task 55: Exited with exit code 1
srun: error: calvin-training-node-019: tasks 24-28,30-31: Exited with exit code 1
srun: error: calvin-training-node-025: task 73: Exited with exit code 1
srun: error: calvin-training-node-030: task 117: Exited with exit code 1
srun: error: calvin-training-node-021: tasks 41-47: Exited with exit code 1
srun: error: calvin-training-node-026: tasks 81-87: Exited with exit code 1
srun: error: calvin-training-node-027: task 92: Exited with exit code 1
srun: error: calvin-training-node-023: task 63: Exited with exit code 1
srun: error: calvin-training-node-031: task 126: Exited with exit code 1
srun: error: calvin-training-node-024: tasks 65-69,71: Exited with exit code 1
srun: error: calvin-training-node-020: tasks 33,38-39: Exited with exit code 1
srun: error: calvin-training-node-018: tasks 17-23: Exited with exit code 1
srun: error: calvin-training-node-028: tasks 97-103: Exited with exit code 1
srun: error: calvin-training-node-025: task 79: Exited with exit code 1
srun: error: calvin-training-node-016: tasks 1-7: Exited with exit code 1
srun: error: calvin-training-node-029: task 105: Exited with exit code 1
srun: error: calvin-training-node-022: task 54: Exited with exit code 1
srun: error: calvin-training-node-019: task 29: Exited with exit code 1
srun: error: calvin-training-node-025: task 76: Exited with exit code 1
srun: error: calvin-training-node-024: task 70: Exited with exit code 1
srun: error: calvin-training-node-020: task 36: Exited with exit code 1
srun: error: calvin-training-node-029: task 106: Exited with exit code 1
srun: error: calvin-training-node-017: tasks 9-10,12-14: Exited with exit code 1
srun: error: calvin-training-node-023: tasks 60-62: Exited with exit code 1
