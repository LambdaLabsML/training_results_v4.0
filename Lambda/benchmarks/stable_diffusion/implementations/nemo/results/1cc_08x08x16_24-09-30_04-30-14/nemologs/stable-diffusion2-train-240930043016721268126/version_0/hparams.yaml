cfg:
  precision: 16
  micro_batch_size: 16
  global_batch_size: 1024
  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 200
  timesteps: 1000
  first_stage_key: images_moments
  cond_stage_key: clip_encoded
  image_size: 64
  channels: 4
  cond_stage_trainable: false
  conditioning_key: crossattn
  monitor: val/loss_simple_ema
  scale_factor: 0.18215
  use_ema: false
  scale_by_std: false
  ckpt_path: /checkpoints/sd/512-base-ema.ckpt
  load_vae: true
  load_unet: false
  load_encoder: true
  ignore_keys: []
  parameterization: v
  clip_denoised: true
  load_only_unet: false
  cosine_s: 0.008
  given_betas: null
  original_elbo_weight: 0
  v_posterior: 0
  l_simple_weight: 1
  use_positional_encodings: false
  learn_logvar: false
  logvar_init: 0
  beta_schedule: linear
  loss_type: l2
  channels_last: true
  concat_mode: true
  cond_stage_forward: null
  text_embedding_dropout_rate: 0.0
  fused_opt: true
  inductor: true
  inductor_cudagraphs: false
  capture_cudagraph_iters: 15
  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    from_pretrained: null
    from_NeMo: null
    image_size: 32
    in_channels: 4
    out_channels: 4
    model_channels: 320
    attention_resolutions:
    - 4
    - 2
    - 1
    num_res_blocks: 2
    channel_mult:
    - 1
    - 2
    - 4
    - 4
    num_head_channels: 64
    use_spatial_transformer: true
    use_linear_in_transformer: true
    transformer_depth: 1
    context_dim: 1024
    use_checkpoint: false
    legacy: false
    use_flash_attention: true
    resblock_gn_groups: 16
    unet_precision: fp16
    timesteps: 1000
  first_stage_config:
    _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
    from_pretrained: null
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      double_z: true
      z_channels: 4
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity
  cond_stage_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder
    arch: ViT-H-14
    version: laion2b_s32b_b79k
    freeze: true
    layer: penultimate
    cache_dir: /checkpoints/clip
  seed: 30346
  resume_from_checkpoint: null
  apex_transformer_log_level: 30
  gradient_as_bucket_view: true
  ddp_overlap: false
  nsys_profile:
    enabled: false
    start_step: 10
    end_step: 10
    ranks:
    - 0
    gen_shape: false
  data:
    num_workers: 16
    train:
      dataset_path: /datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar
      augmentations:
        resize_smallest_side: 512
        center_crop_h_w: 512, 512
        horizontal_flip: false
      filterings: null
    webdataset:
      infinite_sampler: true
      local_root_path: /datasets/laion-400m/webdataset-moments-filtered-encoded
  optim:
    name: distributed_fused_adam
    lr: 0.00012288
    weight_decay: 0.0
    betas:
    - 0.9
    - 0.999
    sched:
      name: WarmupHoldPolicy
      warmup_steps: 1000
      hold_steps: 10000000000000
    bucket_cap_mb: 288
    overlap_grad_sync: true
    overlap_param_sync: false
    contiguous_grad_buffer: true
    contiguous_param_buffer: true
    store_params: true
    dtype: torch.float32
    grad_sync_dtype: torch.float16
    param_sync_dtype: torch.float16
    capturable: true
    distribute_within_nodes: true
