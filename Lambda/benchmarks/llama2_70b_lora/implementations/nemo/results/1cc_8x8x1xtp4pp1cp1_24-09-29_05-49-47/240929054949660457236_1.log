+ echo 'Beginning trial 1 of 10'
Beginning trial 1 of 10
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-llama2_70b_lora:latest 135 8 calvin-training-node-[011,013-019] lambda_1cc 1cc_8x8x1xtp4pp1cp1'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-llama2_70b_lora:latest 135 8 calvin-training-node-[011,013-019] lambda_1cc 1cc_8x8x1xtp4pp1cp1
++ srun --ntasks=1 --container-name=llama2_70b_lora_135 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 8 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=8 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on calvin-training-node-015
Clearing cache on calvin-training-node-018
Clearing cache on calvin-training-node-013
Clearing cache on calvin-training-node-016
Clearing cache on calvin-training-node-017
Clearing cache on calvin-training-node-019
Clearing cache on calvin-training-node-014
Clearing cache on calvin-training-node-011
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=9236
+ SEED=9236
+ srun -l --kill-on-bad-exit=0 --mpi=pmix --ntasks=64 --ntasks-per-node=8 --container-name=llama2_70b_lora_135 --container-mounts=/home/ubuntu/ml-1cc/data/mlperf/llama2_70b_lora/data:/data:ro,/home/ubuntu/ml-1cc/data/mlperf/llama2_70b_lora/ckpt:/ckpt:ro,./results/1cc_8x8x1xtp4pp1cp1_24-09-29_05-49-47:/results:rw,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --export=ALL,MASTER_PORT=29500,MASTER_ADDR=calvin-training-node-011 slurm2pytorch ./run_and_time.sh
 0: STARTING TIMING RUN AT 2024-09-29 05:52:49 AM
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
29: 0it [00:00, ?it/s]0it [00:00, ?it/s]
 9: FlashAttention Installed
 8: FlashAttention Installed
15: FlashAttention Installed
12: FlashAttention Installed
13: FlashAttention Installed
10: FlashAttention Installed
11: FlashAttention Installed
14: FlashAttention Installed
60: FlashAttention Installed
57: FlashAttention Installed
58: FlashAttention Installed
61: FlashAttention Installed
59: FlashAttention Installed
40: FlashAttention Installed
43: FlashAttention Installed
46: FlashAttention Installed
47: FlashAttention Installed
42: FlashAttention Installed
41: FlashAttention Installed
56: FlashAttention Installed
62: FlashAttention Installed
44: FlashAttention Installed
45: FlashAttention Installed
63: FlashAttention Installed
28: FlashAttention Installed
30: FlashAttention Installed
31: FlashAttention Installed
25: FlashAttention Installed
26: FlashAttention Installed
24: FlashAttention Installed
29: FlashAttention Installed
27: FlashAttention Installed
51: FlashAttention Installed
52: FlashAttention Installed
54: FlashAttention Installed
49: FlashAttention Installed
48: FlashAttention Installed
55: FlashAttention Installed
50: FlashAttention Installed
53: FlashAttention Installed
32: FlashAttention Installed
37: FlashAttention Installed
39: FlashAttention Installed
36: FlashAttention Installed
33: FlashAttention Installed
34: FlashAttention Installed
35: FlashAttention Installed
38: FlashAttention Installed
 0: FlashAttention Installed
 5: FlashAttention Installed
 6: FlashAttention Installed
 1: FlashAttention Installed
 4: FlashAttention Installed
 2: FlashAttention Installed
 3: FlashAttention Installed
 7: FlashAttention Installed
16: FlashAttention Installed
17: FlashAttention Installed
20: FlashAttention Installed
22: FlashAttention Installed
21: FlashAttention Installed
19: FlashAttention Installed
23: FlashAttention Installed
18: FlashAttention Installed
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
60: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
59: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
61: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
58: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
57: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
41: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
56: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
42: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
47: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
46: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
62: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
63: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
45: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
44: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
40: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
43: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-29 05:53:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'megatron_gpt_peft_tuning_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
 0:       warnings.warn(msg, UserWarning)
 0:     
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
35: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
34: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
33: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
39: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
37: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
36: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
52: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
54: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
48: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
49: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
53: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
38: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
32: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
50: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
51: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
 0: [NeMo W 2024-09-29 05:53:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
55: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
 0: [NeMo I 2024-09-29 05:53:09 train:59] 
 0:     
 0:     ************** Experiment configuration ***********
 0: [NeMo I 2024-09-29 05:53:09 train:60] 
 0:     model:
 0:       ub_tp_comm_overlap_cfg:
 0:         qkv_fprop:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         fc1_fprop:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         proj_dgrad:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         fc2_dgrad:
 0:           method: ring_exchange
 0:           aggregate: 0
 0:         proj_fprop:
 0:           method: pipeline
 0:           num_sm: 32
 0:           cga_size: 2
 0:           num_splits: 4
 0:           set_sm_margin: 1
 0:           atomic_gemm: 1
 0:         fc2_fprop:
 0:           method: pipeline
 0:           num_sm: 16
 0:           cga_size: 2
 0:           num_splits: 4
 0:           set_sm_margin: 1
 0:           atomic_gemm: 1
 0:         qkv_dgrad:
 0:           method: bulk
 0:           num_sm: 4
 0:           cga_size: 2
 0:           set_sm_margin: 0
 0:         fc1_dgrad:
 0:           method: bulk
 0:           num_sm: 2
 0:           cga_size: 2
 0:           set_sm_margin: 0
 0:       mcore_gpt: true
 0:       seed: 9236
 0:       tensor_model_parallel_size: 4
 0:       pipeline_model_parallel_size: 1
 0:       context_parallel_size: 1
 0:       cpu_offloading: false
 0:       global_batch_size: 16
 0:       micro_batch_size: 1
 0:       max_position_embeddings: 8192
 0:       encoder_seq_length: 8192
 0:       restore_from_path: /ckpt
 0:       resume_from_checkpoint: null
 0:       save_nemo_on_validation_end: false
 0:       sync_batch_comm: false
 0:       megatron_amp_O2: true
 0:       sequence_parallel: 1
 0:       activations_checkpoint_granularity: null
 0:       activations_checkpoint_method: null
 0:       activations_checkpoint_num_layers: null
 0:       activations_checkpoint_layers_per_pipeline: null
 0:       answer_only_loss: true
 0:       gradient_as_bucket_view: false
 0:       hidden_dropout: 0.0
 0:       attention_dropout: 0.0
 0:       ffn_dropout: 0.0
 0:       bias_activation_fusion: true
 0:       bias_dropout_add_fusion: false
 0:       transformer_engine: true
 0:       fp8: true
 0:       fp8_params: true
 0:       fp8_hybrid: true
 0:       fp8_amax_history_len: 32
 0:       fp8_amax_compute_algo: max
 0:       reduce_amax: true
 0:       fp8_e4m3: false
 0:       fp8_interval: 1
 0:       fp8_margin: 0
 0:       fp8_dot_product_attention: 1
 0:       fp8_activation_input_store: 0
 0:       apply_rope_fusion: true
 0:       disable_parameter_transpose_cache: true
 0:       ub_tp_comm_overlap: true
 0:       tp_comm_overlap_ag: true
 0:       tp_comm_overlap_rs: true
 0:       tp_comm_overlap_rs_dgrad: false
 0:       tp_comm_disable_qkv: false
 0:       batch_p2p_comm: 'False'
 0:       virtual_pipeline_model_parallel_size: 1
 0:       sharp: true
 0:       nccl_communicator_config_path: conf/nccl/custom_communicator_cta.yaml
 0:       peft:
 0:         peft_scheme: lora
 0:         restore_from_path: null
 0:         lora_tuning:
 0:           adapter_dim: 16
 0:           alpha: 32
 0:           adapter_dropout: 0.1
 0:           dropout_position: pre
 0:           target_modules:
 0:           - attention
 0:           column_init_method: kaiming
 0:           row_init_method: zero
 0:           layer_selection: null
 0:           weight_tying: false
 0:           position_embedding_strategy: null
 0:           a2a_experimental: 1
 0:       data:
 0:         multiprocessing_context: spawn
 0:         pin_memory: true
 0:         sample_weight: constant
 0:         validation_drop_last: false
 0:         train_ds:
 0:           file_names:
 0:           - /data/train.npy
 0:           packed_sequence: true
 0:           packed_sequence_return_cu_seqlen: false
 0:           index_mapping_dir: /results/data_index/train
 0:           global_batch_size: 16
 0:           micro_batch_size: 1
 0:           shuffle: true
 0:           num_workers: 1
 0:           memmap_workers: 2
 0:           pin_memory: true
 0:           max_seq_length: 8192
 0:           min_seq_length: 1
 0:           drop_last: true
 0:           concat_sampling_probabilities:
 0:           - 1.0
 0:           label_key: output
 0:           add_eos: true
 0:           add_sep: false
 0:           add_bos: false
 0:           truncation_field: input
 0:           prompt_template: '{input} {output}'
 0:           truncation_method: right
 0:           seed: 9236
 0:         validation_ds:
 0:           file_names:
 0:           - /data/validation.npy
 0:           packed_sequence: true
 0:           packed_sequence_return_cu_seqlen: false
 0:           index_mapping_dir: /results/data_index/val
 0:           names: null
 0:           global_batch_size: 16
 0:           micro_batch_size: 1
 0:           shuffle: false
 0:           num_workers: 1
 0:           memmap_workers: 2
 0:           pin_memory: true
 0:           max_seq_length: 8192
 0:           min_seq_length: 1
 0:           drop_last: false
 0:           label_key: output
 0:           add_eos: true
 0:           add_sep: false
 0:           add_bos: false
 0:           write_predictions_to_file: false
 0:           output_file_path_prefix: null
 0:           truncation_field: input
 0:           prompt_template: '{input} {output}'
 0:           tokens_to_generate: 32
 0:           truncation_method: right
 0:           metric:
 0:             name: loss
 0:             average: null
 0:             num_classes: null
 0:       optim:
 0:         name: fused_adam
 0:         lr: 0.00072
 0:         weight_decay: 0.0001
 0:         betas:
 0:         - 0.9
 0:         - 0.999
 0:         eps: 1.0e-08
 0:         amsgrad: false
 0:         sched:
 0:           name: CosineAnnealing
 0:           warmup_ratio: 0.0
 0:           min_lr: 0.0
 0:           constant_steps: 0
 0:           monitor: val_loss
 0:           reduce_on_plateau: false
 0:       custom:
 0:         warmup: true
 0:         warmup_train_steps: 5
 0:         warmup_validation_steps: 5
 0:         reset_fp8_stats_after_warmup: 1
 0:     name: megatron_gpt_peft_lora_tuning
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 8
 0:       accelerator: gpu
 0:       precision: bf16-mixed
 0:       max_steps: 1024
 0:       val_check_interval: 96
 0:       check_val_every_n_epoch: null
 0:       log_every_n_steps: 0
 0:       gradient_clip_val: 0.3
 0:       gradient_clip_algorithm: norm
 0:       num_sanity_val_steps: 0
 0:       max_epochs: 1000
 0:       limit_val_batches: 1.0
 0:       limit_train_batches: 1.0
 0:       limit_test_batches: 0
 0:       logger: false
 0:       enable_checkpointing: false
 0:       use_distributed_sampler: false
 0:       enable_progress_bar: false
 0:     exp_manager:
 0:       explicit_log_dir: null
 0:       exp_dir: /results
 0:       create_wandb_logger: false
 0:       resume_if_exists: false
 0:       resume_ignore_no_checkpoint: true
 0:       create_checkpoint_callback: false
 0:       log_global_rank_0_only: true
 0:       create_early_stopping_callback: false
 0:       create_tensorboard_logger: false
 0:     
 0: [NeMo W 2024-09-29 05:53:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
 0:     
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
 0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
 0: [NeMo I 2024-09-29 05:53:10 save_restore_connector:134] Restoration will occur within pre-extracted directory : `/ckpt`.
 0: [NeMo I 2024-09-29 05:53:10 save_restore_connector:134] Restoration will occur within pre-extracted directory : `/ckpt`.
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:265] Rank 0 has data parallel group : [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60], [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61], [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62], [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:298] Rank 0 has model parallel group: [0, 1, 2, 3]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:299] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:308] Rank 0 has tensor model parallel group: [0, 1, 2, 3]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:312] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-09-29 05:53:10 megatron_init:354] Rank 0 has embedding rank: 0
 0: 24-09-29 05:53:10 - PID:2685838 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo I 2024-09-29 05:53:10 tokenizer_utils:187] Getting SentencePiece with model: /ckpt/d9a3fcd8246a432f95ed96c9011cd224_tokenizer.model
43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
 0: [NeMo I 2024-09-29 05:53:10 megatron_base_model:586] Padded vocab_size: 32256, original vocab_size: 32000, dummy tokens: 256.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:1150] The model: CustomMegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:499] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-29 05:53:10 megatron_base_model:558] The model: CustomMegatronGPTSFTModel() does not have field.name: rotary_percent in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 64 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [NeMo W 2024-09-29 05:53:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:1507: UserWarning: pg_options._timeout was specified, but timeout kwarg has a default value that will always override it. 
 0:       warnings.warn(
 0:     
 0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
 1: NCCL version 2.21.5+cuda12.4
 3: NCCL version 2.21.5+cuda12.4
 0: NCCL version 2.21.5+cuda12.4
 2: NCCL version 2.21.5+cuda12.4
 0: [NeMo I 2024-09-29 05:53:29 nlp_overrides:1127] Restoration will occur within pre-extracted directory : `/ckpt`.
 0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
 0: [NeMo I 2024-09-29 05:57:12 nlp_overrides:1156] Model CustomMegatronGPTSFTModel was successfully restored from /ckpt.
 0: [NeMo W 2024-09-29 05:57:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.
 0:     
 0: [NeMo I 2024-09-29 05:57:12 nlp_adapter_mixins:203] Before adding PEFT params:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 133 M 
 0:     ----------------------------------------
 0:     0         Trainable params
 0:     133 M     Non-trainable params
 0:     133 M     Total params
 0:     533.758   Total estimated model params size (MB)
 0: [NeMo I 2024-09-29 05:57:20 nlp_adapter_mixins:208] After adding PEFT params:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 144 M 
 0:     ----------------------------------------
 0:     11.1 M    Trainable params
 0:     133 M     Non-trainable params
 0:     144 M     Total params
 0:     578.322   Total estimated model params size (MB)
 0: [NeMo W 2024-09-29 05:57:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `CustomMegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
 0:     
 0: [NeMo I 2024-09-29 05:57:30 megatron_gpt_sft_model:804] Building GPT SFT validation datasets.
 0: [NeMo I 2024-09-29 05:57:30 megatron_gpt_sft_model:807] Length of val dataset: 173
 0: [NeMo I 2024-09-29 05:57:30 megatron_gpt_sft_model:814] Building GPT SFT traing datasets.
56: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
40: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
48: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
32: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
56: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 8: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
16: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 0: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
24: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
40: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
32: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
48: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
56: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
40: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
32: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
48: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: > building indices for blendable datasets ...
 0:  > sample ratios:
 0:    dataset 0, input: 1, achieved: 1
 0: [NeMo I 2024-09-29 05:57:36 blendable_dataset:67] > elapsed time for building blendable dataset indices: 5.04 (sec)
 0: [NeMo I 2024-09-29 05:57:36 megatron_gpt_sft_model:816] Length of train dataset: 16466
 0: [NeMo I 2024-09-29 05:57:36 megatron_gpt_sft_model:821] Building dataloader with consumed samples: 0
 0: [NeMo I 2024-09-29 05:57:36 megatron_gpt_sft_model:821] Building dataloader with consumed samples: 0
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo W 2024-09-29 05:57:36 megatron_base_model:1191] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1024.
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_kqv_adapter
 0: [NeMo I 2024-09-29 05:57:36 adapter_mixins:435] Unfrozen adapter : lora_dense_attention_adapter
 0: [NeMo I 2024-09-29 05:57:36 nlp_adapter_mixins:269] Optimizer groups set:
 0:       | Name  | Type          | Params
 0:     ----------------------------------------
 0:     0 | model | Float16Module | 144 M 
 0:     ----------------------------------------
 0:     11.1 M    Trainable params
 0:     133 M     Non-trainable params
 0:     144 M     Total params
 0:     578.322   Total estimated model params size (MB)
 0: [NeMo I 2024-09-29 05:57:36 modelPT:724] Optimizer config = FusedAdam (
 0:     Parameter Group 0
 0:         betas: [0.9, 0.999]
 0:         bias_correction: True
 0:         eps: 1e-08
 0:         lr: 0.00072
 0:         weight_decay: 0.0001
 0:     )
 0: [NeMo I 2024-09-29 05:57:36 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x716929758d30>" 
 0:     will be used during training (effective maximum steps = 1024) - 
 0:     Parameters : 
 0:     (warmup_ratio: 0.0
 0:     min_lr: 0.0
 0:     constant_steps: 0
 0:     max_steps: 1024
 0:     )
 0: [NeMo I 2024-09-29 05:57:36 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x716d903e9630>" 
 0:     will be used during training (effective maximum steps = 1024) - 
 0:     Parameters : 
 0:     (warmup_ratio: 0.0
 0:     min_lr: 0.0
 0:     constant_steps: 0
 0:     max_steps: 1024
 0:     )
 0: 
 0:   | Name  | Type          | Params
 0: ----------------------------------------
 0: 0 | model | Float16Module | 144 M 
 0: ----------------------------------------
 0: 11.1 M    Trainable params
 0: 133 M     Non-trainable params
 0: 144 M     Total params
 0: 578.322   Total estimated model params size (MB)
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456873, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 207}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 208}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "seed", "value": 9236, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 209}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589456874, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 215}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457497, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 220}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457519, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 224}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 228}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 232}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 242}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00072, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 243}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 244}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589457520, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 245}}
 1: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
49: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
57: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
33: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
50: SLURM auto-requeueing enabled. Setting signal handlers.
51: SLURM auto-requeueing enabled. Setting signal handlers.
41: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
58: SLURM auto-requeueing enabled. Setting signal handlers.
42: SLURM auto-requeueing enabled. Setting signal handlers.
43: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
44: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
34: SLURM auto-requeueing enabled. Setting signal handlers.
35: SLURM auto-requeueing enabled. Setting signal handlers.
36: SLURM auto-requeueing enabled. Setting signal handlers.
37: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
52: SLURM auto-requeueing enabled. Setting signal handlers.
45: SLURM auto-requeueing enabled. Setting signal handlers.
38: SLURM auto-requeueing enabled. Setting signal handlers.
59: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
53: SLURM auto-requeueing enabled. Setting signal handlers.
46: SLURM auto-requeueing enabled. Setting signal handlers.
60: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
54: SLURM auto-requeueing enabled. Setting signal handlers.
61: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
55: SLURM auto-requeueing enabled. Setting signal handlers.
62: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
48: SLURM auto-requeueing enabled. Setting signal handlers.
39: SLURM auto-requeueing enabled. Setting signal handlers.
63: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
32: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
40: SLURM auto-requeueing enabled. Setting signal handlers.
56: SLURM auto-requeueing enabled. Setting signal handlers.
47: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [NeMo W 2024-09-29 05:57:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.
 0:     
 0: !!! [UB] Create UbufP2PCommOverlap Communicator
 0: MC initialized succesfully, window size = 549755813888
62: gdrcopy open failed
46: gdrcopy open failed
 3: gdrcopy open failed
52: gdrcopy open failed
 2: gdrcopy open failed
37: gdrcopy open failed
53: gdrcopy open failed
49: gdrcopy open failed
57: gdrcopy open failed
41: gdrcopy open failed
43: gdrcopy open failed
15: gdrcopy open failed
 7: gdrcopy open failed
24: gdrcopy open failed
32: gdrcopy open failed
 8: gdrcopy open failed
51: gdrcopy open failed
61: gdrcopy open failed
22: gdrcopy open failed
33: gdrcopy open failed
63: gdrcopy open failed
 6: gdrcopy open failed
54: gdrcopy open failed
23: gdrcopy open failed
13: gdrcopy open failed
34: gdrcopy open failed
42: gdrcopy open failed
26: gdrcopy open failed
10: gdrcopy open failed
29: gdrcopy open failed
35: gdrcopy open failed
 9: gdrcopy open failed
47: gdrcopy open failed
25: gdrcopy open failed
45: gdrcopy open failed
 4: gdrcopy open failed
38: gdrcopy open failed
58: gdrcopy open failed
 5: gdrcopy open failed
11: gdrcopy open failed
56: gdrcopy open failed
 0: gdrcopy open failed
12: gdrcopy open failed
39: gdrcopy open failed
18: gdrcopy open failed
60: gdrcopy open failed
14: gdrcopy open failed
 1: gdrcopy open failed
30: gdrcopy open failed
50: gdrcopy open failed
44: gdrcopy open failed
28: gdrcopy open failed
36: gdrcopy open failed
40: gdrcopy open failed
55: gdrcopy open failed
20: gdrcopy open failed
16: gdrcopy open failed
27: gdrcopy open failed
31: gdrcopy open failed
21: gdrcopy open failed
17: gdrcopy open failed
19: gdrcopy open failed
59: gdrcopy open failed
48: gdrcopy open failed
 0: !!! [UBP2P] Register UBuf 1
 0: !!! [UBP2P] Register UBuf 2
 0: !!! [UBP2P] Register UBuf 3
 0: !!! [UBP2P] Register UBuf 4
 0: [NeMo W 2024-09-29 05:57:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:119: UserWarning: Atomic GEMM uses a beta API from cublas and is not tested for all use cases.
 0:       warnings.warn(
 0:     
 0: !!! [UB] Register UBuf 5
 0: !!! [UB] Register UBuf 6
 0: !!! [UB] Register UBuf 7
 0: !!! [UB] Register UBuf 8
 0: !!! [UB] Register UBuf 9
 0: !!! [UB] Register UBuf 10
 4: NCCL version 2.21.5+cuda12.4
24: NCCL version 2.21.5+cuda12.4
52: NCCL version 2.21.5+cuda12.4
48: NCCL version 2.21.5+cuda12.4
60: NCCL version 2.21.5+cuda12.4
20: NCCL version 2.21.5+cuda12.4
40: NCCL version 2.21.5+cuda12.4
12: NCCL version 2.21.5+cuda12.4
44: NCCL version 2.21.5+cuda12.4
56: NCCL version 2.21.5+cuda12.4
16: NCCL version 2.21.5+cuda12.4
 8: NCCL version 2.21.5+cuda12.4
28: NCCL version 2.21.5+cuda12.4
36: NCCL version 2.21.5+cuda12.4
32: NCCL version 2.21.5+cuda12.4
 0: [NeMo W 2024-09-29 05:58:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2954: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
 0:       warnings.warn(
 0:     
 0: [NeMo I 2024-09-29 05:58:51 custom_callbacks:40] Finished training warmup: 58.608471155166626s. Starting validation warmup
 0: [NeMo I 2024-09-29 05:58:55 custom_callbacks:54] Time spent in run_training_warmup: 62.08108711242676s
 0: [NeMo I 2024-09-29 05:58:55 custom_callbacks:59] Forcing FP8 stats reinitialization
 0: :::MLLOG {"namespace": "", "time_ms": 1727589535430, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 145}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589535431, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 145}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589535431, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 146, "samples_count": 0}}
 0: [NeMo W 2024-09-29 06:00:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.
 0:     
 0: :::MLLOG {"namespace": "", "time_ms": 1727589663830, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 13.580711430811538}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589663831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589663831, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 1536}}
 0: [NeMo W 2024-09-29 06:01:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
 0:       warnings.warn("This function is only for unittest")
 0:     
 0: :::MLLOG {"namespace": "", "time_ms": 1727589670844, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9567994475364685, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589670844, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589670845, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589694035, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.57829476798466}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589694035, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589694035, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589700143, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9390339851379395, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589700143, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589700143, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589723288, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.610484798319206}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589723288, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589723288, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589729387, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9388481378555298, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589729387, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589729387, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589752528, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.6142465098547}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589752528, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589752528, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589758637, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9318948984146118, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589758637, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589758637, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589781882, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.538821125315042}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589781882, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589781883, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589787878, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9310759902000427, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589787878, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589787878, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589811052, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.58898220137617}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589811053, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589811053, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589817084, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9284157156944275, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589817084, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589817084, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 129, "samples_count": 3456}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589840251, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 16.59629819744668}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 161, "step": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589840251, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 112, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589840251, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 117, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589846417, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9248287677764893, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 181, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589846418, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 186, "samples_count": 3840}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727589846418, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9248287677764893, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 195, "samples_count": 3840, "status": "success"}}
29: FlashAttention Installed
30: FlashAttention Installed
15: FlashAttention Installed
27: FlashAttention Installed
25: FlashAttention Installed
 9: FlashAttention Installed
10: FlashAttention Installed
26: FlashAttention Installed
28: FlashAttention Installed
12: FlashAttention Installed
14: FlashAttention Installed
 8: FlashAttention Installed
46: FlashAttention Installed
40: FlashAttention Installed
34: FlashAttention Installed
29: FlashAttention Installed
50: FlashAttention Installed
49: FlashAttention Installed
27: FlashAttention Installed
42: FlashAttention Installed
14: FlashAttention Installed
15: FlashAttention Installed
44: FlashAttention Installed
53: FlashAttention Installed
55: FlashAttention Installed
12: FlashAttention Installed
 1: FlashAttention Installed
45: FlashAttention Installed
22: FlashAttention Installed
16: FlashAttention Installed
 3: FlashAttention Installed
33: FlashAttention Installed
17: FlashAttention Installed
30: FlashAttention Installed
35: FlashAttention Installed
37: FlashAttention Installed
25: FlashAttention Installed
26: FlashAttention Installed
28: FlashAttention Installed
 7: FlashAttention Installed
 2: FlashAttention Installed
10: FlashAttention Installed
20: FlashAttention Installed
 9: FlashAttention Installed
18: FlashAttention Installed
 5: FlashAttention Installed
 0: FlashAttention Installed
36: FlashAttention Installed
23: FlashAttention Installed
 4: FlashAttention Installed
39: FlashAttention Installed
 8: FlashAttention Installed
46: FlashAttention Installed
57: FlashAttention Installed
 6: FlashAttention Installed
40: FlashAttention Installed
52: FlashAttention Installed
42: FlashAttention Installed
55: FlashAttention Installed
49: FlashAttention Installed
45: FlashAttention Installed
50: FlashAttention Installed
60: FlashAttention Installed
 7: FlashAttention Installed
 1: FlashAttention Installed
 2: FlashAttention Installed
 3: FlashAttention Installed
 5: FlashAttention Installed
17: FlashAttention Installed
20: FlashAttention Installed
18: FlashAttention Installed
53: FlashAttention Installed
22: FlashAttention Installed
34: FlashAttention Installed
44: FlashAttention Installed
39: FlashAttention Installed
37: FlashAttention Installed
16: FlashAttention Installed
33: FlashAttention Installed
35: FlashAttention Installed
 0: FlashAttention Installed
36: FlashAttention Installed
 6: FlashAttention Installed
57: FlashAttention Installed
 4: FlashAttention Installed
23: FlashAttention Installed
52: FlashAttention Installed
60: FlashAttention Installed
48: slurmstepd: error: NVML: Failed to get Compute running procs(7): Insufficient Size
24: FlashAttention Installed
24: FlashAttention Installed
63: FlashAttention Installed
61: FlashAttention Installed
56: FlashAttention Installed
58: FlashAttention Installed
59: FlashAttention Installed
62: FlashAttention Installed
11: FlashAttention Installed
13: FlashAttention Installed
19: FlashAttention Installed
47: FlashAttention Installed
 4: [1727589884.951026] [calvin-training-node-011:2685887:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 4: [1727589884.951054] [calvin-training-node-011:2685887:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 4: [1727589884.951059] [calvin-training-node-011:2685887:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 1: [1727589884.951614] [calvin-training-node-011:2685854:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 1: [1727589884.951634] [calvin-training-node-011:2685854:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 1: [1727589884.951639] [calvin-training-node-011:2685854:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 2: [1727589884.951523] [calvin-training-node-011:2685820:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 2: [1727589884.951552] [calvin-training-node-011:2685820:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 2: [1727589884.951557] [calvin-training-node-011:2685820:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 3: [1727589884.951573] [calvin-training-node-011:2685911:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 3: [1727589884.951594] [calvin-training-node-011:2685911:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 3: [1727589884.951599] [calvin-training-node-011:2685911:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 5: [1727589884.951701] [calvin-training-node-011:2685804:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 5: [1727589884.951719] [calvin-training-node-011:2685804:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 5: [1727589884.951723] [calvin-training-node-011:2685804:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 6: [1727589884.951732] [calvin-training-node-011:2685910:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 6: [1727589884.951750] [calvin-training-node-011:2685910:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 6: [1727589884.951754] [calvin-training-node-011:2685910:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 7: [1727589884.951775] [calvin-training-node-011:2685870:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 7: [1727589884.951794] [calvin-training-node-011:2685870:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 7: [1727589884.951798] [calvin-training-node-011:2685870:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 0: [1727589884.951873] [calvin-training-node-011:2685838:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 0: [1727589884.951892] [calvin-training-node-011:2685838:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 0: [1727589884.951896] [calvin-training-node-011:2685838:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
21: FlashAttention Installed
43: FlashAttention Installed
54: FlashAttention Installed
41: FlashAttention Installed
38: FlashAttention Installed
32: FlashAttention Installed
31: FlashAttention Installed
56: FlashAttention Installed
63: FlashAttention Installed
51: FlashAttention Installed
62: FlashAttention Installed
11: FlashAttention Installed
48: FlashAttention Installed
47: FlashAttention Installed
61: FlashAttention Installed
19: FlashAttention Installed
58: FlashAttention Installed
21: FlashAttention Installed
59: FlashAttention Installed
13: FlashAttention Installed
38: FlashAttention Installed
43: FlashAttention Installed
54: FlashAttention Installed
41: FlashAttention Installed
31: FlashAttention Installed
32: FlashAttention Installed
51: FlashAttention Installed
48: FlashAttention Installed
36: [1727589892.909053] [calvin-training-node-016:2666948:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
36: [1727589892.909086] [calvin-training-node-016:2666948:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
36: [1727589892.909091] [calvin-training-node-016:2666948:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
33: [1727589892.909343] [calvin-training-node-016:2667005:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
33: [1727589892.909376] [calvin-training-node-016:2667005:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
33: [1727589892.909382] [calvin-training-node-016:2667005:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
35: [1727589892.909484] [calvin-training-node-016:2667029:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
35: [1727589892.909504] [calvin-training-node-016:2667029:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
35: [1727589892.909509] [calvin-training-node-016:2667029:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
38: [1727589892.909400] [calvin-training-node-016:2666964:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
38: [1727589892.909425] [calvin-training-node-016:2666964:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
38: [1727589892.909430] [calvin-training-node-016:2666964:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
39: [1727589892.909436] [calvin-training-node-016:2666932:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
39: [1727589892.909455] [calvin-training-node-016:2666932:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
39: [1727589892.909460] [calvin-training-node-016:2666932:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
32: [1727589892.909550] [calvin-training-node-016:2667045:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
32: [1727589892.909570] [calvin-training-node-016:2667045:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
32: [1727589892.909576] [calvin-training-node-016:2667045:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
34: [1727589892.909610] [calvin-training-node-016:2666986:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
34: [1727589892.909628] [calvin-training-node-016:2666986:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
34: [1727589892.909632] [calvin-training-node-016:2666986:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
37: [1727589892.909696] [calvin-training-node-016:2666998:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
37: [1727589892.909720] [calvin-training-node-016:2666998:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
37: [1727589892.909726] [calvin-training-node-016:2666998:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
28: [1727589892.948512] [calvin-training-node-015:2676989:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
28: [1727589892.948532] [calvin-training-node-015:2676989:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
28: [1727589892.948537] [calvin-training-node-015:2676989:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
29: [1727589892.948489] [calvin-training-node-015:2677037:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
29: [1727589892.948516] [calvin-training-node-015:2677037:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
29: [1727589892.948520] [calvin-training-node-015:2677037:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
27: [1727589892.948557] [calvin-training-node-015:2676957:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
27: [1727589892.948578] [calvin-training-node-015:2676957:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
27: [1727589892.948583] [calvin-training-node-015:2676957:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
26: [1727589892.949634] [calvin-training-node-015:2676925:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
26: [1727589892.949663] [calvin-training-node-015:2676925:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
26: [1727589892.949669] [calvin-training-node-015:2676925:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
24: [1727589892.950267] [calvin-training-node-015:2676941:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
24: [1727589892.950298] [calvin-training-node-015:2676941:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
24: [1727589892.950303] [calvin-training-node-015:2676941:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
30: [1727589892.950317] [calvin-training-node-015:2676973:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
30: [1727589892.950335] [calvin-training-node-015:2676973:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
30: [1727589892.950340] [calvin-training-node-015:2676973:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
25: [1727589892.950940] [calvin-training-node-015:2677005:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
25: [1727589892.950967] [calvin-training-node-015:2677005:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
25: [1727589892.950972] [calvin-training-node-015:2677005:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
31: [1727589892.951155] [calvin-training-node-015:2677021:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
31: [1727589892.951189] [calvin-training-node-015:2677021:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
31: [1727589892.951195] [calvin-training-node-015:2677021:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
17: [1727589893.050640] [calvin-training-node-014:2698516:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
17: [1727589893.050670] [calvin-training-node-014:2698516:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
17: [1727589893.050675] [calvin-training-node-014:2698516:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
20: [1727589893.050938] [calvin-training-node-014:2698431:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
20: [1727589893.050970] [calvin-training-node-014:2698431:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
20: [1727589893.050976] [calvin-training-node-014:2698431:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
21: [1727589893.051090] [calvin-training-node-014:2698517:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
21: [1727589893.051111] [calvin-training-node-014:2698517:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
21: [1727589893.051117] [calvin-training-node-014:2698517:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
22: [1727589893.050981] [calvin-training-node-014:2698445:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
22: [1727589893.051002] [calvin-training-node-014:2698445:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
22: [1727589893.051007] [calvin-training-node-014:2698445:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
16: [1727589893.051136] [calvin-training-node-014:2698477:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
16: [1727589893.051155] [calvin-training-node-014:2698477:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
16: [1727589893.051159] [calvin-training-node-014:2698477:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
19: [1727589893.051218] [calvin-training-node-014:2698493:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
19: [1727589893.051238] [calvin-training-node-014:2698493:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
19: [1727589893.051244] [calvin-training-node-014:2698493:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
18: [1727589893.051265] [calvin-training-node-014:2698542:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
18: [1727589893.051290] [calvin-training-node-014:2698542:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
18: [1727589893.051295] [calvin-training-node-014:2698542:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
23: [1727589893.051316] [calvin-training-node-014:2698461:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
23: [1727589893.051333] [calvin-training-node-014:2698461:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
23: [1727589893.051338] [calvin-training-node-014:2698461:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
12: [1727589893.053390] [calvin-training-node-013:2669758:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
12: [1727589893.053423] [calvin-training-node-013:2669758:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
12: [1727589893.053428] [calvin-training-node-013:2669758:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
14: [1727589893.053582] [calvin-training-node-013:2669649:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
14: [1727589893.053606] [calvin-training-node-013:2669649:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
14: [1727589893.053612] [calvin-training-node-013:2669649:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
13: [1727589893.053716] [calvin-training-node-013:2669750:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
13: [1727589893.053750] [calvin-training-node-013:2669750:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
13: [1727589893.053756] [calvin-training-node-013:2669750:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 8: [1727589893.053767] [calvin-training-node-013:2669665:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 8: [1727589893.053788] [calvin-training-node-013:2669665:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 8: [1727589893.053792] [calvin-training-node-013:2669665:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
11: [1727589893.053835] [calvin-training-node-013:2669709:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
11: [1727589893.053855] [calvin-training-node-013:2669709:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
11: [1727589893.053867] [calvin-training-node-013:2669709:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
10: [1727589893.053888] [calvin-training-node-013:2669682:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
10: [1727589893.053907] [calvin-training-node-013:2669682:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
10: [1727589893.053912] [calvin-training-node-013:2669682:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
15: [1727589893.053934] [calvin-training-node-013:2669704:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
15: [1727589893.053955] [calvin-training-node-013:2669704:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
15: [1727589893.053960] [calvin-training-node-013:2669704:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 9: [1727589893.053995] [calvin-training-node-013:2669730:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
 9: [1727589893.054019] [calvin-training-node-013:2669730:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
 9: [1727589893.054024] [calvin-training-node-013:2669730:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
40: [1727589894.128205] [calvin-training-node-017:2845873:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
40: [1727589894.128234] [calvin-training-node-017:2845873:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
40: [1727589894.128240] [calvin-training-node-017:2845873:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
41: [1727589894.128354] [calvin-training-node-017:2845937:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
41: [1727589894.128382] [calvin-training-node-017:2845937:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
41: [1727589894.128389] [calvin-training-node-017:2845937:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
42: [1727589894.128435] [calvin-training-node-017:2845889:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
42: [1727589894.128453] [calvin-training-node-017:2845889:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
42: [1727589894.128457] [calvin-training-node-017:2845889:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
44: [1727589894.128388] [calvin-training-node-017:2845921:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
44: [1727589894.128410] [calvin-training-node-017:2845921:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
44: [1727589894.128415] [calvin-training-node-017:2845921:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
46: [1727589894.128424] [calvin-training-node-017:2845905:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
46: [1727589894.128443] [calvin-training-node-017:2845905:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
46: [1727589894.128448] [calvin-training-node-017:2845905:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
43: [1727589894.128521] [calvin-training-node-017:2845969:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
43: [1727589894.128543] [calvin-training-node-017:2845969:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
43: [1727589894.128549] [calvin-training-node-017:2845969:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
45: [1727589894.129688] [calvin-training-node-017:2845857:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
45: [1727589894.129707] [calvin-training-node-017:2845857:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
45: [1727589894.129712] [calvin-training-node-017:2845857:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
47: [1727589894.129613] [calvin-training-node-017:2845953:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
47: [1727589894.129642] [calvin-training-node-017:2845953:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
47: [1727589894.129648] [calvin-training-node-017:2845953:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
48: [1727589894.399192] [calvin-training-node-018:2658314:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
48: [1727589894.399216] [calvin-training-node-018:2658314:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
48: [1727589894.399222] [calvin-training-node-018:2658314:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
51: [1727589894.399123] [calvin-training-node-018:2658339:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
51: [1727589894.399156] [calvin-training-node-018:2658339:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
51: [1727589894.399162] [calvin-training-node-018:2658339:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
55: [1727589894.399258] [calvin-training-node-018:2658250:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
55: [1727589894.399275] [calvin-training-node-018:2658250:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
55: [1727589894.399279] [calvin-training-node-018:2658250:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
52: [1727589894.399310] [calvin-training-node-018:2658272:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
52: [1727589894.399328] [calvin-training-node-018:2658272:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
52: [1727589894.399332] [calvin-training-node-018:2658272:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
50: [1727589894.399442] [calvin-training-node-018:2658298:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
50: [1727589894.399474] [calvin-training-node-018:2658298:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
50: [1727589894.399480] [calvin-training-node-018:2658298:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
54: [1727589894.399501] [calvin-training-node-018:2658234:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
54: [1727589894.399524] [calvin-training-node-018:2658234:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
54: [1727589894.399531] [calvin-training-node-018:2658234:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
49: [1727589894.399545] [calvin-training-node-018:2658338:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
49: [1727589894.399563] [calvin-training-node-018:2658338:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
49: [1727589894.399569] [calvin-training-node-018:2658338:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
53: [1727589894.399684] [calvin-training-node-018:2658275:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
53: [1727589894.399711] [calvin-training-node-018:2658275:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
53: [1727589894.399716] [calvin-training-node-018:2658275:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
60: [1727589896.289779] [calvin-training-node-019:2642087:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
60: [1727589896.289807] [calvin-training-node-019:2642087:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
60: [1727589896.289813] [calvin-training-node-019:2642087:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
56: [1727589896.290065] [calvin-training-node-019:2642135:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
56: [1727589896.290093] [calvin-training-node-019:2642135:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
56: [1727589896.290099] [calvin-training-node-019:2642135:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
58: [1727589896.290126] [calvin-training-node-019:2642103:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
58: [1727589896.290149] [calvin-training-node-019:2642103:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
58: [1727589896.290154] [calvin-training-node-019:2642103:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
61: [1727589896.290266] [calvin-training-node-019:2642167:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
61: [1727589896.290290] [calvin-training-node-019:2642167:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
61: [1727589896.290297] [calvin-training-node-019:2642167:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
62: [1727589896.290237] [calvin-training-node-019:2642119:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
62: [1727589896.290272] [calvin-training-node-019:2642119:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
62: [1727589896.290278] [calvin-training-node-019:2642119:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
57: [1727589896.290289] [calvin-training-node-019:2642200:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
57: [1727589896.290307] [calvin-training-node-019:2642200:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
57: [1727589896.290312] [calvin-training-node-019:2642200:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
63: [1727589896.290380] [calvin-training-node-019:2642183:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
63: [1727589896.290400] [calvin-training-node-019:2642183:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
63: [1727589896.290405] [calvin-training-node-019:2642183:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
59: [1727589896.290443] [calvin-training-node-019:2642151:0]     ucc_context.c:906  UCC  WARN  tl ctx cuda is still in use
59: [1727589896.290471] [calvin-training-node-019:2642151:0]     ucc_context.c:906  UCC  WARN  tl ctx shm is still in use
59: [1727589896.290476] [calvin-training-node-019:2642151:0]     ucc_context.c:906  UCC  WARN  tl ctx ucp is still in use
 0: ENDING TIMING RUN AT 2024-09-29 06:05:01 AM
 0: RESULT,LLM_FINETUNING,,732,nvidia,2024-09-29 05:52:49 AM
