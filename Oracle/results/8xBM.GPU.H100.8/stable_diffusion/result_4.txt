+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/localdisk/sd/stable_diffusions/stable_diffusions/images/sd+mlperf-nvidia+sd.sqsh 784 8 compute-hpc-node-[25,73,96,147,157,186,194,199] BM.GPU.H100.8 Cluster DGXH100_08x08x16'
:::DLPAL /mnt/localdisk/sd/stable_diffusions/stable_diffusions/images/sd+mlperf-nvidia+sd.sqsh 784 8 compute-hpc-node-[25,73,96,147,157,186,194,199] BM.GPU.H100.8 Cluster DGXH100_08x08x16
++ srun --ntasks=1 --container-name=stable_diffusion_784 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 8 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-1018-oracle","nvidia_kernel_driver":"535.161.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-1018-oracle","nvidia_kernel_driver":"535.161.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=8 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on compute-hpc-node-96
Clearing cache on compute-hpc-node-25
Clearing cache on compute-hpc-node-186
Clearing cache on compute-hpc-node-199
Clearing cache on compute-hpc-node-157
Clearing cache on compute-hpc-node-194
Clearing cache on compute-hpc-node-147
Clearing cache on compute-hpc-node-73
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=8 --mpi=pmix --container-name=stable_diffusion_784 python -c '
from mlperf_logging import mllog
mllogger = mllog.get_mllogger()
mllogger.event(key=mllog.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1715290995551, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995566, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995566, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995605, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995607, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995642, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995671, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1715290995756, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun -l --mpi=pmix --ntasks=64 --ntasks-per-node=8 --container-name=stable_diffusion_784 --container-mounts=/nfs/scratch/sd/stable_diffusions/logs:/results,/mnt/localdisk/sd/stable_diffusions/stable_diffusions/datasets:/datasets,/mnt/localdisk/sd/stable_diffusions/stable_diffusions/checkpoints:/checkpoints,/nfs/scratch/sd/stable_diffusions/nemologs:/nemologs,/nfs/cluster:/nfs/cluster,/opt/openmpi-4.1.4/bin:/opt/openmpi-4.1.4/bin --container-workdir=/workspace/sd --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
 7: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
46: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
47: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
44: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
42: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
45: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
43: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
41: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 1: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
40: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 6: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 5: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 2: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 4: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 0: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 3: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
26: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
13: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
49: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
20: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
39: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
62: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
24: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
25: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
28: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
31: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
30: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
27: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
29: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 8: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
 9: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
10: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
11: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
12: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
15: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
14: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
51: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
48: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
54: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
55: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
52: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
53: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
50: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
19: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
17: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
22: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
18: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
21: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
16: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
23: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
36: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
37: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
35: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
32: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
38: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
33: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
34: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
57: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
56: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
60: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
58: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
63: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
61: STARTING TIMING RUN AT 2024-05-09 09:43:17 PM
59: STARTING TIMING RUN AT 2024-05-09 09:43:18 PM
41: :::MLLOG {"namespace": "", "time_ms": 1715290999207, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
45: :::MLLOG {"namespace": "", "time_ms": 1715290999207, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
47: :::MLLOG {"namespace": "", "time_ms": 1715290999207, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
42: :::MLLOG {"namespace": "", "time_ms": 1715290999207, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
40: :::MLLOG {"namespace": "", "time_ms": 1715290999209, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
43: :::MLLOG {"namespace": "", "time_ms": 1715290999209, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
44: :::MLLOG {"namespace": "", "time_ms": 1715290999209, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
46: :::MLLOG {"namespace": "", "time_ms": 1715290999209, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 3: :::MLLOG {"namespace": "", "time_ms": 1715290999231, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 1: :::MLLOG {"namespace": "", "time_ms": 1715290999231, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 2: :::MLLOG {"namespace": "", "time_ms": 1715290999231, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 5: :::MLLOG {"namespace": "", "time_ms": 1715290999231, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 6: :::MLLOG {"namespace": "", "time_ms": 1715290999233, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 7: :::MLLOG {"namespace": "", "time_ms": 1715290999234, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715290999234, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 4: :::MLLOG {"namespace": "", "time_ms": 1715290999235, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 9: :::MLLOG {"namespace": "", "time_ms": 1715290999237, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 8: :::MLLOG {"namespace": "", "time_ms": 1715290999244, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
10: :::MLLOG {"namespace": "", "time_ms": 1715290999244, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
12: :::MLLOG {"namespace": "", "time_ms": 1715290999245, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
11: :::MLLOG {"namespace": "", "time_ms": 1715290999245, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
15: :::MLLOG {"namespace": "", "time_ms": 1715290999250, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
14: :::MLLOG {"namespace": "", "time_ms": 1715290999250, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
13: :::MLLOG {"namespace": "", "time_ms": 1715290999251, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
60: :::MLLOG {"namespace": "", "time_ms": 1715290999251, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
62: :::MLLOG {"namespace": "", "time_ms": 1715290999251, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
57: :::MLLOG {"namespace": "", "time_ms": 1715290999251, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
61: :::MLLOG {"namespace": "", "time_ms": 1715290999253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
56: :::MLLOG {"namespace": "", "time_ms": 1715290999253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
32: :::MLLOG {"namespace": "", "time_ms": 1715290999253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
38: :::MLLOG {"namespace": "", "time_ms": 1715290999253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
39: :::MLLOG {"namespace": "", "time_ms": 1715290999253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
35: :::MLLOG {"namespace": "", "time_ms": 1715290999254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
58: :::MLLOG {"namespace": "", "time_ms": 1715290999254, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
21: :::MLLOG {"namespace": "", "time_ms": 1715290999255, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
23: :::MLLOG {"namespace": "", "time_ms": 1715290999255, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
18: :::MLLOG {"namespace": "", "time_ms": 1715290999255, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
37: :::MLLOG {"namespace": "", "time_ms": 1715290999256, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
36: :::MLLOG {"namespace": "", "time_ms": 1715290999257, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
19: :::MLLOG {"namespace": "", "time_ms": 1715290999257, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
22: :::MLLOG {"namespace": "", "time_ms": 1715290999258, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
16: :::MLLOG {"namespace": "", "time_ms": 1715290999258, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
24: :::MLLOG {"namespace": "", "time_ms": 1715290999260, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
31: :::MLLOG {"namespace": "", "time_ms": 1715290999260, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
27: :::MLLOG {"namespace": "", "time_ms": 1715290999260, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
25: :::MLLOG {"namespace": "", "time_ms": 1715290999260, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
29: :::MLLOG {"namespace": "", "time_ms": 1715290999261, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
30: :::MLLOG {"namespace": "", "time_ms": 1715290999262, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
26: :::MLLOG {"namespace": "", "time_ms": 1715290999262, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
33: :::MLLOG {"namespace": "", "time_ms": 1715290999265, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
28: :::MLLOG {"namespace": "", "time_ms": 1715290999266, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
59: :::MLLOG {"namespace": "", "time_ms": 1715290999268, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
63: :::MLLOG {"namespace": "", "time_ms": 1715290999268, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
34: :::MLLOG {"namespace": "", "time_ms": 1715290999276, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
53: :::MLLOG {"namespace": "", "time_ms": 1715290999278, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
51: :::MLLOG {"namespace": "", "time_ms": 1715290999278, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
52: :::MLLOG {"namespace": "", "time_ms": 1715290999278, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
54: :::MLLOG {"namespace": "", "time_ms": 1715290999279, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
48: :::MLLOG {"namespace": "", "time_ms": 1715290999280, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
55: :::MLLOG {"namespace": "", "time_ms": 1715290999281, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
49: :::MLLOG {"namespace": "", "time_ms": 1715290999282, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
50: :::MLLOG {"namespace": "", "time_ms": 1715290999282, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
20: :::MLLOG {"namespace": "", "time_ms": 1715290999386, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
17: :::MLLOG {"namespace": "", "time_ms": 1715290999394, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 9: RANDOM_SEED=14374
47: RANDOM_SEED=14374
42: RANDOM_SEED=14374
45: RANDOM_SEED=14374
41: RANDOM_SEED=14374
 8: RANDOM_SEED=14374
10: RANDOM_SEED=14374
12: RANDOM_SEED=14374
11: RANDOM_SEED=14374
40: RANDOM_SEED=14374
44: RANDOM_SEED=14374
46: RANDOM_SEED=14374
43: RANDOM_SEED=14374
 1: RANDOM_SEED=14374
 2: RANDOM_SEED=14374
 3: RANDOM_SEED=14374
 5: RANDOM_SEED=14374
 6: RANDOM_SEED=14374
18: RANDOM_SEED=14374
 0: RANDOM_SEED=14374
21: RANDOM_SEED=14374
23: RANDOM_SEED=14374
 7: RANDOM_SEED=14374
57: RANDOM_SEED=14374
60: RANDOM_SEED=14374
62: RANDOM_SEED=14374
14: RANDOM_SEED=14374
59: RANDOM_SEED=14374
15: RANDOM_SEED=14374
39: RANDOM_SEED=14374
35: RANDOM_SEED=14374
38: RANDOM_SEED=14374
32: RANDOM_SEED=14374
 4: RANDOM_SEED=14374
22: RANDOM_SEED=14374
16: RANDOM_SEED=14374
19: RANDOM_SEED=14374
61: RANDOM_SEED=14374
13: RANDOM_SEED=14374
56: RANDOM_SEED=14374
37: RANDOM_SEED=14374
58: RANDOM_SEED=14374
24: RANDOM_SEED=14374
25: RANDOM_SEED=14374
31: RANDOM_SEED=14374
27: RANDOM_SEED=14374
36: RANDOM_SEED=14374
33: RANDOM_SEED=14374
29: RANDOM_SEED=14374
28: RANDOM_SEED=14374
26: RANDOM_SEED=14374
54: RANDOM_SEED=14374
51: RANDOM_SEED=14374
52: RANDOM_SEED=14374
53: RANDOM_SEED=14374
30: RANDOM_SEED=14374
63: RANDOM_SEED=14374
48: RANDOM_SEED=14374
55: RANDOM_SEED=14374
50: RANDOM_SEED=14374
49: RANDOM_SEED=14374
34: RANDOM_SEED=14374
20: RANDOM_SEED=14374
17: RANDOM_SEED=14374
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
11: FlashAttention Installed
10: FlashAttention Installed
 9: FlashAttention Installed
57: FlashAttention Installed
19: FlashAttention Installed
59: FlashAttention Installed
18: FlashAttention Installed
32: FlashAttention Installed
33: FlashAttention Installed
34: FlashAttention Installed
35: FlashAttention Installed
42: FlashAttention Installed
43: FlashAttention Installed
45: FlashAttention Installed
15: FlashAttention Installed
 8: FlashAttention Installed
36: FlashAttention Installed
37: FlashAttention Installed
38: FlashAttention Installed
39: FlashAttention Installed
13: FlashAttention Installed
14: FlashAttention Installed
12: FlashAttention Installed
51: FlashAttention Installed
49: FlashAttention Installed
 3: FlashAttention Installed
 1: FlashAttention Installed
 5: FlashAttention Installed
27: FlashAttention Installed
25: FlashAttention Installed
26: FlashAttention Installed
46: FlashAttention Installed
47: FlashAttention Installed
41: FlashAttention Installed
44: FlashAttention Installed
40: FlashAttention Installed
 7: FlashAttention Installed
 2: FlashAttention Installed
 6: FlashAttention Installed
16: FlashAttention Installed
23: FlashAttention Installed
21: FlashAttention Installed
 0: FlashAttention Installed
22: FlashAttention Installed
20: FlashAttention Installed
 4: FlashAttention Installed
55: FlashAttention Installed
17: FlashAttention Installed
24: FlashAttention Installed
53: FlashAttention Installed
50: FlashAttention Installed
31: FlashAttention Installed
30: FlashAttention Installed
54: FlashAttention Installed
28: FlashAttention Installed
52: FlashAttention Installed
29: FlashAttention Installed
48: FlashAttention Installed
62: FlashAttention Installed
63: FlashAttention Installed
61: FlashAttention Installed
60: FlashAttention Installed
56: FlashAttention Installed
58: FlashAttention Installed
13: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
14: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
15: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
11: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
10: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 8: :::MLLOG {"namespace": "", "time_ms": 1715291015185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
12: :::MLLOG {"namespace": "", "time_ms": 1715291015186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 9: :::MLLOG {"namespace": "", "time_ms": 1715291015186, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
19: :::MLLOG {"namespace": "", "time_ms": 1715291015278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
21: :::MLLOG {"namespace": "", "time_ms": 1715291015278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
17: :::MLLOG {"namespace": "", "time_ms": 1715291015278, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
16: :::MLLOG {"namespace": "", "time_ms": 1715291015279, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
23: :::MLLOG {"namespace": "", "time_ms": 1715291015280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
18: :::MLLOG {"namespace": "", "time_ms": 1715291015281, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
22: :::MLLOG {"namespace": "", "time_ms": 1715291015287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
20: :::MLLOG {"namespace": "", "time_ms": 1715291015287, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
59: :::MLLOG {"namespace": "", "time_ms": 1715291015518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
58: :::MLLOG {"namespace": "", "time_ms": 1715291015518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
62: :::MLLOG {"namespace": "", "time_ms": 1715291015518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
56: :::MLLOG {"namespace": "", "time_ms": 1715291015518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
63: :::MLLOG {"namespace": "", "time_ms": 1715291015518, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
61: :::MLLOG {"namespace": "", "time_ms": 1715291015519, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
60: :::MLLOG {"namespace": "", "time_ms": 1715291015519, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
57: :::MLLOG {"namespace": "", "time_ms": 1715291015520, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
39: :::MLLOG {"namespace": "", "time_ms": 1715291015616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
37: :::MLLOG {"namespace": "", "time_ms": 1715291015616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
38: :::MLLOG {"namespace": "", "time_ms": 1715291015616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
35: :::MLLOG {"namespace": "", "time_ms": 1715291015616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
33: :::MLLOG {"namespace": "", "time_ms": 1715291015616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
36: :::MLLOG {"namespace": "", "time_ms": 1715291015617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
32: :::MLLOG {"namespace": "", "time_ms": 1715291015618, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
34: :::MLLOG {"namespace": "", "time_ms": 1715291015618, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
43: :::MLLOG {"namespace": "", "time_ms": 1715291015644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
47: :::MLLOG {"namespace": "", "time_ms": 1715291015644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
45: :::MLLOG {"namespace": "", "time_ms": 1715291015644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
42: :::MLLOG {"namespace": "", "time_ms": 1715291015644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
46: :::MLLOG {"namespace": "", "time_ms": 1715291015645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
44: :::MLLOG {"namespace": "", "time_ms": 1715291015645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
40: :::MLLOG {"namespace": "", "time_ms": 1715291015645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
41: :::MLLOG {"namespace": "", "time_ms": 1715291015646, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
55: :::MLLOG {"namespace": "", "time_ms": 1715291015794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
48: :::MLLOG {"namespace": "", "time_ms": 1715291015794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
49: :::MLLOG {"namespace": "", "time_ms": 1715291015794, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
50: :::MLLOG {"namespace": "", "time_ms": 1715291015795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
51: :::MLLOG {"namespace": "", "time_ms": 1715291015795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
52: :::MLLOG {"namespace": "", "time_ms": 1715291015795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
53: :::MLLOG {"namespace": "", "time_ms": 1715291015796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
54: :::MLLOG {"namespace": "", "time_ms": 1715291015796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
10: :::MLLOG {"namespace": "", "time_ms": 1715291015821, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1088836931, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
10: [rank: 10] Seed set to 1088836931
15: :::MLLOG {"namespace": "", "time_ms": 1715291015824, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4292446245, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
15: [rank: 15] Seed set to 4292446245
12: :::MLLOG {"namespace": "", "time_ms": 1715291015825, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3112136728, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
12: [rank: 12] Seed set to 3112136728
13: :::MLLOG {"namespace": "", "time_ms": 1715291015833, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4036806970, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
13: [rank: 13] Seed set to 4036806970
28: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
30: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
25: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
24: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
31: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
29: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
27: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
26: :::MLLOG {"namespace": "", "time_ms": 1715291015834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 9: :::MLLOG {"namespace": "", "time_ms": 1715291015836, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3020821731, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 9: [rank: 9] Seed set to 3020821731
 8: :::MLLOG {"namespace": "", "time_ms": 1715291015836, "event_type": "POINT_IN_TIME", "key": "seed", "value": 59460107, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 8: [rank: 8] Seed set to 59460107
11: :::MLLOG {"namespace": "", "time_ms": 1715291015837, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4133017892, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
11: [rank: 11] Seed set to 4133017892
14: :::MLLOG {"namespace": "", "time_ms": 1715291015840, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2385671098, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
14: [rank: 14] Seed set to 2385671098
 0: [NeMo W 2024-05-09 21:43:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
 6: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 7: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 1: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 4: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 2: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 5: :::MLLOG {"namespace": "", "time_ms": 1715291015851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 3: :::MLLOG {"namespace": "", "time_ms": 1715291015852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
19: :::MLLOG {"namespace": "", "time_ms": 1715291015905, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1559619285, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
19: [rank: 19] Seed set to 1559619285
17: :::MLLOG {"namespace": "", "time_ms": 1715291015911, "event_type": "POINT_IN_TIME", "key": "seed", "value": 128094880, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
17: [rank: 17] Seed set to 128094880
22: :::MLLOG {"namespace": "", "time_ms": 1715291015917, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3059577671, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
22: [rank: 22] Seed set to 3059577671
21: :::MLLOG {"namespace": "", "time_ms": 1715291015936, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2170899939, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
21: [rank: 21] Seed set to 2170899939
23: :::MLLOG {"namespace": "", "time_ms": 1715291015937, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3984555644, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
23: [rank: 23] Seed set to 3984555644
18: :::MLLOG {"namespace": "", "time_ms": 1715291015937, "event_type": "POINT_IN_TIME", "key": "seed", "value": 145142766, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
18: [rank: 18] Seed set to 145142766
16: :::MLLOG {"namespace": "", "time_ms": 1715291015937, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1066130061, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
16: [rank: 16] Seed set to 1066130061
20: :::MLLOG {"namespace": "", "time_ms": 1715291015937, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3641938478, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
20: [rank: 20] Seed set to 3641938478
58: :::MLLOG {"namespace": "", "time_ms": 1715291016092, "event_type": "POINT_IN_TIME", "key": "seed", "value": 456766335, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
58: [rank: 58] Seed set to 456766335
59: :::MLLOG {"namespace": "", "time_ms": 1715291016104, "event_type": "POINT_IN_TIME", "key": "seed", "value": 452194231, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
59: [rank: 59] Seed set to 452194231
63: :::MLLOG {"namespace": "", "time_ms": 1715291016127, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1882806941, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
63: [rank: 63] Seed set to 1882806941
62: :::MLLOG {"namespace": "", "time_ms": 1715291016130, "event_type": "POINT_IN_TIME", "key": "seed", "value": 990860283, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
62: [rank: 62] Seed set to 990860283
60: :::MLLOG {"namespace": "", "time_ms": 1715291016133, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2085076274, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
60: [rank: 60] Seed set to 2085076274
56: :::MLLOG {"namespace": "", "time_ms": 1715291016133, "event_type": "POINT_IN_TIME", "key": "seed", "value": 352900632, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
56: [rank: 56] Seed set to 352900632
57: :::MLLOG {"namespace": "", "time_ms": 1715291016133, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1968500461, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
57: [rank: 57] Seed set to 1968500461
61: :::MLLOG {"namespace": "", "time_ms": 1715291016134, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1453661995, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
61: [rank: 61] Seed set to 1453661995
32: :::MLLOG {"namespace": "", "time_ms": 1715291016232, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4132945725, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
32: [rank: 32] Seed set to 4132945725
33: :::MLLOG {"namespace": "", "time_ms": 1715291016233, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3637519243, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
33: [rank: 33] Seed set to 3637519243
34: :::MLLOG {"namespace": "", "time_ms": 1715291016236, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1806133993, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
34: [rank: 34] Seed set to 1806133993
37: :::MLLOG {"namespace": "", "time_ms": 1715291016250, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3359391927, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
37: [rank: 37] Seed set to 3359391927
45: :::MLLOG {"namespace": "", "time_ms": 1715291016257, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2431406478, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
45: [rank: 45] Seed set to 2431406478
40: :::MLLOG {"namespace": "", "time_ms": 1715291016258, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3846099586, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
40: [rank: 40] Seed set to 3846099586
38: :::MLLOG {"namespace": "", "time_ms": 1715291016260, "event_type": "POINT_IN_TIME", "key": "seed", "value": 10841111, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
38: [rank: 38] Seed set to 10841111
35: :::MLLOG {"namespace": "", "time_ms": 1715291016260, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3478808410, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
35: [rank: 35] Seed set to 3478808410
39: :::MLLOG {"namespace": "", "time_ms": 1715291016260, "event_type": "POINT_IN_TIME", "key": "seed", "value": 537806782, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
39: [rank: 39] Seed set to 537806782
42: :::MLLOG {"namespace": "", "time_ms": 1715291016260, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3600058237, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
42: [rank: 42] Seed set to 3600058237
36: :::MLLOG {"namespace": "", "time_ms": 1715291016263, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3732503568, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
36: [rank: 36] Seed set to 3732503568
46: :::MLLOG {"namespace": "", "time_ms": 1715291016263, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3371291008, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
46: [rank: 46] Seed set to 3371291008
44: :::MLLOG {"namespace": "", "time_ms": 1715291016273, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3622077073, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
44: [rank: 44] Seed set to 3622077073
47: :::MLLOG {"namespace": "", "time_ms": 1715291016279, "event_type": "POINT_IN_TIME", "key": "seed", "value": 794455739, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
47: [rank: 47] Seed set to 794455739
41: :::MLLOG {"namespace": "", "time_ms": 1715291016281, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2781477147, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
41: [rank: 41] Seed set to 2781477147
43: :::MLLOG {"namespace": "", "time_ms": 1715291016284, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4072472847, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
43: [rank: 43] Seed set to 4072472847
49: :::MLLOG {"namespace": "", "time_ms": 1715291016400, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3043647767, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
49: [rank: 49] Seed set to 3043647767
48: :::MLLOG {"namespace": "", "time_ms": 1715291016417, "event_type": "POINT_IN_TIME", "key": "seed", "value": 665230422, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
48: [rank: 48] Seed set to 665230422
50: :::MLLOG {"namespace": "", "time_ms": 1715291016430, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2721796576, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
50: [rank: 50] Seed set to 2721796576
52: :::MLLOG {"namespace": "", "time_ms": 1715291016430, "event_type": "POINT_IN_TIME", "key": "seed", "value": 707218325, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
52: [rank: 52] Seed set to 707218325
27: :::MLLOG {"namespace": "", "time_ms": 1715291016432, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3414745270, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
27: [rank: 27] Seed set to 3414745270
55: :::MLLOG {"namespace": "", "time_ms": 1715291016433, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1017218269, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
55: [rank: 55] Seed set to 1017218269
54: :::MLLOG {"namespace": "", "time_ms": 1715291016435, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1476480276, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
54: [rank: 54] Seed set to 1476480276
51: :::MLLOG {"namespace": "", "time_ms": 1715291016436, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2556651688, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
51: [rank: 51] Seed set to 2556651688
53: :::MLLOG {"namespace": "", "time_ms": 1715291016436, "event_type": "POINT_IN_TIME", "key": "seed", "value": 700672152, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
53: [rank: 53] Seed set to 700672152
24: :::MLLOG {"namespace": "", "time_ms": 1715291016437, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1809915594, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
24: [rank: 24] Seed set to 1809915594
 0: [NeMo I 2024-05-09 21:43:36 main:65] L2 promotion: 128 B
 0: :::MLLOG {"namespace": "", "time_ms": 1715291016450, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1751802132, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 0: [rank: 0] Seed set to 1751802132
 0: [NeMo I 2024-05-09 21:43:36 main:85] 
 0:     
 0:     ************** Experiment configuration ***********
 1: :::MLLOG {"namespace": "", "time_ms": 1715291016454, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2502358170, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 1: [rank: 1] Seed set to 2502358170
 0: [NeMo I 2024-05-09 21:43:36 main:86] 
 0:     name: stable-diffusion2-train-240509214240561128956
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 8
 0:       accelerator: gpu
 0:       logger: false
 0:       enable_checkpointing: false
 0:       max_epochs: -1
 0:       max_steps: 3500
 0:       log_every_n_steps: 10000
 0:       accumulate_grad_batches: 1
 0:       gradient_clip_val: 1.0
 0:       benchmark: false
 0:       enable_model_summary: true
 0:     exp_manager:
 0:       exp_dir: /tmp/nemologs
 0:       name: ${name}
 0:       create_wandb_logger: false
 0:       wandb_logger_kwargs:
 0:         project: stable-diffusion
 0:         group: nemo-sd
 0:         name: ${name}
 0:         resume: true
 0:       create_checkpoint_callback: true
 0:       create_tensorboard_logger: true
 0:       checkpoint_callback_params:
 0:         every_n_train_steps: 500
 0:         every_n_epochs: 0
 0:         monitor: timestamp
 0:         filename: ${name}--{timestamp}-{step}-{consumed_samples}
 0:         save_top_k: -1
 0:         save_last: false
 0:         save_nemo_on_train_end: false
 0:         save_weights_only: true
 0:       resume_if_exists: true
 0:       resume_ignore_no_checkpoint: true
 0:       ema:
 0:         enable: false
 0:         decay: 0.9999
 0:         validate_original_weights: false
 0:         every_n_steps: 1
 0:         cpu_offload: false
 0:       create_preemption_callback: false
 0:       log_step_timing: false
 0:     model:
 0:       precision: 16
 0:       micro_batch_size: 16
 0:       global_batch_size: 1024
 0:       linear_start: 0.00085
 0:       linear_end: 0.012
 0:       num_timesteps_cond: 1
 0:       log_every_t: 200
 0:       timesteps: 1000
 0:       first_stage_key: images_moments
 0:       cond_stage_key: clip_encoded
 0:       image_size: 64
 0:       channels: 4
 0:       cond_stage_trainable: false
 0:       conditioning_key: crossattn
 0:       monitor: val/loss_simple_ema
 0:       scale_factor: 0.18215
 0:       use_ema: false
 0:       scale_by_std: false
 0:       ckpt_path: /checkpoints/sd/512-base-ema.ckpt
 0:       load_vae: true
 0:       load_unet: false
 0:       load_encoder: true
 0:       ignore_keys: []
 0:       parameterization: v
 0:       clip_denoised: true
 0:       load_only_unet: false
 0:       cosine_s: 0.008
 0:       given_betas: null
 0:       original_elbo_weight: 0
 0:       v_posterior: 0
 0:       l_simple_weight: 1
 0:       use_positional_encodings: false
 0:       learn_logvar: false
 0:       logvar_init: 0
 0:       beta_schedule: linear
 0:       loss_type: l2
 0:       channels_last: true
 0:       concat_mode: true
 0:       cond_stage_forward: null
 0:       text_embedding_dropout_rate: 0.0
 0:       fused_opt: true
 0:       inductor: true
 0:       inductor_cudagraphs: false
 0:       capture_cudagraph_iters: 15
 0:       unet_config:
 0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
 0:         from_pretrained: null
 0:         from_NeMo: null
 0:         image_size: 32
 0:         in_channels: 4
 0:         out_channels: 4
 0:         model_channels: 320
 0:         attention_resolutions:
 0:         - 4
 0:         - 2
 0:         - 1
 0:         num_res_blocks: 2
 0:         channel_mult:
 0:         - 1
 0:         - 2
 0:         - 4
 0:         - 4
 0:         num_head_channels: 64
 0:         use_spatial_transformer: true
 0:         use_linear_in_transformer: true
 0:         transformer_depth: 1
 0:         context_dim: 1024
 0:         use_checkpoint: false
 0:         legacy: false
 0:         use_flash_attention: true
 0:         resblock_gn_groups: 16
 0:         unet_precision: fp16
 0:         timesteps: ${model.timesteps}
 0:       first_stage_config:
 0:         _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
 0:         from_pretrained: null
 0:         embed_dim: 4
 0:         monitor: val/rec_loss
 0:         ddconfig:
 0:           double_z: true
 0:           z_channels: 4
 0:           resolution: 256
 0:           in_channels: 3
 0:           out_ch: 3
 0:           ch: 128
 0:           ch_mult:
 0:           - 1
 0:           - 2
 0:           - 4
 0:           - 4
 0:           num_res_blocks: 2
 0:           attn_resolutions: []
 0:           dropout: 0.0
 0:         lossconfig:
 0:           target: torch.nn.Identity
 0:       cond_stage_config:
 0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder
 0:         arch: ViT-H-14
 0:         version: laion2b_s32b_b79k
 0:         freeze: true
 0:         layer: penultimate
 0:         cache_dir: /checkpoints/clip
 0:       seed: 14374
 0:       resume_from_checkpoint: null
 0:       apex_transformer_log_level: 30
 0:       gradient_as_bucket_view: true
 0:       ddp_overlap: false
 0:       nsys_profile:
 0:         enabled: false
 0:         start_step: 10
 0:         end_step: 10
 0:         ranks:
 0:         - 0
 0:         gen_shape: false
 0:       data:
 0:         num_workers: 16
 0:         train:
 0:           dataset_path: /datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar
 0:           augmentations:
 0:             resize_smallest_side: 512
 0:             center_crop_h_w: 512, 512
 0:             horizontal_flip: false
 0:           filterings: null
 0:         webdataset:
 0:           infinite_sampler: true
 0:           local_root_path: /datasets/laion-400m/webdataset-moments-filtered-encoded
 0:       optim:
 0:         name: distributed_fused_adam
 0:         lr: 0.0001024
 0:         weight_decay: 0.0
 0:         betas:
 0:         - 0.9
 0:         - 0.999
 0:         sched:
 0:           name: WarmupHoldPolicy
 0:           warmup_steps: 1000
 0:           hold_steps: 10000000000000
 0:         bucket_cap_mb: 288
 0:         overlap_grad_sync: true
 0:         overlap_param_sync: false
 0:         contiguous_grad_buffer: true
 0:         contiguous_param_buffer: true
 0:         store_params: true
 0:         dtype: torch.float32
 0:         grad_sync_dtype: torch.float16
 0:         param_sync_dtype: torch.float16
 0:         capturable: true
 0:         distribute_within_nodes: true
 0:     
 0: [NeMo W 2024-05-09 21:43:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
 0:     
28: :::MLLOG {"namespace": "", "time_ms": 1715291016466, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2142456780, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
28: [rank: 28] Seed set to 2142456780
31: :::MLLOG {"namespace": "", "time_ms": 1715291016467, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4176308445, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
31: [rank: 31] Seed set to 4176308445
26: :::MLLOG {"namespace": "", "time_ms": 1715291016468, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2213635717, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
26: [rank: 26] Seed set to 2213635717
25: :::MLLOG {"namespace": "", "time_ms": 1715291016468, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3524690094, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
25: [rank: 25] Seed set to 3524690094
30: :::MLLOG {"namespace": "", "time_ms": 1715291016468, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3581171287, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
30: [rank: 30] Seed set to 3581171287
29: :::MLLOG {"namespace": "", "time_ms": 1715291016468, "event_type": "POINT_IN_TIME", "key": "seed", "value": 609754100, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
29: [rank: 29] Seed set to 609754100
 6: :::MLLOG {"namespace": "", "time_ms": 1715291016487, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3117652927, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 6: [rank: 6] Seed set to 3117652927
 7: :::MLLOG {"namespace": "", "time_ms": 1715291016488, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3416308273, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 7: [rank: 7] Seed set to 3416308273
 4: :::MLLOG {"namespace": "", "time_ms": 1715291016488, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3011316352, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 4: [rank: 4] Seed set to 3011316352
 3: :::MLLOG {"namespace": "", "time_ms": 1715291016489, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3423325841, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 3: [rank: 3] Seed set to 3423325841
 2: :::MLLOG {"namespace": "", "time_ms": 1715291016489, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3063585131, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 2: [rank: 2] Seed set to 3063585131
 5: :::MLLOG {"namespace": "", "time_ms": 1715291016489, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1462985841, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 5: [rank: 5] Seed set to 1462985841
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo E 2024-05-09 21:43:36 exp_manager:556] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
 0: [NeMo W 2024-05-09 21:43:36 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
 0: [NeMo W 2024-05-09 21:43:36 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints. Training from scratch.
 0: [NeMo I 2024-05-09 21:43:36 exp_manager:396] Experiments will be logged at /tmp/nemologs/stable-diffusion2-train-240509214240561128956
 0: [NeMo I 2024-05-09 21:43:36 exp_manager:856] TensorboardLogger has been set up
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
 0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
 0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:43:36 megatron_init:354] Rank 0 has embedding rank: 0
 0: 24-05-09 21:43:36 - PID:96768 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:43:36 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
 0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
 0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-05-09 21:43:36 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-05-09 21:43:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:43:36 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:43:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:43:36 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:43:37 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:37 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:38 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:39 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:40 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:40 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:43:40 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:43:40 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:43:41 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:43:41 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:43:41 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:43:41 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
58: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
59: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
63: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
62: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
57: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
61: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
56: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
60: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
33: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
32: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
34: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
37: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
35: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
38: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
39: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
36: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
45: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
40: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
42: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
46: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
44: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
47: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
43: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
41: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
49: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
48: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
50: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
52: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
55: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
51: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
53: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
54: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:43:42 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-05-09 21:43:42 ddpm:168] Use system random generator since CUDA graph enabled
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
 8: making attention of type 'vanilla' with 512 in_channels
16: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
56: making attention of type 'vanilla' with 512 in_channels
56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
56: making attention of type 'vanilla' with 512 in_channels
 9: making attention of type 'vanilla' with 512 in_channels
11: making attention of type 'vanilla' with 512 in_channels
19: making attention of type 'vanilla' with 512 in_channels
17: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
10: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
13: making attention of type 'vanilla' with 512 in_channels
14: making attention of type 'vanilla' with 512 in_channels
 9: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
11: making attention of type 'vanilla' with 512 in_channels
20: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
21: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
17: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
18: making attention of type 'vanilla' with 512 in_channels
15: making attention of type 'vanilla' with 512 in_channels
23: making attention of type 'vanilla' with 512 in_channels
20: making attention of type 'vanilla' with 512 in_channels
22: making attention of type 'vanilla' with 512 in_channels
21: making attention of type 'vanilla' with 512 in_channels
40: making attention of type 'vanilla' with 512 in_channels
32: making attention of type 'vanilla' with 512 in_channels
40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
59: making attention of type 'vanilla' with 512 in_channels
40: making attention of type 'vanilla' with 512 in_channels
58: making attention of type 'vanilla' with 512 in_channels
57: making attention of type 'vanilla' with 512 in_channels
32: making attention of type 'vanilla' with 512 in_channels
59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
56: Loaded ViT-H-14 model config.
61: making attention of type 'vanilla' with 512 in_channels
58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
62: making attention of type 'vanilla' with 512 in_channels
57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
63: making attention of type 'vanilla' with 512 in_channels
59: making attention of type 'vanilla' with 512 in_channels
60: making attention of type 'vanilla' with 512 in_channels
58: making attention of type 'vanilla' with 512 in_channels
61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
57: making attention of type 'vanilla' with 512 in_channels
62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
61: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
62: making attention of type 'vanilla' with 512 in_channels
63: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
60: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
48: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
24: making attention of type 'vanilla' with 512 in_channels
48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
48: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
45: making attention of type 'vanilla' with 512 in_channels
40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
40: Loaded ViT-H-14 model config.
33: making attention of type 'vanilla' with 512 in_channels
44: making attention of type 'vanilla' with 512 in_channels
46: making attention of type 'vanilla' with 512 in_channels
43: making attention of type 'vanilla' with 512 in_channels
36: making attention of type 'vanilla' with 512 in_channels
32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
47: making attention of type 'vanilla' with 512 in_channels
32: Loaded ViT-H-14 model config.
38: making attention of type 'vanilla' with 512 in_channels
42: making attention of type 'vanilla' with 512 in_channels
45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
35: making attention of type 'vanilla' with 512 in_channels
33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
39: making attention of type 'vanilla' with 512 in_channels
37: making attention of type 'vanilla' with 512 in_channels
34: making attention of type 'vanilla' with 512 in_channels
46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
41: making attention of type 'vanilla' with 512 in_channels
36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
45: making attention of type 'vanilla' with 512 in_channels
59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
59: Loaded ViT-H-14 model config.
33: making attention of type 'vanilla' with 512 in_channels
44: making attention of type 'vanilla' with 512 in_channels
42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
58: Loaded ViT-H-14 model config.
37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
46: making attention of type 'vanilla' with 512 in_channels
43: making attention of type 'vanilla' with 512 in_channels
57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
57: Loaded ViT-H-14 model config.
36: making attention of type 'vanilla' with 512 in_channels
47: making attention of type 'vanilla' with 512 in_channels
38: making attention of type 'vanilla' with 512 in_channels
41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
42: making attention of type 'vanilla' with 512 in_channels
35: making attention of type 'vanilla' with 512 in_channels
39: making attention of type 'vanilla' with 512 in_channels
37: making attention of type 'vanilla' with 512 in_channels
34: making attention of type 'vanilla' with 512 in_channels
61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
61: Loaded ViT-H-14 model config.
62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
62: Loaded ViT-H-14 model config.
41: making attention of type 'vanilla' with 512 in_channels
63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
63: Loaded ViT-H-14 model config.
60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
60: Loaded ViT-H-14 model config.
49: making attention of type 'vanilla' with 512 in_channels
50: making attention of type 'vanilla' with 512 in_channels
51: making attention of type 'vanilla' with 512 in_channels
49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
54: making attention of type 'vanilla' with 512 in_channels
52: making attention of type 'vanilla' with 512 in_channels
48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
48: Loaded ViT-H-14 model config.
50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
27: making attention of type 'vanilla' with 512 in_channels
53: making attention of type 'vanilla' with 512 in_channels
49: making attention of type 'vanilla' with 512 in_channels
55: making attention of type 'vanilla' with 512 in_channels
 3: making attention of type 'vanilla' with 512 in_channels
 7: making attention of type 'vanilla' with 512 in_channels
31: making attention of type 'vanilla' with 512 in_channels
25: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
 4: making attention of type 'vanilla' with 512 in_channels
50: making attention of type 'vanilla' with 512 in_channels
51: making attention of type 'vanilla' with 512 in_channels
54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
30: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: Loaded ViT-H-14 model config.
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
54: making attention of type 'vanilla' with 512 in_channels
52: making attention of type 'vanilla' with 512 in_channels
29: making attention of type 'vanilla' with 512 in_channels
 2: making attention of type 'vanilla' with 512 in_channels
 5: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
53: making attention of type 'vanilla' with 512 in_channels
55: making attention of type 'vanilla' with 512 in_channels
 7: making attention of type 'vanilla' with 512 in_channels
 3: making attention of type 'vanilla' with 512 in_channels
31: making attention of type 'vanilla' with 512 in_channels
25: making attention of type 'vanilla' with 512 in_channels
28: making attention of type 'vanilla' with 512 in_channels
 4: making attention of type 'vanilla' with 512 in_channels
26: making attention of type 'vanilla' with 512 in_channels
30: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
45: Loaded ViT-H-14 model config.
44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
44: Loaded ViT-H-14 model config.
33: Loaded ViT-H-14 model config.
29: making attention of type 'vanilla' with 512 in_channels
46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
46: Loaded ViT-H-14 model config.
43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
43: Loaded ViT-H-14 model config.
36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
36: Loaded ViT-H-14 model config.
47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
47: Loaded ViT-H-14 model config.
38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
38: Loaded ViT-H-14 model config.
35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
35: Loaded ViT-H-14 model config.
42: Loaded ViT-H-14 model config.
39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
39: Loaded ViT-H-14 model config.
37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
37: Loaded ViT-H-14 model config.
34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
34: Loaded ViT-H-14 model config.
41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
41: Loaded ViT-H-14 model config.
49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
49: Loaded ViT-H-14 model config.
50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
50: Loaded ViT-H-14 model config.
51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
51: Loaded ViT-H-14 model config.
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
54: Loaded ViT-H-14 model config.
52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
52: Loaded ViT-H-14 model config.
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
53: Loaded ViT-H-14 model config.
55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
55: Loaded ViT-H-14 model config.
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
25: Loaded ViT-H-14 model config.
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-05-09 21:43:55 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-05-09 21:43:55 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-05-09 21:43:55 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-05-09 21:43:55 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 0: [NeMo I 2024-05-09 21:43:56 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-05-09 21:43:56 ddpm:303] Missing Keys: ['model.diffusion_model._orig_mod.time_embed.0.weight', 'model.diffusion_model._orig_mod.time_embed.0.bias', 'model.diffusion_model._orig_mod.time_embed.2.weight', 'model.diffusion_model._orig_mod.time_embed.2.bias', 'model.diffusion_model._orig_mod.input_blocks.0.0.weight', 'model.diffusion_model._orig_mod.input_blocks.0.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.weight', 'mo
 0: del.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.tr
 0: ansformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffus
 0: ion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1
 0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transf
 0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
 0: ocks.2.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.
 0: _orig_mod.input_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.tra
 0: nsformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_mo
 0: del._orig_mod.input_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_in.weight', 'model.dif
 0: fusion_model._orig_mod.input_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.trans
 0: former_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.6.0.op.weight', 'mo
 0: del.diffusion_model._orig_mod.input_blocks.6.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1
 0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transf
 0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
 0: ocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.in
 0: put_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.tran
 0: sformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.bias', 'model.diffusion_model._orig_mod.
 0: input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_la
 0: yers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.2.weight', 'model.diffusion_model._orig_mo
 0: d.middle_block.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.norm.weight', 'model.diffusion_model._orig_mod.middle_block.1.norm.bias', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_
 0: model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.bias', 'model.
 0: diffusion_model._orig_mod.middle_block.1.proj_out.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_out.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.weight', '
 0: model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion
 0: _model._orig_mod.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_m
 0: od.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layer
 0: s.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.
 0: 0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diff
 0: usion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out
 0: _layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.t
 0: o_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'mode
 0: l.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5
 0: .0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.a
 0: ttn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.bias',
 0:  'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_lay
 0: ers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.ou
 0: tput_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.outp
 0: ut_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.e
 0: mb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_
 0: mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mo
 0: d.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
 0: .8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.
 0: _orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._o
 0: rig_mod.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_l
 0: ayers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.a
 0: ttn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_
 0: out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
 0: .10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.
 0: 1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.
 0: 10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.weight', 'm
 0: odel.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model
 0: .diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'mo
 0: del.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.bias', 'model.diffusion_model._orig_mod.out.0.weight', 'model.diffusion_model._orig_mod.out.0.bias', 'model.diffusion_model._orig_mod.out.1.weight', 'model.diffusion_model._orig_mod.
 0: out.1.bias']
 0: [NeMo I 2024-05-09 21:43:56 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 0: [NeMo W 2024-05-09 21:43:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronLatentDiffusion.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
 0:     
 0: [NeMo W 2024-05-09 21:43:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
 0:     
 0: [rank: 0] Seed set to 1751802132
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
 8: [rank: 8] Seed set to 59460107
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
40: [rank: 40] Seed set to 3846099586
40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
16: [rank: 16] Seed set to 1066130061
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
 1: [rank: 1] Seed set to 2502358170
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
12: [rank: 12] Seed set to 3112136728
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
 2: [rank: 2] Seed set to 3063585131
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
13: [rank: 13] Seed set to 4036806970
14: [rank: 14] Seed set to 2385671098
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
10: [rank: 10] Seed set to 1088836931
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
23: [rank: 23] Seed set to 3984555644
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
 5: [rank: 5] Seed set to 1462985841
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
 9: [rank: 9] Seed set to 3020821731
 6: [rank: 6] Seed set to 3117652927
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
19: [rank: 19] Seed set to 1559619285
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
17: [rank: 17] Seed set to 128094880
22: [rank: 22] Seed set to 3059577671
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
21: [rank: 21] Seed set to 2170899939
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
 4: [rank: 4] Seed set to 3011316352
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
59: [rank: 59] Seed set to 452194231
 3: [rank: 3] Seed set to 3423325841
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
57: [rank: 57] Seed set to 1968500461
57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
42: [rank: 42] Seed set to 3600058237
42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
58: [rank: 58] Seed set to 456766335
58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
61: [rank: 61] Seed set to 1453661995
61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
62: [rank: 62] Seed set to 990860283
62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
60: [rank: 60] Seed set to 2085076274
60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
63: [rank: 63] Seed set to 1882806941
63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
11: [rank: 11] Seed set to 4133017892
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
15: [rank: 15] Seed set to 4292446245
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
41: [rank: 41] Seed set to 2781477147
41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
56: [rank: 56] Seed set to 352900632
56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
32: [rank: 32] Seed set to 4132945725
32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
43: [rank: 43] Seed set to 4072472847
43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
18: [rank: 18] Seed set to 145142766
20: [rank: 20] Seed set to 3641938478
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
39: [rank: 39] Seed set to 537806782
39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
29: [rank: 29] Seed set to 609754100
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
36: [rank: 36] Seed set to 3732503568
36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
38: [rank: 38] Seed set to 10841111
38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
27: [rank: 27] Seed set to 3414745270
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
 7: [rank: 7] Seed set to 3416308273
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
31: [rank: 31] Seed set to 4176308445
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
28: [rank: 28] Seed set to 2142456780
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
24: [rank: 24] Seed set to 1809915594
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
45: [rank: 45] Seed set to 2431406478
45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
51: [rank: 51] Seed set to 2556651688
51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
46: [rank: 46] Seed set to 3371291008
46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
47: [rank: 47] Seed set to 794455739
44: [rank: 44] Seed set to 3622077073
47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
26: [rank: 26] Seed set to 2213635717
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
50: [rank: 50] Seed set to 2721796576
50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
55: [rank: 55] Seed set to 1017218269
55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
52: [rank: 52] Seed set to 707218325
52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
33: [rank: 33] Seed set to 3637519243
33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
34: [rank: 34] Seed set to 1806133993
35: [rank: 35] Seed set to 3478808410
34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
37: [rank: 37] Seed set to 3359391927
37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
25: [rank: 25] Seed set to 3524690094
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
30: [rank: 30] Seed set to 3581171287
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
48: [rank: 48] Seed set to 665230422
48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
49: [rank: 49] Seed set to 3043647767
49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
54: [rank: 54] Seed set to 1476480276
54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
53: [rank: 53] Seed set to 700672152
53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 64 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [NeMo I 2024-05-09 21:44:04 main:199] Warmup allreduce with communicator at 776c3ca143b0, size 8
 6: NCCL version 2.21.5+cuda12.4
 0: NCCL version 2.21.5+cuda12.4
 2: NCCL version 2.21.5+cuda12.4
 3: NCCL version 2.21.5+cuda12.4
 1: NCCL version 2.21.5+cuda12.4
 4: NCCL version 2.21.5+cuda12.4
 5: NCCL version 2.21.5+cuda12.4
 7: NCCL version 2.21.5+cuda12.4
 9: NCCL version 2.21.5+cuda12.4
10: NCCL version 2.21.5+cuda12.4
11: NCCL version 2.21.5+cuda12.4
12: NCCL version 2.21.5+cuda12.4
13: NCCL version 2.21.5+cuda12.4
14: NCCL version 2.21.5+cuda12.4
 8: NCCL version 2.21.5+cuda12.4
15: NCCL version 2.21.5+cuda12.4
16: NCCL version 2.21.5+cuda12.4
23: NCCL version 2.21.5+cuda12.4
22: NCCL version 2.21.5+cuda12.4
21: NCCL version 2.21.5+cuda12.4
19: NCCL version 2.21.5+cuda12.4
20: NCCL version 2.21.5+cuda12.4
18: NCCL version 2.21.5+cuda12.4
17: NCCL version 2.21.5+cuda12.4
26: NCCL version 2.21.5+cuda12.4
25: NCCL version 2.21.5+cuda12.4
27: NCCL version 2.21.5+cuda12.4
28: NCCL version 2.21.5+cuda12.4
30: NCCL version 2.21.5+cuda12.4
31: NCCL version 2.21.5+cuda12.4
29: NCCL version 2.21.5+cuda12.4
24: NCCL version 2.21.5+cuda12.4
32: NCCL version 2.21.5+cuda12.4
39: NCCL version 2.21.5+cuda12.4
38: NCCL version 2.21.5+cuda12.4
37: NCCL version 2.21.5+cuda12.4
36: NCCL version 2.21.5+cuda12.4
35: NCCL version 2.21.5+cuda12.4
33: NCCL version 2.21.5+cuda12.4
34: NCCL version 2.21.5+cuda12.4
41: NCCL version 2.21.5+cuda12.4
42: NCCL version 2.21.5+cuda12.4
43: NCCL version 2.21.5+cuda12.4
44: NCCL version 2.21.5+cuda12.4
45: NCCL version 2.21.5+cuda12.4
47: NCCL version 2.21.5+cuda12.4
46: NCCL version 2.21.5+cuda12.4
40: NCCL version 2.21.5+cuda12.4
48: NCCL version 2.21.5+cuda12.4
55: NCCL version 2.21.5+cuda12.4
54: NCCL version 2.21.5+cuda12.4
53: NCCL version 2.21.5+cuda12.4
51: NCCL version 2.21.5+cuda12.4
52: NCCL version 2.21.5+cuda12.4
49: NCCL version 2.21.5+cuda12.4
50: NCCL version 2.21.5+cuda12.4
57: NCCL version 2.21.5+cuda12.4
58: NCCL version 2.21.5+cuda12.4
59: NCCL version 2.21.5+cuda12.4
60: NCCL version 2.21.5+cuda12.4
61: NCCL version 2.21.5+cuda12.4
56: NCCL version 2.21.5+cuda12.4
63: NCCL version 2.21.5+cuda12.4
62: NCCL version 2.21.5+cuda12.4
 0: [NeMo I 2024-05-09 21:44:15 ddpm:1977] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 8.66e+08. Total number of model parameters: 8.66e+08.
 0: [NeMo I 2024-05-09 21:44:15 ddpm:2005] Building datasets for Stable Diffusion...
 0: [NeMo I 2024-05-09 21:44:15 webdataset:145] Read Webdataset locally. Data stores at /datasets/laion-400m/webdataset-moments-filtered-encoded
 0: [NeMo I 2024-05-09 21:44:15 webdataset:221] Setting nbatches=812 for infinite sampler. world_size=64
 0: [NeMo I 2024-05-09 21:44:15 webdataset:224] Total number of training shards: 832
 0: [NeMo I 2024-05-09 21:44:15 webdataset:225] Total training key count: 832000
 0: [NeMo I 2024-05-09 21:44:15 ddpm:2025] Length of train dataset: 832000
 0: [NeMo I 2024-05-09 21:44:15 ddpm:2030] Finished building datasets for LatentDiffusion.
 0: [NeMo I 2024-05-09 21:44:15 ddpm:2036] Setting up train dataloader with len(len(self._train_ds)): 832000 and consumed samples: 0
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2024-05-09 21:44:16 modelPT:724] Optimizer config = MegatronDistributedFusedAdam (
 0:     Parameter Group 0
 0:         betas: [0.9, 0.999]
 0:         bias_correction: True
 0:         eps: 1e-08
 0:         lr: 0.00010240000119665638
 0:         weight_decay: 0.0
 0:     adam_w_mode: True
 0:     amsgrad: False
 0:     dtype: torch.float32
 0:     grad_sync_dtype: torch.float16
 0:     param_sync_dtype: torch.float16
 0:     device: cuda:0
 0:     process_group: 0x776c3c6f7870, world size 64
 0:     distributed_process_group: 0x776c3c593bb0, world size 8
 0:     redundant_process_group: 0x776c3ca143b0, world size 8
 0:     average_grad_sync: True
 0:     overlap_grad_sync: True
 0:     overlap_param_sync: False
 0:     bucket_cap_mb: 288
 0:     pipeline_size: 2
 0:     contiguous_param_buffer: True
 0:     contiguous_grad_buffer: True
 0:     store_params: True
 0:     store_param_remainders: False
 0:     with_scaled_states: False
 0:     nccl_ub: False
 0:     capturable: True
 0:     )
 0: [NeMo I 2024-05-09 21:44:16 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.WarmupHoldPolicy object at 0x776bca1fcdf0>" 
 0:     will be used during training (effective maximum steps = 3500) - 
 0:     Parameters : 
 0:     (warmup_steps: 1000
 0:     hold_steps: 10000000000000
 0:     max_steps: 3500
 0:     )
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056394, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 249}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 1024, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 253}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 257}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 258}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 259}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 260}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 261}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0001024, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 6513144, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 269}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056538, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291056539, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: 
 0:   | Name  | Type            | Params
 0: ------------------------------------------
 0: 0 | model | LatentDiffusion | 865 M 
 0: ------------------------------------------
 0: 865 M     Trainable params
 0: 0         Non-trainable params
 0: 865 M     Total params
 0: 3,463.643 Total estimated model params size (MB)
 0: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
56: SLURM auto-requeueing enabled. Setting signal handlers.
32: SLURM auto-requeueing enabled. Setting signal handlers.
40: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
41: SLURM auto-requeueing enabled. Setting signal handlers.
48: SLURM auto-requeueing enabled. Setting signal handlers.
57: SLURM auto-requeueing enabled. Setting signal handlers.
33: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
58: SLURM auto-requeueing enabled. Setting signal handlers.
49: SLURM auto-requeueing enabled. Setting signal handlers.
34: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
42: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
59: SLURM auto-requeueing enabled. Setting signal handlers.
51: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
50: SLURM auto-requeueing enabled. Setting signal handlers.
60: SLURM auto-requeueing enabled. Setting signal handlers.
35: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
43: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
52: SLURM auto-requeueing enabled. Setting signal handlers.
53: SLURM auto-requeueing enabled. Setting signal handlers.
61: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
54: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
62: SLURM auto-requeueing enabled. Setting signal handlers.
63: SLURM auto-requeueing enabled. Setting signal handlers.
44: SLURM auto-requeueing enabled. Setting signal handlers.
36: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
55: SLURM auto-requeueing enabled. Setting signal handlers.
37: SLURM auto-requeueing enabled. Setting signal handlers.
45: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
38: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
46: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
39: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
47: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [NeMo W 2024-05-09 21:44:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:104: Total length of `list` across ranks is zero. Please make sure this was your intention.
 0:     
 0: [NeMo W 2024-05-09 21:44:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:121: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.
 0:     
 0: CUDAGraphCallback: disable autocast cache.
 0: [NeMo W 2024-05-09 21:46:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:       warnings.warn(
 0:     
 0: [NeMo W 2024-05-09 21:46:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('global_step', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'global_step': ...})` instead.
 0:     
 0: [NeMo W 2024-05-09 21:46:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('consumed_samples', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'consumed_samples': ...})` instead.
 0:     
 0: [NeMo I 2024-05-09 21:46:35 callbacks:158] CUDAGraphCallback: capturing CUDA graph for module MegatronLatentDiffusion.
 0: CUDAGraphCallback: set optimizer.zero_grad as nop during graph capturing.
 0: :::MLLOG {"namespace": "", "time_ms": 1715291197191, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291197193, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291197195, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291210664, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 102400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291210665, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 102400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291225279, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 204800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291225280, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 204800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291239965, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 307200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291239966, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 307200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291254717, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 409600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291254718, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 409600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291269491, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 512000}}
 0: Epoch 0, global step 500: 'timestamp' reached 1715291269491.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt' as top 1
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291270694, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 512000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291284463, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 614400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291284465, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 614400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291299267, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 716800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291299268, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 716800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291314075, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 819200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291314195, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 819200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291328891, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 921600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291328892, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 921600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291343701, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1024000}}
 0: Epoch 1, global step 1000: 'timestamp' reached 1715291343700.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt' as top 2
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291345016, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1024000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291358815, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1126400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291358816, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1126400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291373651, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1228800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291373652, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1228800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291388491, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1331200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291388492, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1331200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291403338, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1433600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291403339, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1433600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291418179, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1536000}}
 0: Epoch 1, global step 1500: 'timestamp' reached 1715291418178.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt' as top 3
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291419492, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1536000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291433303, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1638400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291433425, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1638400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291448148, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1740800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291448149, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1740800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291462986, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1843200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291462987, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1843200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291477828, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1945600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291477829, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1945600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291492671, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2048000}}
 0: Epoch 2, global step 2000: 'timestamp' reached 1715291492671.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt' as top 4
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291493989, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2048000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291507805, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2150400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291507806, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2150400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291522668, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2252800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291522669, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2252800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291537525, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2355200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291537525, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2355200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291552383, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2457600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291552474, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2457600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291567247, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2560000}}
 0: Epoch 3, global step 2500: 'timestamp' reached 1715291567247.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt' as top 5
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291568561, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2560000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291582385, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2662400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291582386, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2662400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291597246, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2764800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291597247, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2764800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291612103, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2867200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291612104, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2867200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291626957, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2969600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291626958, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2969600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291641818, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3072000}}
 0: Epoch 3, global step 3000: 'timestamp' reached 1715291641818.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt' as top 6
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1715291643133, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3072000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291656967, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3174400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291656968, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3174400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291671839, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3276800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291671910, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3276800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291686711, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3379200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291686712, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3379200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291701585, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3481600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291701586, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3481600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291716452, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3584000}}
 0: Epoch 4, global step 3500: 'timestamp' reached 1715291716451.00000 (best 1715291269491.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt' as top 7
 0: Saving /tmp/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt in the background
 0: `Trainer.fit` stopped: `max_steps=3500` reached.
 0: Moving checkpoints to nemologs
 0: total 0
 0: drwxrwxr-x 4 ubuntu ubuntu 300 May  9 21:47 stable-diffusion2-train-240509214240561128956
 0: NCCL version 2.21.5+cuda12.4
57: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 6: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
63: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
61: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 2: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 4: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
59: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 5: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
60: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 1: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 7: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
58: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
62: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 3: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
50: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
15: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
20: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
45: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
38: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
54: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
31: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
13: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
47: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
22: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
52: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
18: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
27: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
41: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
36: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
34: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
16: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
49: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
11: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 9: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
43: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
29: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
33: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
23: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
51: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
25: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
12: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
42: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
35: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
19: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
24: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
14: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
44: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
37: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
17: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
56: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 0: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
55: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
53: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
10: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
46: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
28: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
30: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
39: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
32: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
48: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
40: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 8: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
21: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
26: CKPT_PATH=/nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/
 6: FlashAttention Installed
63: FlashAttention Installed
 1: FlashAttention Installed
 4: FlashAttention Installed
57: FlashAttention Installed
61: FlashAttention Installed
59: FlashAttention Installed
62: FlashAttention Installed
60: FlashAttention Installed
 3: FlashAttention Installed
 5: FlashAttention Installed
 2: FlashAttention Installed
63: Loaded ViT-H-14 model config.
13: FlashAttention Installed
58: FlashAttention Installed
 6: Loaded ViT-H-14 model config.
31: FlashAttention Installed
 7: FlashAttention Installed
15: FlashAttention Installed
50: FlashAttention Installed
20: FlashAttention Installed
22: FlashAttention Installed
 1: Loaded ViT-H-14 model config.
 4: Loaded ViT-H-14 model config.
27: FlashAttention Installed
29: FlashAttention Installed
49: FlashAttention Installed
54: FlashAttention Installed
 9: FlashAttention Installed
57: Loaded ViT-H-14 model config.
18: FlashAttention Installed
61: Loaded ViT-H-14 model config.
41: FlashAttention Installed
16: FlashAttention Installed
45: FlashAttention Installed
23: FlashAttention Installed
52: FlashAttention Installed
19: FlashAttention Installed
38: FlashAttention Installed
11: FlashAttention Installed
34: FlashAttention Installed
 3: Loaded ViT-H-14 model config.
59: Loaded ViT-H-14 model config.
62: Loaded ViT-H-14 model config.
60: Loaded ViT-H-14 model config.
25: FlashAttention Installed
42: FlashAttention Installed
43: FlashAttention Installed
58: Loaded ViT-H-14 model config.
 5: Loaded ViT-H-14 model config.
 2: Loaded ViT-H-14 model config.
53: FlashAttention Installed
51: FlashAttention Installed
 7: Loaded ViT-H-14 model config.
13: Loaded ViT-H-14 model config.
15: Loaded ViT-H-14 model config.
31: Loaded ViT-H-14 model config.
35: FlashAttention Installed
47: FlashAttention Installed
44: FlashAttention Installed
24: FlashAttention Installed
14: FlashAttention Installed
10: FlashAttention Installed
 0: FlashAttention Installed
17: FlashAttention Installed
 0: [NeMo W 2024-05-09 21:56:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
 0: [NeMo W 2024-05-09 21:56:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
 0:       warnings.warn(
 0:     
 0: [NeMo W 2024-05-09 21:56:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
 0:       warnings.warn(msg)
 0:     
12: FlashAttention Installed
56: FlashAttention Installed
50: Loaded ViT-H-14 model config.
46: FlashAttention Installed
36: FlashAttention Installed
33: FlashAttention Installed
29: Loaded ViT-H-14 model config.
55: FlashAttention Installed
27: Loaded ViT-H-14 model config.
20: Loaded ViT-H-14 model config.
22: Loaded ViT-H-14 model config.
39: FlashAttention Installed
 9: Loaded ViT-H-14 model config.
37: FlashAttention Installed
30: FlashAttention Installed
16: Loaded ViT-H-14 model config.
28: FlashAttention Installed
49: Loaded ViT-H-14 model config.
54: Loaded ViT-H-14 model config.
52: Loaded ViT-H-14 model config.
11: Loaded ViT-H-14 model config.
23: Loaded ViT-H-14 model config.
18: Loaded ViT-H-14 model config.
19: Loaded ViT-H-14 model config.
41: Loaded ViT-H-14 model config.
45: Loaded ViT-H-14 model config.
38: Loaded ViT-H-14 model config.
34: Loaded ViT-H-14 model config.
35: Loaded ViT-H-14 model config.
53: Loaded ViT-H-14 model config.
51: Loaded ViT-H-14 model config.
25: Loaded ViT-H-14 model config.
 0: Loaded ViT-H-14 model config.
56: Loaded ViT-H-14 model config.
24: Loaded ViT-H-14 model config.
17: Loaded ViT-H-14 model config.
42: Loaded ViT-H-14 model config.
55: Loaded ViT-H-14 model config.
43: Loaded ViT-H-14 model config.
10: Loaded ViT-H-14 model config.
14: Loaded ViT-H-14 model config.
12: Loaded ViT-H-14 model config.
47: Loaded ViT-H-14 model config.
46: Loaded ViT-H-14 model config.
44: Loaded ViT-H-14 model config.
 8: FlashAttention Installed
48: FlashAttention Installed
32: FlashAttention Installed
36: Loaded ViT-H-14 model config.
33: Loaded ViT-H-14 model config.
40: FlashAttention Installed
39: Loaded ViT-H-14 model config.
21: FlashAttention Installed
37: Loaded ViT-H-14 model config.
28: Loaded ViT-H-14 model config.
30: Loaded ViT-H-14 model config.
26: FlashAttention Installed
 8: Loaded ViT-H-14 model config.
32: Loaded ViT-H-14 model config.
48: Loaded ViT-H-14 model config.
40: Loaded ViT-H-14 model config.
21: Loaded ViT-H-14 model config.
26: Loaded ViT-H-14 model config.
63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Using 16bit Automatic Mixed Precision (AMP)
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo W 2024-05-09 21:56:41 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: Found checkpoints:
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:56:43 megatron_init:354] Rank 0 has embedding rank: 0
 0: 24-05-09 21:56:43 - PID:146141 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:56:43 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-05-09 21:56:43 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:56:43 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
61: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
57: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
59: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
63: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
56: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
58: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
62: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
60: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:56:44 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
49: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
51: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
54: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
48: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
53: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
52: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
55: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
50: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
38: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
33: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
35: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
36: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
37: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
34: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
39: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
32: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
42: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
41: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
43: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
45: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
44: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
40: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
47: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
46: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:56:45 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:56:46 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:56:46 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 6: Found checkpoints:
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 1: Found checkpoints:
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 1: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 21:56:47 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 4: Found checkpoints:
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 4: making attention of type 'vanilla' with 512 in_channels
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 7: Found checkpoints:
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 7: making attention of type 'vanilla' with 512 in_channels
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: making attention of type 'vanilla' with 512 in_channels
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 3: Found checkpoints:
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 3: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
 5: Found checkpoints:
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
 2: Found checkpoints:
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 2: making attention of type 'vanilla' with 512 in_channels
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 0: [NeMo I 2024-05-09 21:56:48 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-05-09 21:56:48 ddpm:168] Use system random generator since CUDA graph enabled
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
61: Found checkpoints:
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
61: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
61: making attention of type 'vanilla' with 512 in_channels
61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
61: making attention of type 'vanilla' with 512 in_channels
61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
61: Loaded ViT-H-14 model config.
62: Found checkpoints:
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
62: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
62: making attention of type 'vanilla' with 512 in_channels
62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
62: making attention of type 'vanilla' with 512 in_channels
62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
62: Loaded ViT-H-14 model config.
58: Found checkpoints:
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
58: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
58: making attention of type 'vanilla' with 512 in_channels
58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
58: making attention of type 'vanilla' with 512 in_channels
58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
58: Loaded ViT-H-14 model config.
57: Found checkpoints:
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
57: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
57: making attention of type 'vanilla' with 512 in_channels
57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
57: making attention of type 'vanilla' with 512 in_channels
57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
57: Loaded ViT-H-14 model config.
59: Found checkpoints:
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
59: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
59: making attention of type 'vanilla' with 512 in_channels
59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
59: making attention of type 'vanilla' with 512 in_channels
59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
59: Loaded ViT-H-14 model config.
56: Found checkpoints:
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
56: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
56: making attention of type 'vanilla' with 512 in_channels
56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
56: making attention of type 'vanilla' with 512 in_channels
56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
56: Loaded ViT-H-14 model config.
60: Found checkpoints:
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
60: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
60: making attention of type 'vanilla' with 512 in_channels
60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
60: making attention of type 'vanilla' with 512 in_channels
60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
60: Loaded ViT-H-14 model config.
63: Found checkpoints:
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
63: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
63: making attention of type 'vanilla' with 512 in_channels
63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
63: making attention of type 'vanilla' with 512 in_channels
63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
63: Loaded ViT-H-14 model config.
26: Found checkpoints:
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
26: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
26: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
10: Found checkpoints:
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
10: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
13: Found checkpoints:
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
13: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
13: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
30: Found checkpoints:
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
30: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
30: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
25: Found checkpoints:
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
25: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
25: making attention of type 'vanilla' with 512 in_channels
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: making attention of type 'vanilla' with 512 in_channels
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
29: Found checkpoints:
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
29: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
29: making attention of type 'vanilla' with 512 in_channels
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
 9: Found checkpoints:
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 9: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
14: Found checkpoints:
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
14: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
14: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
 8: Found checkpoints:
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
31: Found checkpoints:
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
31: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
31: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
24: Found checkpoints:
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
24: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
27: Found checkpoints:
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
27: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
12: Found checkpoints:
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
12: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
12: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
28: Found checkpoints:
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
28: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
28: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
11: Found checkpoints:
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
11: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
11: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
52: Found checkpoints:
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
52: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
52: making attention of type 'vanilla' with 512 in_channels
52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
52: making attention of type 'vanilla' with 512 in_channels
52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
52: Loaded ViT-H-14 model config.
49: Found checkpoints:
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
49: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
49: making attention of type 'vanilla' with 512 in_channels
49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
49: making attention of type 'vanilla' with 512 in_channels
49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
49: Loaded ViT-H-14 model config.
53: Found checkpoints:
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
53: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
53: making attention of type 'vanilla' with 512 in_channels
53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
53: making attention of type 'vanilla' with 512 in_channels
53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
53: Loaded ViT-H-14 model config.
50: Found checkpoints:
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
50: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
50: making attention of type 'vanilla' with 512 in_channels
50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
50: making attention of type 'vanilla' with 512 in_channels
50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
50: Loaded ViT-H-14 model config.
54: Found checkpoints:
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
54: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
54: making attention of type 'vanilla' with 512 in_channels
54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
54: making attention of type 'vanilla' with 512 in_channels
54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
54: Loaded ViT-H-14 model config.
55: Found checkpoints:
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
55: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
55: making attention of type 'vanilla' with 512 in_channels
55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
55: making attention of type 'vanilla' with 512 in_channels
55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
55: Loaded ViT-H-14 model config.
48: Found checkpoints:
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
48: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
48: making attention of type 'vanilla' with 512 in_channels
48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
48: making attention of type 'vanilla' with 512 in_channels
48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
48: Loaded ViT-H-14 model config.
51: Found checkpoints:
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
51: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
51: making attention of type 'vanilla' with 512 in_channels
51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
51: making attention of type 'vanilla' with 512 in_channels
51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
51: Loaded ViT-H-14 model config.
15: Found checkpoints:
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
15: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
15: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
17: Found checkpoints:
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
17: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
17: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: making attention of type 'vanilla' with 512 in_channels
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
20: Found checkpoints:
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
20: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
20: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
21: Found checkpoints:
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
21: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
21: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
19: Found checkpoints:
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
19: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
19: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
23: Found checkpoints:
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
23: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
23: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
39: Found checkpoints:
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
39: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
39: making attention of type 'vanilla' with 512 in_channels
39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
39: making attention of type 'vanilla' with 512 in_channels
39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
39: Loaded ViT-H-14 model config.
37: Found checkpoints:
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
37: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
37: making attention of type 'vanilla' with 512 in_channels
37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
37: making attention of type 'vanilla' with 512 in_channels
37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
37: Loaded ViT-H-14 model config.
22: Found checkpoints:
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
22: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
22: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
33: Found checkpoints:
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
33: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
33: making attention of type 'vanilla' with 512 in_channels
33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
33: making attention of type 'vanilla' with 512 in_channels
33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
33: Loaded ViT-H-14 model config.
38: Found checkpoints:
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
38: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
38: making attention of type 'vanilla' with 512 in_channels
38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
38: making attention of type 'vanilla' with 512 in_channels
38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
38: Loaded ViT-H-14 model config.
35: Found checkpoints:
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
35: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
35: making attention of type 'vanilla' with 512 in_channels
35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
35: making attention of type 'vanilla' with 512 in_channels
35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
35: Loaded ViT-H-14 model config.
40: Found checkpoints:
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
40: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
40: making attention of type 'vanilla' with 512 in_channels
40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
40: making attention of type 'vanilla' with 512 in_channels
40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
40: Loaded ViT-H-14 model config.
34: Found checkpoints:
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
34: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
34: making attention of type 'vanilla' with 512 in_channels
34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
34: making attention of type 'vanilla' with 512 in_channels
34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
34: Loaded ViT-H-14 model config.
18: Found checkpoints:
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
18: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
18: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
44: Found checkpoints:
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
44: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
44: making attention of type 'vanilla' with 512 in_channels
44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
44: making attention of type 'vanilla' with 512 in_channels
44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
44: Loaded ViT-H-14 model config.
41: Found checkpoints:
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
41: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
41: making attention of type 'vanilla' with 512 in_channels
41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
41: making attention of type 'vanilla' with 512 in_channels
41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
41: Loaded ViT-H-14 model config.
36: Found checkpoints:
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
36: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
36: making attention of type 'vanilla' with 512 in_channels
36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
36: making attention of type 'vanilla' with 512 in_channels
36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
36: Loaded ViT-H-14 model config.
42: Found checkpoints:
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
42: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
42: making attention of type 'vanilla' with 512 in_channels
42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
42: making attention of type 'vanilla' with 512 in_channels
42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
42: Loaded ViT-H-14 model config.
46: Found checkpoints:
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
46: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
46: making attention of type 'vanilla' with 512 in_channels
46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
46: making attention of type 'vanilla' with 512 in_channels
46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
46: Loaded ViT-H-14 model config.
43: Found checkpoints:
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
43: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
43: making attention of type 'vanilla' with 512 in_channels
43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
43: making attention of type 'vanilla' with 512 in_channels
43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
43: Loaded ViT-H-14 model config.
16: Found checkpoints:
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
16: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
45: Found checkpoints:
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
45: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
45: making attention of type 'vanilla' with 512 in_channels
45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
45: making attention of type 'vanilla' with 512 in_channels
45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
45: Loaded ViT-H-14 model config.
32: Found checkpoints:
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
32: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
32: making attention of type 'vanilla' with 512 in_channels
32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
32: making attention of type 'vanilla' with 512 in_channels
32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
32: Loaded ViT-H-14 model config.
47: Found checkpoints:
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291492671.0-step=2000-consumed_samples=2048000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291567247.0-step=2500-consumed_samples=2560000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291641818.0-step=3000-consumed_samples=3072000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291716451.0-step=3500-consumed_samples=3584000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291269491.0-step=500-consumed_samples=512000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291343700.0-step=1000-consumed_samples=1024000.0.ckpt
47: /nemologs/stable-diffusion2-train-240509214240561128956/checkpoints/stable-diffusion2-train-240509214240561128956--timestamp=1715291418178.0-step=1500-consumed_samples=1536000.0.ckpt
47: making attention of type 'vanilla' with 512 in_channels
47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
47: making attention of type 'vanilla' with 512 in_channels
47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
47: Loaded ViT-H-14 model config.
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-05-09 21:56:59 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-05-09 21:56:59 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-05-09 21:56:59 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-05-09 21:56:59 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
 0: [NeMo I 2024-05-09 21:56:59 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-05-09 21:56:59 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
 0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
 0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
 0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
 0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
 0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
 0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
 0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
 0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
 0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
 0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
 0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
 0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
 0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
 0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
 0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
 0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
 0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
 0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
 0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
 0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
 0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
 0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
 0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
 0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
 0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
 0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
 0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
 0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
 0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
 0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
 0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
 0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
 0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
 0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
 0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
 0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
 0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
 0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
 0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
 0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
 0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
 0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
 0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
 0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
 0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
 0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
 0: [NeMo I 2024-05-09 21:56:59 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 64 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: Global ID: 0, local ID: 0, world size: 64
 0: Rank 0 before barrier
 0: NCCL version 2.21.5+cuda12.4
 1: Global ID: 1, local ID: 1, world size: 64
 1: Rank 1 before barrier
 2: Global ID: 2, local ID: 2, world size: 64
 2: Rank 2 before barrier
 3: Global ID: 3, local ID: 3, world size: 64
 3: Rank 3 before barrier
 4: Global ID: 4, local ID: 4, world size: 64
 4: Rank 4 before barrier
 5: Global ID: 5, local ID: 5, world size: 64
 5: Rank 5 before barrier
 6: Global ID: 6, local ID: 6, world size: 64
 6: Rank 6 before barrier
 7: Global ID: 7, local ID: 7, world size: 64
 7: Rank 7 before barrier
 8: Global ID: 8, local ID: 0, world size: 64
 8: Rank 8 before barrier
 9: Global ID: 9, local ID: 1, world size: 64
 9: Rank 9 before barrier
10: Global ID: 10, local ID: 2, world size: 64
10: Rank 10 before barrier
11: Global ID: 11, local ID: 3, world size: 64
11: Rank 11 before barrier
12: Global ID: 12, local ID: 4, world size: 64
12: Rank 12 before barrier
13: Global ID: 13, local ID: 5, world size: 64
13: Rank 13 before barrier
14: Global ID: 14, local ID: 6, world size: 64
14: Rank 14 before barrier
15: Global ID: 15, local ID: 7, world size: 64
15: Rank 15 before barrier
16: Global ID: 16, local ID: 0, world size: 64
16: Rank 16 before barrier
17: Global ID: 17, local ID: 1, world size: 64
17: Rank 17 before barrier
18: Global ID: 18, local ID: 2, world size: 64
18: Rank 18 before barrier
19: Global ID: 19, local ID: 3, world size: 64
19: Rank 19 before barrier
20: Global ID: 20, local ID: 4, world size: 64
20: Rank 20 before barrier
21: Global ID: 21, local ID: 5, world size: 64
21: Rank 21 before barrier
22: Global ID: 22, local ID: 6, world size: 64
22: Rank 22 before barrier
23: Global ID: 23, local ID: 7, world size: 64
23: Rank 23 before barrier
24: Global ID: 24, local ID: 0, world size: 64
24: Rank 24 before barrier
25: Global ID: 25, local ID: 1, world size: 64
25: Rank 25 before barrier
26: Global ID: 26, local ID: 2, world size: 64
26: Rank 26 before barrier
27: Global ID: 27, local ID: 3, world size: 64
27: Rank 27 before barrier
28: Global ID: 28, local ID: 4, world size: 64
28: Rank 28 before barrier
29: Global ID: 29, local ID: 5, world size: 64
29: Rank 29 before barrier
30: Global ID: 30, local ID: 6, world size: 64
30: Rank 30 before barrier
31: Global ID: 31, local ID: 7, world size: 64
31: Rank 31 before barrier
32: Global ID: 32, local ID: 0, world size: 64
32: Rank 32 before barrier
33: Global ID: 33, local ID: 1, world size: 64
33: Rank 33 before barrier
34: Global ID: 34, local ID: 2, world size: 64
34: Rank 34 before barrier
35: Global ID: 35, local ID: 3, world size: 64
35: Rank 35 before barrier
36: Global ID: 36, local ID: 4, world size: 64
36: Rank 36 before barrier
37: Global ID: 37, local ID: 5, world size: 64
37: Rank 37 before barrier
38: Global ID: 38, local ID: 6, world size: 64
38: Rank 38 before barrier
39: Global ID: 39, local ID: 7, world size: 64
39: Rank 39 before barrier
40: Global ID: 40, local ID: 0, world size: 64
40: Rank 40 before barrier
41: Global ID: 41, local ID: 1, world size: 64
41: Rank 41 before barrier
42: Global ID: 42, local ID: 2, world size: 64
42: Rank 42 before barrier
43: Global ID: 43, local ID: 3, world size: 64
43: Rank 43 before barrier
44: Global ID: 44, local ID: 4, world size: 64
44: Rank 44 before barrier
45: Global ID: 45, local ID: 5, world size: 64
45: Rank 45 before barrier
46: Global ID: 46, local ID: 6, world size: 64
46: Rank 46 before barrier
47: Global ID: 47, local ID: 7, world size: 64
47: Rank 47 before barrier
48: Global ID: 48, local ID: 0, world size: 64
48: Rank 48 before barrier
49: Global ID: 49, local ID: 1, world size: 64
49: Rank 49 before barrier
50: Global ID: 50, local ID: 2, world size: 64
50: Rank 50 before barrier
51: Global ID: 51, local ID: 3, world size: 64
51: Rank 51 before barrier
52: Global ID: 52, local ID: 4, world size: 64
52: Rank 52 before barrier
53: Global ID: 53, local ID: 5, world size: 64
53: Rank 53 before barrier
54: Global ID: 54, local ID: 6, world size: 64
54: Rank 54 before barrier
55: Global ID: 55, local ID: 7, world size: 64
55: Rank 55 before barrier
56: Global ID: 56, local ID: 0, world size: 64
56: Rank 56 before barrier
57: Global ID: 57, local ID: 1, world size: 64
57: Rank 57 before barrier
58: Global ID: 58, local ID: 2, world size: 64
58: Rank 58 before barrier
59: Global ID: 59, local ID: 3, world size: 64
59: Rank 59 before barrier
60: Global ID: 60, local ID: 4, world size: 64
60: Rank 60 before barrier
61: Global ID: 61, local ID: 5, world size: 64
61: Rank 61 before barrier
62: Global ID: 62, local ID: 6, world size: 64
62: Rank 62 before barrier
63: Global ID: 63, local ID: 7, world size: 64
63: Rank 63 before barrier
 0: :::MLLOG {"namespace": "", "time_ms": 1715291832291, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 2048000}}
 0: Assigned 469 prompts for this worker.
 0: :::MLLOG {"namespace": "", "time_ms": 1715291976433, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 2048000}}
48: Assigned 468 prompts for this worker.
 1: Assigned 469 prompts for this worker.
50: Assigned 468 prompts for this worker.
32: Assigned 469 prompts for this worker.
17: Assigned 469 prompts for this worker.
57: Assigned 468 prompts for this worker.
 3: Assigned 469 prompts for this worker.
 2: Assigned 469 prompts for this worker.
56: Assigned 468 prompts for this worker.
 4: Assigned 469 prompts for this worker.
41: Assigned 469 prompts for this worker.
 8: Assigned 469 prompts for this worker.
16: Assigned 469 prompts for this worker.
51: Assigned 468 prompts for this worker.
34: Assigned 469 prompts for this worker.
 5: Assigned 469 prompts for this worker.
18: Assigned 469 prompts for this worker.
20: Assigned 469 prompts for this worker.
59: Assigned 468 prompts for this worker.
38: Assigned 469 prompts for this worker.
49: Assigned 468 prompts for this worker.
40: Assigned 469 prompts for this worker.
 7: Assigned 469 prompts for this worker.
53: Assigned 468 prompts for this worker.
11: Assigned 469 prompts for this worker.
19: Assigned 469 prompts for this worker.
58: Assigned 468 prompts for this worker.
55: Assigned 468 prompts for this worker.
 6: Assigned 469 prompts for this worker.
27: Assigned 469 prompts for this worker.
21: Assigned 469 prompts for this worker.
23: Assigned 469 prompts for this worker.
36: Assigned 469 prompts for this worker.
61: Assigned 468 prompts for this worker.
62: Assigned 468 prompts for this worker.
52: Assigned 468 prompts for this worker.
25: Assigned 469 prompts for this worker.
10: Assigned 469 prompts for this worker.
29: Assigned 469 prompts for this worker.
60: Assigned 468 prompts for this worker.
22: Assigned 469 prompts for this worker.
54: Assigned 468 prompts for this worker.
46: Assigned 469 prompts for this worker.
15: Assigned 469 prompts for this worker.
 9: Assigned 469 prompts for this worker.
12: Assigned 469 prompts for this worker.
14: Assigned 469 prompts for this worker.
42: Assigned 469 prompts for this worker.
13: Assigned 469 prompts for this worker.
44: Assigned 469 prompts for this worker.
45: Assigned 469 prompts for this worker.
43: Assigned 469 prompts for this worker.
47: Assigned 469 prompts for this worker.
31: Assigned 469 prompts for this worker.
63: Assigned 468 prompts for this worker.
35: Assigned 469 prompts for this worker.
33: Assigned 469 prompts for this worker.
37: Assigned 469 prompts for this worker.
39: Assigned 469 prompts for this worker.
30: Assigned 469 prompts for this worker.
26: Assigned 469 prompts for this worker.
28: Assigned 469 prompts for this worker.
24: Assigned 469 prompts for this worker.
 0: Calculating FID activations:   0%|          | 0/15 [00:00<?, ?it/s]Calculating FID activations:   7%|▋         | 1/15 [00:00<00:13,  1.00it/s]Calculating FID activations:  67%|██████▋   | 10/15 [00:01<00:00, 11.62it/s]Calculating FID activations:  93%|█████████▎| 14/15 [00:01<00:00, 15.48it/s]Calculating FID activations: 100%|██████████| 15/15 [00:01<00:00,  9.25it/s]
 0: Computed feature activations of size torch.Size([469, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1715291988054, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 97.57104768685855, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 2048000, "metric": "FID"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715291996882, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.14684101939201355, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 2048000, "metric": "CLIP"}}
 0: Using 16bit Automatic Mixed Precision (AMP)
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo W 2024-05-09 21:59:56 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
 0: [NeMo I 2024-05-09 21:59:58 megatron_init:354] Rank 0 has embedding rank: 0
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-05-09 21:59:58 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 16, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 14374, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.0001024, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-05-09 21:59:58 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-05-09 21:59:58 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 6: [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:59:58 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 4: [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 21:59:58 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:59:58 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 21:59:58 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:59:59 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-05-09 21:59:59 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
13: [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
48: [rank48]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
51: [rank51]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
54: [rank54]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
53: [rank53]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
55: [rank55]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
49: [rank49]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
50: [rank50]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
52: [rank52]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 22:00:01 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
59: [rank59]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
61: [rank61]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
58: [rank58]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
63: [rank63]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
60: [rank60]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
62: [rank62]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
57: [rank57]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
56: [rank56]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
32: [rank32]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
35: [rank35]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
37: [rank37]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
36: [rank36]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
33: [rank33]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
34: [rank34]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
38: [rank38]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
39: [rank39]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 22:00:01 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
17: [rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 22:00:01 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
47: [rank47]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
40: [rank40]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
45: [rank45]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
41: [rank41]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
46: [rank46]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
44: [rank44]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
43: [rank43]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
42: [rank42]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 22:00:02 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-05-09 22:00:02 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-05-09 22:00:02 ddpm:168] Use system random generator since CUDA graph enabled
 4: Computed feature activations of size torch.Size([469, 2048])
 4: making attention of type 'vanilla' with 512 in_channels
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 6: Computed feature activations of size torch.Size([469, 2048])
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
 3: Computed feature activations of size torch.Size([469, 2048])
 3: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
 1: Computed feature activations of size torch.Size([469, 2048])
 1: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
 2: Computed feature activations of size torch.Size([469, 2048])
 2: making attention of type 'vanilla' with 512 in_channels
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 7: Computed feature activations of size torch.Size([469, 2048])
 7: making attention of type 'vanilla' with 512 in_channels
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: making attention of type 'vanilla' with 512 in_channels
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 5: Computed feature activations of size torch.Size([469, 2048])
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
48: Computed feature activations of size torch.Size([468, 2048])
48: making attention of type 'vanilla' with 512 in_channels
48: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
48: making attention of type 'vanilla' with 512 in_channels
48: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
48: Loaded ViT-H-14 model config.
11: Computed feature activations of size torch.Size([469, 2048])
11: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
53: Computed feature activations of size torch.Size([468, 2048])
53: making attention of type 'vanilla' with 512 in_channels
53: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
53: making attention of type 'vanilla' with 512 in_channels
53: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
53: Loaded ViT-H-14 model config.
10: Computed feature activations of size torch.Size([469, 2048])
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
54: Computed feature activations of size torch.Size([468, 2048])
54: making attention of type 'vanilla' with 512 in_channels
54: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
54: making attention of type 'vanilla' with 512 in_channels
54: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
54: Loaded ViT-H-14 model config.
13: Computed feature activations of size torch.Size([469, 2048])
13: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
27: Computed feature activations of size torch.Size([469, 2048])
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
55: Computed feature activations of size torch.Size([468, 2048])
55: making attention of type 'vanilla' with 512 in_channels
55: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
55: making attention of type 'vanilla' with 512 in_channels
55: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
55: Loaded ViT-H-14 model config.
51: Computed feature activations of size torch.Size([468, 2048])
51: making attention of type 'vanilla' with 512 in_channels
51: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
51: making attention of type 'vanilla' with 512 in_channels
51: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
51: Loaded ViT-H-14 model config.
24: Computed feature activations of size torch.Size([469, 2048])
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
31: Computed feature activations of size torch.Size([469, 2048])
31: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
25: Computed feature activations of size torch.Size([469, 2048])
25: making attention of type 'vanilla' with 512 in_channels
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: making attention of type 'vanilla' with 512 in_channels
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
61: Computed feature activations of size torch.Size([468, 2048])
61: making attention of type 'vanilla' with 512 in_channels
61: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
61: making attention of type 'vanilla' with 512 in_channels
61: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
61: Loaded ViT-H-14 model config.
62: Computed feature activations of size torch.Size([468, 2048])
62: making attention of type 'vanilla' with 512 in_channels
62: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
62: making attention of type 'vanilla' with 512 in_channels
62: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
62: Loaded ViT-H-14 model config.
49: Computed feature activations of size torch.Size([468, 2048])
49: making attention of type 'vanilla' with 512 in_channels
49: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
49: making attention of type 'vanilla' with 512 in_channels
49: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
49: Loaded ViT-H-14 model config.
58: Computed feature activations of size torch.Size([468, 2048])
58: making attention of type 'vanilla' with 512 in_channels
58: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
58: making attention of type 'vanilla' with 512 in_channels
58: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
58: Loaded ViT-H-14 model config.
52: Computed feature activations of size torch.Size([468, 2048])
52: making attention of type 'vanilla' with 512 in_channels
52: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
52: making attention of type 'vanilla' with 512 in_channels
52: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
52: Loaded ViT-H-14 model config.
14: Computed feature activations of size torch.Size([469, 2048])
14: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
50: Computed feature activations of size torch.Size([468, 2048])
50: making attention of type 'vanilla' with 512 in_channels
50: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
50: making attention of type 'vanilla' with 512 in_channels
50: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
50: Loaded ViT-H-14 model config.
59: Computed feature activations of size torch.Size([468, 2048])
59: making attention of type 'vanilla' with 512 in_channels
59: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
59: making attention of type 'vanilla' with 512 in_channels
59: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
59: Loaded ViT-H-14 model config.
12: Computed feature activations of size torch.Size([469, 2048])
12: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
 9: Computed feature activations of size torch.Size([469, 2048])
 9: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
 8: Computed feature activations of size torch.Size([469, 2048])
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
26: Computed feature activations of size torch.Size([469, 2048])
26: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
30: Computed feature activations of size torch.Size([469, 2048])
30: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
35: Computed feature activations of size torch.Size([469, 2048])
35: making attention of type 'vanilla' with 512 in_channels
35: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
35: making attention of type 'vanilla' with 512 in_channels
35: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
35: Loaded ViT-H-14 model config.
28: Computed feature activations of size torch.Size([469, 2048])
28: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
32: Computed feature activations of size torch.Size([469, 2048])
32: making attention of type 'vanilla' with 512 in_channels
32: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
32: making attention of type 'vanilla' with 512 in_channels
32: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
32: Loaded ViT-H-14 model config.
37: Computed feature activations of size torch.Size([469, 2048])
37: making attention of type 'vanilla' with 512 in_channels
37: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
37: making attention of type 'vanilla' with 512 in_channels
37: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
37: Loaded ViT-H-14 model config.
29: Computed feature activations of size torch.Size([469, 2048])
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
29: making attention of type 'vanilla' with 512 in_channels
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
56: Computed feature activations of size torch.Size([468, 2048])
56: making attention of type 'vanilla' with 512 in_channels
56: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
56: making attention of type 'vanilla' with 512 in_channels
56: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
56: Loaded ViT-H-14 model config.
17: Computed feature activations of size torch.Size([469, 2048])
17: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: making attention of type 'vanilla' with 512 in_channels
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
60: Computed feature activations of size torch.Size([468, 2048])
60: making attention of type 'vanilla' with 512 in_channels
60: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
60: making attention of type 'vanilla' with 512 in_channels
60: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
60: Loaded ViT-H-14 model config.
22: Computed feature activations of size torch.Size([469, 2048])
22: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
19: Computed feature activations of size torch.Size([469, 2048])
19: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
21: Computed feature activations of size torch.Size([469, 2048])
21: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
15: Computed feature activations of size torch.Size([469, 2048])
15: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
63: Computed feature activations of size torch.Size([468, 2048])
63: making attention of type 'vanilla' with 512 in_channels
63: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
63: making attention of type 'vanilla' with 512 in_channels
63: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
63: Loaded ViT-H-14 model config.
57: Computed feature activations of size torch.Size([468, 2048])
57: making attention of type 'vanilla' with 512 in_channels
57: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
57: making attention of type 'vanilla' with 512 in_channels
57: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
57: Loaded ViT-H-14 model config.
20: Computed feature activations of size torch.Size([469, 2048])
20: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
16: Computed feature activations of size torch.Size([469, 2048])
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
38: Computed feature activations of size torch.Size([469, 2048])
38: making attention of type 'vanilla' with 512 in_channels
38: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
38: making attention of type 'vanilla' with 512 in_channels
38: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
38: Loaded ViT-H-14 model config.
34: Computed feature activations of size torch.Size([469, 2048])
34: making attention of type 'vanilla' with 512 in_channels
34: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
34: making attention of type 'vanilla' with 512 in_channels
34: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
34: Loaded ViT-H-14 model config.
39: Computed feature activations of size torch.Size([469, 2048])
39: making attention of type 'vanilla' with 512 in_channels
39: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
39: making attention of type 'vanilla' with 512 in_channels
39: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
39: Loaded ViT-H-14 model config.
33: Computed feature activations of size torch.Size([469, 2048])
33: making attention of type 'vanilla' with 512 in_channels
33: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
33: making attention of type 'vanilla' with 512 in_channels
33: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
33: Loaded ViT-H-14 model config.
18: Computed feature activations of size torch.Size([469, 2048])
18: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
36: Computed feature activations of size torch.Size([469, 2048])
36: making attention of type 'vanilla' with 512 in_channels
36: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
36: making attention of type 'vanilla' with 512 in_channels
36: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
36: Loaded ViT-H-14 model config.
23: Computed feature activations of size torch.Size([469, 2048])
23: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
47: Computed feature activations of size torch.Size([469, 2048])
47: making attention of type 'vanilla' with 512 in_channels
47: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
47: making attention of type 'vanilla' with 512 in_channels
47: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
47: Loaded ViT-H-14 model config.
46: Computed feature activations of size torch.Size([469, 2048])
46: making attention of type 'vanilla' with 512 in_channels
46: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
46: making attention of type 'vanilla' with 512 in_channels
46: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
46: Loaded ViT-H-14 model config.
40: Computed feature activations of size torch.Size([469, 2048])
40: making attention of type 'vanilla' with 512 in_channels
40: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
40: making attention of type 'vanilla' with 512 in_channels
40: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
40: Loaded ViT-H-14 model config.
45: Computed feature activations of size torch.Size([469, 2048])
45: making attention of type 'vanilla' with 512 in_channels
45: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
45: making attention of type 'vanilla' with 512 in_channels
45: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
45: Loaded ViT-H-14 model config.
42: Computed feature activations of size torch.Size([469, 2048])
42: making attention of type 'vanilla' with 512 in_channels
42: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
42: making attention of type 'vanilla' with 512 in_channels
42: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
42: Loaded ViT-H-14 model config.
41: Computed feature activations of size torch.Size([469, 2048])
41: making attention of type 'vanilla' with 512 in_channels
41: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
41: making attention of type 'vanilla' with 512 in_channels
41: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
41: Loaded ViT-H-14 model config.
44: Computed feature activations of size torch.Size([469, 2048])
44: making attention of type 'vanilla' with 512 in_channels
44: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
44: making attention of type 'vanilla' with 512 in_channels
44: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
44: Loaded ViT-H-14 model config.
43: Computed feature activations of size torch.Size([469, 2048])
43: making attention of type 'vanilla' with 512 in_channels
43: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
43: making attention of type 'vanilla' with 512 in_channels
43: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
43: Loaded ViT-H-14 model config.
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
48: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
55: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
53: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
51: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
54: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
61: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
62: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
59: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
58: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
32: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
35: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
37: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
49: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
50: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
52: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
63: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
56: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
60: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
57: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
38: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
34: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
39: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
33: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
36: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
46: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
40: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
47: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
45: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
42: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
41: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
44: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
43: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-05-09 22:00:13 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-05-09 22:00:13 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-05-09 22:00:13 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-05-09 22:00:13 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 0: [NeMo I 2024-05-09 22:00:14 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-05-09 22:00:14 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
 0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
 0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
 0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
 0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
 0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
 0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
 0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
 0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
 0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
 0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
 0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
 0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
 0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
 0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
 0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
 0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
 0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
 0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
 0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
 0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
 0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
 0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
 0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
 0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
 0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
 0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
 0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
 0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
 0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
 0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
 0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
 0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
 0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
 0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
 0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
 0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
 0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
 0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
 0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
 0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
 0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
 0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
 0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
 0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
 0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
 0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
 0: [NeMo I 2024-05-09 22:00:14 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 0: Global ID: 0, local ID: 0, world size: 64
 0: Rank 0 before barrier
 1: Global ID: 1, local ID: 1, world size: 64
 1: Rank 1 before barrier
 2: Global ID: 2, local ID: 2, world size: 64
 2: Rank 2 before barrier
 3: Global ID: 3, local ID: 3, world size: 64
 3: Rank 3 before barrier
 4: Global ID: 4, local ID: 4, world size: 64
 4: Rank 4 before barrier
 5: Global ID: 5, local ID: 5, world size: 64
 5: Rank 5 before barrier
 6: Global ID: 6, local ID: 6, world size: 64
 6: Rank 6 before barrier
 7: Global ID: 7, local ID: 7, world size: 64
 7: Rank 7 before barrier
 8: Global ID: 8, local ID: 0, world size: 64
 8: Rank 8 before barrier
 9: Global ID: 9, local ID: 1, world size: 64
 9: Rank 9 before barrier
10: Global ID: 10, local ID: 2, world size: 64
10: Rank 10 before barrier
11: Global ID: 11, local ID: 3, world size: 64
11: Rank 11 before barrier
12: Global ID: 12, local ID: 4, world size: 64
12: Rank 12 before barrier
13: Global ID: 13, local ID: 5, world size: 64
13: Rank 13 before barrier
14: Global ID: 14, local ID: 6, world size: 64
14: Rank 14 before barrier
15: Global ID: 15, local ID: 7, world size: 64
15: Rank 15 before barrier
16: Global ID: 16, local ID: 0, world size: 64
16: Rank 16 before barrier
17: Global ID: 17, local ID: 1, world size: 64
17: Rank 17 before barrier
18: Global ID: 18, local ID: 2, world size: 64
18: Rank 18 before barrier
19: Global ID: 19, local ID: 3, world size: 64
19: Rank 19 before barrier
20: Global ID: 20, local ID: 4, world size: 64
20: Rank 20 before barrier
21: Global ID: 21, local ID: 5, world size: 64
21: Rank 21 before barrier
22: Global ID: 22, local ID: 6, world size: 64
22: Rank 22 before barrier
23: Global ID: 23, local ID: 7, world size: 64
23: Rank 23 before barrier
24: Global ID: 24, local ID: 0, world size: 64
24: Rank 24 before barrier
25: Global ID: 25, local ID: 1, world size: 64
25: Rank 25 before barrier
26: Global ID: 26, local ID: 2, world size: 64
26: Rank 26 before barrier
27: Global ID: 27, local ID: 3, world size: 64
27: Rank 27 before barrier
28: Global ID: 28, local ID: 4, world size: 64
28: Rank 28 before barrier
29: Global ID: 29, local ID: 5, world size: 64
29: Rank 29 before barrier
30: Global ID: 30, local ID: 6, world size: 64
30: Rank 30 before barrier
31: Global ID: 31, local ID: 7, world size: 64
31: Rank 31 before barrier
32: Global ID: 32, local ID: 0, world size: 64
32: Rank 32 before barrier
33: Global ID: 33, local ID: 1, world size: 64
33: Rank 33 before barrier
34: Global ID: 34, local ID: 2, world size: 64
34: Rank 34 before barrier
35: Global ID: 35, local ID: 3, world size: 64
35: Rank 35 before barrier
36: Global ID: 36, local ID: 4, world size: 64
36: Rank 36 before barrier
37: Global ID: 37, local ID: 5, world size: 64
37: Rank 37 before barrier
38: Global ID: 38, local ID: 6, world size: 64
38: Rank 38 before barrier
39: Global ID: 39, local ID: 7, world size: 64
39: Rank 39 before barrier
40: Global ID: 40, local ID: 0, world size: 64
40: Rank 40 before barrier
41: Global ID: 41, local ID: 1, world size: 64
41: Rank 41 before barrier
42: Global ID: 42, local ID: 2, world size: 64
42: Rank 42 before barrier
43: Global ID: 43, local ID: 3, world size: 64
43: Rank 43 before barrier
44: Global ID: 44, local ID: 4, world size: 64
44: Rank 44 before barrier
45: Global ID: 45, local ID: 5, world size: 64
45: Rank 45 before barrier
46: Global ID: 46, local ID: 6, world size: 64
46: Rank 46 before barrier
47: Global ID: 47, local ID: 7, world size: 64
47: Rank 47 before barrier
48: Global ID: 48, local ID: 0, world size: 64
48: Rank 48 before barrier
49: Global ID: 49, local ID: 1, world size: 64
49: Rank 49 before barrier
50: Global ID: 50, local ID: 2, world size: 64
50: Rank 50 before barrier
51: Global ID: 51, local ID: 3, world size: 64
51: Rank 51 before barrier
52: Global ID: 52, local ID: 4, world size: 64
52: Rank 52 before barrier
53: Global ID: 53, local ID: 5, world size: 64
53: Rank 53 before barrier
54: Global ID: 54, local ID: 6, world size: 64
54: Rank 54 before barrier
55: Global ID: 55, local ID: 7, world size: 64
55: Rank 55 before barrier
56: Global ID: 56, local ID: 0, world size: 64
56: Rank 56 before barrier
57: Global ID: 57, local ID: 1, world size: 64
57: Rank 57 before barrier
58: Global ID: 58, local ID: 2, world size: 64
58: Rank 58 before barrier
59: Global ID: 59, local ID: 3, world size: 64
59: Rank 59 before barrier
60: Global ID: 60, local ID: 4, world size: 64
60: Rank 60 before barrier
61: Global ID: 61, local ID: 5, world size: 64
61: Rank 61 before barrier
62: Global ID: 62, local ID: 6, world size: 64
62: Rank 62 before barrier
63: Global ID: 63, local ID: 7, world size: 64
63: Rank 63 before barrier
 0: :::MLLOG {"namespace": "", "time_ms": 1715292020881, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 2560000}}
 0: Assigned 469 prompts for this worker.
 0: :::MLLOG {"namespace": "", "time_ms": 1715292164399, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 2560000}}
24: Assigned 469 prompts for this worker.
 8: Assigned 469 prompts for this worker.
48: Assigned 468 prompts for this worker.
32: Assigned 469 prompts for this worker.
40: Assigned 469 prompts for this worker.
16: Assigned 469 prompts for this worker.
26: Assigned 469 prompts for this worker.
49: Assigned 468 prompts for this worker.
25: Assigned 469 prompts for this worker.
17: Assigned 469 prompts for this worker.
41: Assigned 469 prompts for this worker.
35: Assigned 469 prompts for this worker.
11: Assigned 469 prompts for this worker.
51: Assigned 468 prompts for this worker.
18: Assigned 469 prompts for this worker.
56: Assigned 468 prompts for this worker.
 1: Assigned 469 prompts for this worker.
42: Assigned 469 prompts for this worker.
33: Assigned 469 prompts for this worker.
34: Assigned 469 prompts for this worker.
 4: Assigned 469 prompts for this worker.
 9: Assigned 469 prompts for this worker.
58: Assigned 468 prompts for this worker.
37: Assigned 469 prompts for this worker.
10: Assigned 469 prompts for this worker.
19: Assigned 469 prompts for this worker.
50: Assigned 468 prompts for this worker.
57: Assigned 468 prompts for this worker.
43: Assigned 469 prompts for this worker.
36: Assigned 469 prompts for this worker.
20: Assigned 469 prompts for this worker.
38: Assigned 469 prompts for this worker.
21: Assigned 469 prompts for this worker.
44: Assigned 469 prompts for this worker.
53: Assigned 468 prompts for this worker.
59: Assigned 468 prompts for this worker.
 2: Assigned 469 prompts for this worker.
45: Assigned 469 prompts for this worker.
52: Assigned 468 prompts for this worker.
27: Assigned 469 prompts for this worker.
12: Assigned 469 prompts for this worker.
13: Assigned 469 prompts for this worker.
14: Assigned 469 prompts for this worker.
54: Assigned 468 prompts for this worker.
60: Assigned 468 prompts for this worker.
47: Assigned 469 prompts for this worker.
 3: Assigned 469 prompts for this worker.
28: Assigned 469 prompts for this worker.
39: Assigned 469 prompts for this worker.
30: Assigned 469 prompts for this worker.
23: Assigned 469 prompts for this worker.
 5: Assigned 469 prompts for this worker.
29: Assigned 469 prompts for this worker.
22: Assigned 469 prompts for this worker.
46: Assigned 469 prompts for this worker.
31: Assigned 469 prompts for this worker.
55: Assigned 468 prompts for this worker.
61: Assigned 468 prompts for this worker.
 6: Assigned 469 prompts for this worker.
 7: Assigned 469 prompts for this worker.
15: Assigned 469 prompts for this worker.
62: Assigned 468 prompts for this worker.
63: Assigned 468 prompts for this worker.
 0: Calculating FID activations:   0%|          | 0/15 [00:00<?, ?it/s]Calculating FID activations:   7%|▋         | 1/15 [00:00<00:08,  1.61it/s]Calculating FID activations:  33%|███▎      | 5/15 [00:00<00:01,  8.48it/s]Calculating FID activations:  60%|██████    | 9/15 [00:00<00:00, 14.11it/s]Calculating FID activations:  87%|████████▋ | 13/15 [00:00<00:00, 19.37it/s]Calculating FID activations: 100%|██████████| 15/15 [00:01<00:00, 12.22it/s]
 0: Computed feature activations of size torch.Size([469, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1715292175764, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 87.77958814194216, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 2560000, "metric": "FID"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1715292185072, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.16736268997192383, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 2560000, "metric": "CLIP"}}
 4: Computed feature activations of size torch.Size([469, 2048])
37: Computed feature activations of size torch.Size([469, 2048])
 7: Computed feature activations of size torch.Size([469, 2048])
17: Computed feature activations of size torch.Size([469, 2048])
51: Computed feature activations of size torch.Size([468, 2048])
 6: Computed feature activations of size torch.Size([469, 2048])
25: Computed feature activations of size torch.Size([469, 2048])
58: Computed feature activations of size torch.Size([468, 2048])
 1: Computed feature activations of size torch.Size([469, 2048])
10: Computed feature activations of size torch.Size([469, 2048])
54: Computed feature activations of size torch.Size([468, 2048])
35: Computed feature activations of size torch.Size([469, 2048])
55: Computed feature activations of size torch.Size([468, 2048])
21: Computed feature activations of size torch.Size([469, 2048])
53: Computed feature activations of size torch.Size([468, 2048])
18: Computed feature activations of size torch.Size([469, 2048])
20: Computed feature activations of size torch.Size([469, 2048])
59: Computed feature activations of size torch.Size([468, 2048])
23: Computed feature activations of size torch.Size([469, 2048])
60: Computed feature activations of size torch.Size([468, 2048])
16: Computed feature activations of size torch.Size([469, 2048])
61: Computed feature activations of size torch.Size([468, 2048])
41: Computed feature activations of size torch.Size([469, 2048])
62: Computed feature activations of size torch.Size([468, 2048])
63: Computed feature activations of size torch.Size([468, 2048])
26: Computed feature activations of size torch.Size([469, 2048])
27: Computed feature activations of size torch.Size([469, 2048])
32: Computed feature activations of size torch.Size([469, 2048])
 5: Computed feature activations of size torch.Size([469, 2048])
33: Computed feature activations of size torch.Size([469, 2048])
 3: Computed feature activations of size torch.Size([469, 2048])
48: Computed feature activations of size torch.Size([468, 2048])
24: Computed feature activations of size torch.Size([469, 2048])
34: Computed feature activations of size torch.Size([469, 2048])
39: Computed feature activations of size torch.Size([469, 2048])
42: Computed feature activations of size torch.Size([469, 2048])
49: Computed feature activations of size torch.Size([468, 2048])
40: Computed feature activations of size torch.Size([469, 2048])
11: Computed feature activations of size torch.Size([469, 2048])
50: Computed feature activations of size torch.Size([468, 2048])
13: Computed feature activations of size torch.Size([469, 2048])
19: Computed feature activations of size torch.Size([469, 2048])
52: Computed feature activations of size torch.Size([468, 2048])
31: Computed feature activations of size torch.Size([469, 2048])
29: Computed feature activations of size torch.Size([469, 2048])
57: Computed feature activations of size torch.Size([468, 2048])
 2: Computed feature activations of size torch.Size([469, 2048])
45: Computed feature activations of size torch.Size([469, 2048])
22: Computed feature activations of size torch.Size([469, 2048])
46: Computed feature activations of size torch.Size([469, 2048])
28: Computed feature activations of size torch.Size([469, 2048])
12: Computed feature activations of size torch.Size([469, 2048])
36: Computed feature activations of size torch.Size([469, 2048])
43: Computed feature activations of size torch.Size([469, 2048])
14: Computed feature activations of size torch.Size([469, 2048])
38: Computed feature activations of size torch.Size([469, 2048])
47: Computed feature activations of size torch.Size([469, 2048])
 9: Computed feature activations of size torch.Size([469, 2048])
30: Computed feature activations of size torch.Size([469, 2048])
15: Computed feature activations of size torch.Size([469, 2048])
44: Computed feature activations of size torch.Size([469, 2048])
 8: Computed feature activations of size torch.Size([469, 2048])
56: Computed feature activations of size torch.Size([468, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1715291567247, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", "lineno": 186, "status": "success", "step_num": 2500}}
 0: ENDING TIMING RUN AT 2024-05-09 10:03:10 PM
 0: RESULT,stable_diffusion,1193,ubuntu,2024-05-09 09:43:17 PM
 4: ENDING TIMING RUN AT 2024-05-09 10:03:10 PM
 4: RESULT,stable_diffusion,1193,ubuntu,2024-05-09 09:43:17 PM
63: ENDING TIMING RUN AT 2024-05-09 10:03:10 PM
63: RESULT,stable_diffusion,1193,ubuntu,2024-05-09 09:43:17 PM
 7: ENDING TIMING RUN AT 2024-05-09 10:03:10 PM
 7: RESULT,stable_diffusion,1193,ubuntu,2024-05-09 09:43:17 PM
61: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
61: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
 1: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 1: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
59: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
59: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:18 PM
 2: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 2: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
11: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
11: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
18: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
18: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
62: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
62: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
52: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
52: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
 3: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 3: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
13: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
13: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
27: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
27: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
57: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
57: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
47: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
47: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
20: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
20: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
 6: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 6: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
48: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
48: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
56: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
56: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
 9: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 9: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
25: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
25: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
 5: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
 5: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
34: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
34: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
41: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
41: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
16: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
16: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
50: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
50: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
58: ENDING TIMING RUN AT 2024-05-09 10:03:11 PM
58: RESULT,stable_diffusion,1194,ubuntu,2024-05-09 09:43:17 PM
24: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
24: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
14: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
14: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
35: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
35: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
22: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
22: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
40: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
40: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
60: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
60: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
54: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
54: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
31: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
31: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
10: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
10: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
23: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
23: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
32: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
32: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
43: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
43: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
51: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
51: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
 8: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
 8: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
26: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
26: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
21: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
21: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
42: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
42: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
36: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
36: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
53: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
53: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
12: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
12: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
28: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
28: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
38: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
38: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
17: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
17: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
45: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
45: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
49: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
49: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
29: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
29: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
15: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
15: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
37: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
37: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
19: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
19: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
44: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
44: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
55: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
55: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
30: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
30: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
33: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
33: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
46: ENDING TIMING RUN AT 2024-05-09 10:03:12 PM
46: RESULT,stable_diffusion,1195,ubuntu,2024-05-09 09:43:17 PM
39: ENDING TIMING RUN AT 2024-05-09 10:03:13 PM
39: RESULT,stable_diffusion,1196,ubuntu,2024-05-09 09:43:17 PM
