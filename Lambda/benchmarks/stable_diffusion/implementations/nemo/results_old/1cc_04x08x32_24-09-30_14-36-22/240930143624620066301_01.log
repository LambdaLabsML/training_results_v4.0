+ echo 'Beginning trial 01 of 10'
Beginning trial 01 of 10
+ echo ':::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-stable_diffusion-pyt:latest 161 4 calvin-training-node-[001-004] lambda_1cc 1cc_04x08x32'
:::DLPAL calvin-training-head-003:5000#local/mlperf-nvidia-stable_diffusion-pyt:latest 161 4 calvin-training-node-[001-004] lambda_1cc 1cc_04x08x32
++ srun --ntasks=1 --container-name=stable_diffusion_161 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 4 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"LAMBDA","division":"closed","status":"onprem","system_name":"lambda_1cc","number_of_nodes":"4","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"52","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"1.7 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.04","framework_name":"","other_software_stack":{"cuda_version":"12.4.1.003","cuda_driver_version":"550.54.15","nccl_version":"2.21.5","cublas_version":"12.4.5.8","cudnn_version":"9.1.0.70","trt_version":"8.6.3.1+cuda12.2.2.009","dali_version":"1.36.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 6.5.0-35-generic","nvidia_kernel_driver":"535.161.08"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=4 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on calvin-training-node-004
Clearing cache on calvin-training-node-002
Clearing cache on calvin-training-node-003
Clearing cache on calvin-training-node-001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=4 --mpi=pmix --container-name=stable_diffusion_161 python -c '
from mlperf_logging import mllog
mllogger = mllog.get_mllogger()
mllogger.event(key=mllog.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1727707149877, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1727707149897, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1727707149945, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1727707150406, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun -l --mpi=pmix --ntasks=32 --ntasks-per-node=8 --container-name=stable_diffusion_161 --container-mounts=./results/1cc_04x08x32_24-09-30_14-36-22:/results,/home/ubuntu/data/mlperf/stable_diffusion:/datasets,/home/ubuntu/data/mlperf/stable_diffusion:/checkpoints,./results/1cc_04x08x32_24-09-30_14-36-22/nemologs:/nemologs,/dev/infiniband/uverbs0:/dev/infiniband/uverbs0,/dev/infiniband/uverbs1:/dev/infiniband/uverbs1,/dev/infiniband/uverbs2:/dev/infiniband/uverbs2,/dev/infiniband/uverbs3:/dev/infiniband/uverbs3,/dev/infiniband/uverbs4:/dev/infiniband/uverbs4,/dev/infiniband/uverbs5:/dev/infiniband/uverbs5,/dev/infiniband/uverbs6:/dev/infiniband/uverbs6,/dev/infiniband/uverbs7:/dev/infiniband/uverbs7 --container-workdir=/workspace/sd --export=ALL,MASTER_PORT=29500,MASTER_ADDR=calvin-training-node-001 slurm2pytorch ./run_and_time.sh
 5: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 3: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 1: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 2: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 7: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
13: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 9: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
25: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
16: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 4: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
29: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
26: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 0: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 6: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
11: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
24: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
17: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
15: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
14: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
30: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
18: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
22: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
12: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
 8: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
28: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
31: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
10: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
20: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
27: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
19: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
23: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
21: STARTING TIMING RUN AT 2024-09-30 02:39:18 PM
26: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
27: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
28: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
30: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
24: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
25: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
31: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
29: :::MLLOG {"namespace": "", "time_ms": 1727707159729, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 8: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
12: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
14: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
11: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
13: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
15: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 9: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
10: :::MLLOG {"namespace": "", "time_ms": 1727707159789, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 1: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 2: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 4: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 5: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 6: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 7: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
 3: :::MLLOG {"namespace": "", "time_ms": 1727707159845, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
26: RANDOM_SEED=30346
30: RANDOM_SEED=30346
28: RANDOM_SEED=30346
24: RANDOM_SEED=30346
25: RANDOM_SEED=30346
31: RANDOM_SEED=30346
27: RANDOM_SEED=30346
29: RANDOM_SEED=30346
11: RANDOM_SEED=30346
13: RANDOM_SEED=30346
 8: RANDOM_SEED=30346
14: RANDOM_SEED=30346
12: RANDOM_SEED=30346
 9: RANDOM_SEED=30346
15: RANDOM_SEED=30346
10: RANDOM_SEED=30346
 1: RANDOM_SEED=30346
 5: RANDOM_SEED=30346
 2: RANDOM_SEED=30346
 6: RANDOM_SEED=30346
 0: RANDOM_SEED=30346
 3: RANDOM_SEED=30346
 7: RANDOM_SEED=30346
 4: RANDOM_SEED=30346
17: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
18: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
19: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
23: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
16: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
20: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
21: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
22: :::MLLOG {"namespace": "", "time_ms": 1727707160285, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
18: RANDOM_SEED=30346
23: RANDOM_SEED=30346
19: RANDOM_SEED=30346
17: RANDOM_SEED=30346
22: RANDOM_SEED=30346
20: RANDOM_SEED=30346
21: RANDOM_SEED=30346
16: RANDOM_SEED=30346
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 4: You are offline and the cache for model files in Transformers v4.22.0 has been updated while your local cache seems to be the one of a previous version. It is very likely that all your calls to any `from_pretrained()` method will fail. Remove the offline mode and enable internet connection to have your cache be updated automatically, then you can go back to offline mode.
 4: 0it [00:00, ?it/s]0it [00:00, ?it/s]
16: You are offline and the cache for model files in Transformers v4.22.0 has been updated while your local cache seems to be the one of a previous version. It is very likely that all your calls to any `from_pretrained()` method will fail. Remove the offline mode and enable internet connection to have your cache be updated automatically, then you can go back to offline mode.
16: 0it [00:00, ?it/s]0it [00:00, ?it/s]
15: FlashAttention Installed
14: FlashAttention Installed
 9: FlashAttention Installed
12: FlashAttention Installed
 8: FlashAttention Installed
10: FlashAttention Installed
13: FlashAttention Installed
11: FlashAttention Installed
26: FlashAttention Installed
27: FlashAttention Installed
28: FlashAttention Installed
29: FlashAttention Installed
30: FlashAttention Installed
31: FlashAttention Installed
25: FlashAttention Installed
24: FlashAttention Installed
 0: FlashAttention Installed
 4: FlashAttention Installed
 5: FlashAttention Installed
 7: FlashAttention Installed
 1: FlashAttention Installed
 2: FlashAttention Installed
 3: FlashAttention Installed
 6: FlashAttention Installed
17: FlashAttention Installed
22: FlashAttention Installed
20: FlashAttention Installed
23: FlashAttention Installed
16: FlashAttention Installed
19: FlashAttention Installed
21: FlashAttention Installed
18: FlashAttention Installed
10: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
14: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
12: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
11: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 9: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
13: :::MLLOG {"namespace": "", "time_ms": 1727707177228, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
15: :::MLLOG {"namespace": "", "time_ms": 1727707177229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 8: :::MLLOG {"namespace": "", "time_ms": 1727707177231, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
25: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
29: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
31: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
27: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
26: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
24: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
28: :::MLLOG {"namespace": "", "time_ms": 1727707177753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
30: :::MLLOG {"namespace": "", "time_ms": 1727707177754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
10: :::MLLOG {"namespace": "", "time_ms": 1727707178415, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2155027037, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
10: [rank: 10] Seed set to 2155027037
14: :::MLLOG {"namespace": "", "time_ms": 1727707178415, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3113561360, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
14: [rank: 14] Seed set to 3113561360
11: :::MLLOG {"namespace": "", "time_ms": 1727707178416, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1787326884, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
11: [rank: 11] Seed set to 1787326884
 9: :::MLLOG {"namespace": "", "time_ms": 1727707178421, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2805580309, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 9: [rank: 9] Seed set to 2805580309
12: :::MLLOG {"namespace": "", "time_ms": 1727707178425, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1889092595, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
12: [rank: 12] Seed set to 1889092595
 8: :::MLLOG {"namespace": "", "time_ms": 1727707178445, "event_type": "POINT_IN_TIME", "key": "seed", "value": 828203007, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 8: [rank: 8] Seed set to 828203007
15: :::MLLOG {"namespace": "", "time_ms": 1727707178451, "event_type": "POINT_IN_TIME", "key": "seed", "value": 294679604, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
15: [rank: 15] Seed set to 294679604
13: :::MLLOG {"namespace": "", "time_ms": 1727707178459, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1241603319, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
13: [rank: 13] Seed set to 1241603319
31: :::MLLOG {"namespace": "", "time_ms": 1727707178947, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3267527404, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
31: [rank: 31] Seed set to 3267527404
24: :::MLLOG {"namespace": "", "time_ms": 1727707178948, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1179649059, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
24: [rank: 24] Seed set to 1179649059
26: :::MLLOG {"namespace": "", "time_ms": 1727707178950, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3334560731, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
26: [rank: 26] Seed set to 3334560731
30: :::MLLOG {"namespace": "", "time_ms": 1727707178951, "event_type": "POINT_IN_TIME", "key": "seed", "value": 64987781, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
30: [rank: 30] Seed set to 64987781
28: :::MLLOG {"namespace": "", "time_ms": 1727707178951, "event_type": "POINT_IN_TIME", "key": "seed", "value": 771632000, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
28: [rank: 28] Seed set to 771632000
27: :::MLLOG {"namespace": "", "time_ms": 1727707178952, "event_type": "POINT_IN_TIME", "key": "seed", "value": 352317957, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
27: [rank: 27] Seed set to 352317957
29: :::MLLOG {"namespace": "", "time_ms": 1727707178952, "event_type": "POINT_IN_TIME", "key": "seed", "value": 456176175, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
29: [rank: 29] Seed set to 456176175
25: :::MLLOG {"namespace": "", "time_ms": 1727707178954, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3767332308, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
25: [rank: 25] Seed set to 3767332308
 1: :::MLLOG {"namespace": "", "time_ms": 1727707179569, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 2: :::MLLOG {"namespace": "", "time_ms": 1727707179569, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 4: :::MLLOG {"namespace": "", "time_ms": 1727707179569, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 3: :::MLLOG {"namespace": "", "time_ms": 1727707179570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 7: :::MLLOG {"namespace": "", "time_ms": 1727707179570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 0: [NeMo W 2024-09-30 14:39:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
 5: :::MLLOG {"namespace": "", "time_ms": 1727707179570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707179571, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 6: :::MLLOG {"namespace": "", "time_ms": 1727707179571, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
19: :::MLLOG {"namespace": "", "time_ms": 1727707180453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
21: :::MLLOG {"namespace": "", "time_ms": 1727707180453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
18: :::MLLOG {"namespace": "", "time_ms": 1727707180454, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
20: :::MLLOG {"namespace": "", "time_ms": 1727707180455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
17: :::MLLOG {"namespace": "", "time_ms": 1727707180455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
23: :::MLLOG {"namespace": "", "time_ms": 1727707180455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
22: :::MLLOG {"namespace": "", "time_ms": 1727707180456, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
16: :::MLLOG {"namespace": "", "time_ms": 1727707180458, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/sd/main.py", "lineno": 71}}
 6: :::MLLOG {"namespace": "", "time_ms": 1727707181707, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1715304078, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 6: [rank: 6] Seed set to 1715304078
 3: :::MLLOG {"namespace": "", "time_ms": 1727707181708, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3622821745, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 3: [rank: 3] Seed set to 3622821745
 2: :::MLLOG {"namespace": "", "time_ms": 1727707181714, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1946410267, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 2: [rank: 2] Seed set to 1946410267
 5: :::MLLOG {"namespace": "", "time_ms": 1727707181747, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1694005089, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 5: [rank: 5] Seed set to 1694005089
 7: :::MLLOG {"namespace": "", "time_ms": 1727707181816, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3052926752, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 7: [rank: 7] Seed set to 3052926752
 4: :::MLLOG {"namespace": "", "time_ms": 1727707181817, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3040966262, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 4: [rank: 4] Seed set to 3040966262
 0: [NeMo I 2024-09-30 14:39:41 main:65] L2 promotion: 128 B
 0: :::MLLOG {"namespace": "", "time_ms": 1727707181828, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1297300301, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 0: [rank: 0] Seed set to 1297300301
 0: [NeMo I 2024-09-30 14:39:41 main:85] 
 0:     
 0:     ************** Experiment configuration ***********
 0: [NeMo I 2024-09-30 14:39:41 main:86] 
 0:     name: stable-diffusion2-train-240930143624620066301
 0:     trainer:
 0:       devices: 8
 0:       num_nodes: 4
 0:       accelerator: gpu
 0:       logger: false
 0:       enable_checkpointing: false
 0:       max_epochs: -1
 0:       max_steps: 4000
 0:       log_every_n_steps: 10000
 0:       accumulate_grad_batches: 1
 0:       gradient_clip_val: 1.0
 0:       benchmark: false
 0:       enable_model_summary: true
 0:     exp_manager:
 0:       exp_dir: /tmp/nemologs
 0:       name: ${name}
 0:       create_wandb_logger: false
 0:       wandb_logger_kwargs:
 0:         project: stable-diffusion
 0:         group: nemo-sd
 0:         name: ${name}
 0:         resume: true
 0:       create_checkpoint_callback: true
 0:       create_tensorboard_logger: true
 0:       checkpoint_callback_params:
 0:         every_n_train_steps: 1000
 0:         every_n_epochs: 0
 0:         monitor: timestamp
 0:         filename: ${name}--{timestamp}-{step}-{consumed_samples}
 0:         save_top_k: -1
 0:         save_last: false
 0:         save_nemo_on_train_end: false
 0:         save_weights_only: true
 0:       resume_if_exists: true
 0:       resume_ignore_no_checkpoint: true
 0:       ema:
 0:         enable: false
 0:         decay: 0.9999
 0:         validate_original_weights: false
 0:         every_n_steps: 1
 0:         cpu_offload: false
 0:       create_preemption_callback: false
 0:       log_step_timing: false
 0:     model:
 0:       precision: 16
 0:       micro_batch_size: 32
 0:       global_batch_size: 1024
 0:       linear_start: 0.00085
 0:       linear_end: 0.012
 0:       num_timesteps_cond: 1
 0:       log_every_t: 200
 0:       timesteps: 1000
 0:       first_stage_key: images_moments
 0:       cond_stage_key: clip_encoded
 0:       image_size: 64
 0:       channels: 4
 0:       cond_stage_trainable: false
 0:       conditioning_key: crossattn
 0:       monitor: val/loss_simple_ema
 0:       scale_factor: 0.18215
 0:       use_ema: false
 0:       scale_by_std: false
 0:       ckpt_path: /checkpoints/sd/512-base-ema.ckpt
 0:       load_vae: true
 0:       load_unet: false
 0:       load_encoder: true
 0:       ignore_keys: []
 0:       parameterization: v
 0:       clip_denoised: true
 0:       load_only_unet: false
 0:       cosine_s: 0.008
 0:       given_betas: null
 0:       original_elbo_weight: 0
 0:       v_posterior: 0
 0:       l_simple_weight: 1
 0:       use_positional_encodings: false
 0:       learn_logvar: false
 0:       logvar_init: 0
 0:       beta_schedule: linear
 0:       loss_type: l2
 0:       channels_last: true
 0:       concat_mode: true
 0:       cond_stage_forward: null
 0:       text_embedding_dropout_rate: 0.0
 0:       fused_opt: true
 0:       inductor: true
 0:       inductor_cudagraphs: false
 0:       capture_cudagraph_iters: 15
 0:       unet_config:
 0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
 0:         from_pretrained: null
 0:         from_NeMo: null
 0:         image_size: 32
 0:         in_channels: 4
 0:         out_channels: 4
 0:         model_channels: 320
 0:         attention_resolutions:
 0:         - 4
 0:         - 2
 0:         - 1
 0:         num_res_blocks: 2
 0:         channel_mult:
 0:         - 1
 0:         - 2
 0:         - 4
 0:         - 4
 0:         num_head_channels: 64
 0:         use_spatial_transformer: true
 0:         use_linear_in_transformer: true
 0:         transformer_depth: 1
 0:         context_dim: 1024
 0:         use_checkpoint: false
 0:         legacy: false
 0:         use_flash_attention: true
 0:         resblock_gn_groups: 16
 0:         unet_precision: fp16
 0:         timesteps: ${model.timesteps}
 0:       first_stage_config:
 0:         _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
 0:         from_pretrained: null
 0:         embed_dim: 4
 0:         monitor: val/rec_loss
 0:         ddconfig:
 0:           double_z: true
 0:           z_channels: 4
 0:           resolution: 256
 0:           in_channels: 3
 0:           out_ch: 3
 0:           ch: 128
 0:           ch_mult:
 0:           - 1
 0:           - 2
 0:           - 4
 0:           - 4
 0:           num_res_blocks: 2
 0:           attn_resolutions: []
 0:           dropout: 0.0
 0:         lossconfig:
 0:           target: torch.nn.Identity
 0:       cond_stage_config:
 0:         _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder
 0:         arch: ViT-H-14
 0:         version: laion2b_s32b_b79k
 0:         freeze: true
 0:         layer: penultimate
 0:         cache_dir: /checkpoints/clip
 0:       seed: 30346
 0:       resume_from_checkpoint: null
 0:       apex_transformer_log_level: 30
 0:       gradient_as_bucket_view: true
 0:       ddp_overlap: false
 0:       nsys_profile:
 0:         enabled: false
 0:         start_step: 10
 0:         end_step: 10
 0:         ranks:
 0:         - 0
 0:         gen_shape: false
 0:       data:
 0:         num_workers: 16
 0:         train:
 0:           dataset_path: /datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar
 0:           augmentations:
 0:             resize_smallest_side: 512
 0:             center_crop_h_w: 512, 512
 0:             horizontal_flip: false
 0:           filterings: null
 0:         webdataset:
 0:           infinite_sampler: true
 0:           local_root_path: /datasets/laion-400m/webdataset-moments-filtered-encoded
 0:       optim:
 0:         name: distributed_fused_adam
 0:         lr: 0.00012288
 0:         weight_decay: 0.0
 0:         betas:
 0:         - 0.9
 0:         - 0.999
 0:         sched:
 0:           name: WarmupHoldPolicy
 0:           warmup_steps: 1000
 0:           hold_steps: 10000000000000
 0:         bucket_cap_mb: 288
 0:         overlap_grad_sync: true
 0:         overlap_param_sync: false
 0:         contiguous_grad_buffer: true
 0:         contiguous_param_buffer: true
 0:         store_params: true
 0:         dtype: torch.float32
 0:         grad_sync_dtype: torch.float16
 0:         param_sync_dtype: torch.float16
 0:         capturable: true
 0:         distribute_within_nodes: true
 0:     
 0: [NeMo W 2024-09-30 14:39:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
 0:     
 1: :::MLLOG {"namespace": "", "time_ms": 1727707181911, "event_type": "POINT_IN_TIME", "key": "seed", "value": 54135524, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
 1: [rank: 1] Seed set to 54135524
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo E 2024-09-30 14:39:41 exp_manager:556] You are running multi-node training without SLURM handling the processes. Please note that this is not tested in NeMo and could result in errors.
 0: [NeMo W 2024-09-30 14:39:41 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.
 0: [NeMo W 2024-09-30 14:39:41 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints. Training from scratch.
 0: [NeMo I 2024-09-30 14:39:41 exp_manager:396] Experiments will be logged at /tmp/nemologs/stable-diffusion2-train-240930143624620066301
 0: [NeMo I 2024-09-30 14:39:41 exp_manager:856] TensorboardLogger has been set up
 0: [NeMo W 2024-09-30 14:39:41 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:41 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
 0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
 0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 14:39:42 megatron_init:354] Rank 0 has embedding rank: 0
 0: 24-09-30 14:39:42 - PID:875156 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 14:39:42 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': True, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in
 0: _channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filter
 0: ed-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-09-30 14:39:42 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
21: :::MLLOG {"namespace": "", "time_ms": 1727707182727, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2287505850, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
21: [rank: 21] Seed set to 2287505850
19: :::MLLOG {"namespace": "", "time_ms": 1727707182727, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1856655070, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
19: [rank: 19] Seed set to 1856655070
18: :::MLLOG {"namespace": "", "time_ms": 1727707182728, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3927225718, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
18: [rank: 18] Seed set to 3927225718
22: :::MLLOG {"namespace": "", "time_ms": 1727707182728, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2912312202, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
22: [rank: 22] Seed set to 2912312202
17: :::MLLOG {"namespace": "", "time_ms": 1727707182733, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3983369469, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
17: [rank: 17] Seed set to 3983369469
20: :::MLLOG {"namespace": "", "time_ms": 1727707182742, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1599013956, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
20: [rank: 20] Seed set to 1599013956
23: :::MLLOG {"namespace": "", "time_ms": 1727707182770, "event_type": "POINT_IN_TIME", "key": "seed", "value": 298412165, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
23: [rank: 23] Seed set to 298412165
16: :::MLLOG {"namespace": "", "time_ms": 1727707182774, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2305963854, "metadata": {"file": "/workspace/sd/main.py", "lineno": 82}}
16: [rank: 16] Seed set to 2305963854
 0: [NeMo I 2024-09-30 14:39:42 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 14:39:43 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 14:39:44 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 14:39:45 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 14:39:45 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 14:39:45 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 14:39:45 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 14:39:46 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 14:39:46 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 14:39:46 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 14:39:46 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 14:39:47 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-09-30 14:39:47 ddpm:168] Use system random generator since CUDA graph enabled
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
12: making attention of type 'vanilla' with 512 in_channels
14: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
14: making attention of type 'vanilla' with 512 in_channels
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
 9: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
11: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
15: making attention of type 'vanilla' with 512 in_channels
13: making attention of type 'vanilla' with 512 in_channels
11: making attention of type 'vanilla' with 512 in_channels
25: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
13: making attention of type 'vanilla' with 512 in_channels
25: making attention of type 'vanilla' with 512 in_channels
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
29: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
26: making attention of type 'vanilla' with 512 in_channels
27: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
28: making attention of type 'vanilla' with 512 in_channels
26: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
 1: making attention of type 'vanilla' with 512 in_channels
 3: making attention of type 'vanilla' with 512 in_channels
 2: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 2: making attention of type 'vanilla' with 512 in_channels
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 7: making attention of type 'vanilla' with 512 in_channels
 5: making attention of type 'vanilla' with 512 in_channels
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
 4: making attention of type 'vanilla' with 512 in_channels
 7: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
20: making attention of type 'vanilla' with 512 in_channels
17: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
21: making attention of type 'vanilla' with 512 in_channels
17: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
18: making attention of type 'vanilla' with 512 in_channels
19: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
18: making attention of type 'vanilla' with 512 in_channels
19: making attention of type 'vanilla' with 512 in_channels
23: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
23: making attention of type 'vanilla' with 512 in_channels
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-09-30 14:40:06 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-09-30 14:40:06 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-09-30 14:40:06 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-09-30 14:40:06 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 0: [NeMo I 2024-09-30 14:40:07 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-09-30 14:40:07 ddpm:303] Missing Keys: ['model.diffusion_model._orig_mod.time_embed.0.weight', 'model.diffusion_model._orig_mod.time_embed.0.bias', 'model.diffusion_model._orig_mod.time_embed.2.weight', 'model.diffusion_model._orig_mod.time_embed.2.bias', 'model.diffusion_model._orig_mod.input_blocks.0.0.weight', 'model.diffusion_model._orig_mod.input_blocks.0.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.weight', 'mo
 0: del.diffusion_model._orig_mod.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.tr
 0: ansformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffus
 0: ion_model._orig_mod.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.1.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1
 0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transf
 0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.2.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
 0: ocks.2.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.3.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.
 0: _orig_mod.input_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.tra
 0: nsformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_mo
 0: del._orig_mod.input_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_in.weight', 'model.dif
 0: fusion_model._orig_mod.input_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.trans
 0: former_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.6.0.op.weight', 'mo
 0: del.diffusion_model._orig_mod.input_blocks.6.0.op.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1
 0: .norm.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transf
 0: ormer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_bl
 0: ocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.in
 0: put_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.tran
 0: sformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.input_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.weight', 'model.diffusion_model._orig_mod.input_blocks.9.0.op.bias', 'model.diffusion_model._orig_mod.
 0: input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_la
 0: yers.1.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.0.out_layers.2.weight', 'model.diffusion_model._orig_mo
 0: d.middle_block.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.norm.weight', 'model.diffusion_model._orig_mod.middle_block.1.norm.bias', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_in.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_
 0: model._orig_mod.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.middle_block.1.transformer_blocks.0.norm3.bias', 'model.
 0: diffusion_model._orig_mod.middle_block.1.proj_out.weight', 'model.diffusion_model._orig_mod.middle_block.1.proj_out.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.in_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.weight', 'model.diffusion_model._orig_mod.middle_block.2.emb_layers.1.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.0.bias', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.weight', 'model.diffusion_model._orig_mod.middle_block.2.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.weight', '
 0: model.diffusion_model._orig_mod.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion
 0: _model._orig_mod.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model._orig_m
 0: od.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.2.1.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layer
 0: s.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn1.to_out.
 0: 0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diff
 0: usion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.3.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out
 0: _layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn1.t
 0: o_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'mode
 0: l.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.4.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5
 0: .0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.a
 0: ttn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm1.bias',
 0:  'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.5.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.5.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.emb_lay
 0: ers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.ou
 0: tput_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.outp
 0: ut_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.6.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.e
 0: mb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_
 0: mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mo
 0: d.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.7.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
 0: .8.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.
 0: _orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._o
 0: rig_mod.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.8.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.weight', 'model.diffusion_model._orig_mod.output_blocks.8.2.conv.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.in_l
 0: ayers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.a
 0: ttn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_
 0: out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.9.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.in_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks
 0: .10.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.
 0: 1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.
 0: 10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.10.1.proj_out.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.weight', 'm
 0: odel.diffusion_model._orig_mod.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model._orig_mod.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.norm.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_in.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model
 0: .diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'mo
 0: del.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.weight', 'model.diffusion_model._orig_mod.output_blocks.11.1.proj_out.bias', 'model.diffusion_model._orig_mod.out.0.weight', 'model.diffusion_model._orig_mod.out.0.bias', 'model.diffusion_model._orig_mod.out.1.weight', 'model.diffusion_model._orig_mod.
 0: out.1.bias']
 0: [NeMo I 2024-09-30 14:40:07 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 7: [rank: 7] Seed set to 3052926752
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/32
 2: [rank: 2] Seed set to 1946410267
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/32
 1: [rank: 1] Seed set to 54135524
 3: [rank: 3] Seed set to 3622821745
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/32
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/32
 4: [rank: 4] Seed set to 3040966262
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/32
 5: [rank: 5] Seed set to 1694005089
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/32
 0: [NeMo W 2024-09-30 14:40:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:181: You have overridden `MegatronLatentDiffusion.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
 0:     
 0: [NeMo W 2024-09-30 14:40:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:163: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
 0:     
 0: [rank: 0] Seed set to 1297300301
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/32
 6: [rank: 6] Seed set to 1715304078
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/32
 8: [rank: 8] Seed set to 828203007
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/32
24: [rank: 24] Seed set to 1179649059
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/32
10: [rank: 10] Seed set to 2155027037
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/32
12: [rank: 12] Seed set to 1889092595
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/32
14: [rank: 14] Seed set to 3113561360
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/32
25: [rank: 25] Seed set to 3767332308
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/32
 9: [rank: 9] Seed set to 2805580309
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/32
31: [rank: 31] Seed set to 3267527404
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/32
29: [rank: 29] Seed set to 456176175
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/32
15: [rank: 15] Seed set to 294679604
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/32
11: [rank: 11] Seed set to 1787326884
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/32
26: [rank: 26] Seed set to 3334560731
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/32
28: [rank: 28] Seed set to 771632000
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/32
13: [rank: 13] Seed set to 1241603319
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/32
30: [rank: 30] Seed set to 64987781
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/32
27: [rank: 27] Seed set to 352317957
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/32
16: [rank: 16] Seed set to 2305963854
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/32
20: [rank: 20] Seed set to 1599013956
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/32
17: [rank: 17] Seed set to 3983369469
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/32
21: [rank: 21] Seed set to 2287505850
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/32
19: [rank: 19] Seed set to 1856655070
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/32
22: [rank: 22] Seed set to 2912312202
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/32
18: [rank: 18] Seed set to 3927225718
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/32
23: [rank: 23] Seed set to 298412165
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/32
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 32 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: [NeMo I 2024-09-30 14:40:24 main:199] Warmup allreduce with communicator at 710fbbfd4870, size 4
 0: NCCL version 2.21.5+cuda12.4
 5: NCCL version 2.21.5+cuda12.4
 2: NCCL version 2.21.5+cuda12.4
 1: NCCL version 2.21.5+cuda12.4
 4: NCCL version 2.21.5+cuda12.4
 3: NCCL version 2.21.5+cuda12.4
 7: NCCL version 2.21.5+cuda12.4
 6: NCCL version 2.21.5+cuda12.4
 8: NCCL version 2.21.5+cuda12.4
10: NCCL version 2.21.5+cuda12.4
14: NCCL version 2.21.5+cuda12.4
 9: NCCL version 2.21.5+cuda12.4
12: NCCL version 2.21.5+cuda12.4
15: NCCL version 2.21.5+cuda12.4
11: NCCL version 2.21.5+cuda12.4
13: NCCL version 2.21.5+cuda12.4
16: NCCL version 2.21.5+cuda12.4
17: NCCL version 2.21.5+cuda12.4
20: NCCL version 2.21.5+cuda12.4
21: NCCL version 2.21.5+cuda12.4
31: NCCL version 2.21.5+cuda12.4
18: NCCL version 2.21.5+cuda12.4
23: NCCL version 2.21.5+cuda12.4
25: NCCL version 2.21.5+cuda12.4
24: NCCL version 2.21.5+cuda12.4
22: NCCL version 2.21.5+cuda12.4
29: NCCL version 2.21.5+cuda12.4
19: NCCL version 2.21.5+cuda12.4
27: NCCL version 2.21.5+cuda12.4
28: NCCL version 2.21.5+cuda12.4
30: NCCL version 2.21.5+cuda12.4
26: NCCL version 2.21.5+cuda12.4
 0: [NeMo I 2024-09-30 14:40:42 ddpm:1977] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 8.66e+08. Total number of model parameters: 8.66e+08.
 0: [NeMo I 2024-09-30 14:40:42 ddpm:2005] Building datasets for Stable Diffusion...
 0: [NeMo I 2024-09-30 14:40:42 webdataset:145] Read Webdataset locally. Data stores at /datasets/laion-400m/webdataset-moments-filtered-encoded
 0: [NeMo I 2024-09-30 14:40:42 webdataset:221] Setting nbatches=1625 for infinite sampler. world_size=32
 0: [NeMo I 2024-09-30 14:40:42 webdataset:224] Total number of training shards: 832
 0: [NeMo I 2024-09-30 14:40:42 webdataset:225] Total training key count: 832000
 0: [NeMo I 2024-09-30 14:40:42 ddpm:2025] Length of train dataset: 832000
 0: [NeMo I 2024-09-30 14:40:42 ddpm:2030] Finished building datasets for LatentDiffusion.
 0: [NeMo I 2024-09-30 14:40:42 ddpm:2036] Setting up train dataloader with len(len(self._train_ds)): 832000 and consumed samples: 0
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: [NeMo I 2024-09-30 14:40:44 modelPT:724] Optimizer config = MegatronDistributedFusedAdam (
 0:     Parameter Group 0
 0:         betas: [0.9, 0.999]
 0:         bias_correction: True
 0:         eps: 1e-08
 0:         lr: 0.00012287999561522156
 0:         weight_decay: 0.0
 0:     adam_w_mode: True
 0:     amsgrad: False
 0:     dtype: torch.float32
 0:     grad_sync_dtype: torch.float16
 0:     param_sync_dtype: torch.float16
 0:     device: cuda:0
 0:     process_group: 0x710fb95502f0, world size 32
 0:     distributed_process_group: 0x710fb95538b0, world size 8
 0:     redundant_process_group: 0x710fbbfd4870, world size 4
 0:     average_grad_sync: True
 0:     overlap_grad_sync: True
 0:     overlap_param_sync: False
 0:     bucket_cap_mb: 288
 0:     pipeline_size: 2
 0:     contiguous_param_buffer: True
 0:     contiguous_grad_buffer: True
 0:     store_params: True
 0:     store_param_remainders: False
 0:     with_scaled_states: False
 0:     nccl_ub: False
 0:     capturable: True
 0:     )
 0: [NeMo I 2024-09-30 14:40:44 lr_scheduler:923] Scheduler "<nemo.core.optim.lr_scheduler.WarmupHoldPolicy object at 0x710f5befeb30>" 
 0:     will be used during training (effective maximum steps = 4000) - 
 0:     Parameters : 
 0:     (warmup_steps: 1000
 0:     hold_steps: 10000000000000
 0:     max_steps: 4000
 0:     )
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245173, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 249}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 1024, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 253}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adamw", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 257}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_1", "value": 0.9, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 258}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "opt_adamw_beta_2", "value": 0.999, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 259}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "opt_adamw_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 260}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245295, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.01, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 261}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00012288, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 263}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 1000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 264}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 6513144, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 269}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 30000, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 270}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "stable_diffusion", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707245296, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "4xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 272}}
 0: 
 0:   | Name  | Type            | Params
 0: ------------------------------------------
 0: 0 | model | LatentDiffusion | 865 M 
 0: ------------------------------------------
 0: 865 M     Trainable params
 0: 0         Non-trainable params
 0: 865 M     Total params
 0: 3,463.643 Total estimated model params size (MB)
17: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
 0: [NeMo W 2024-09-30 14:40:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:104: Total length of `list` across ranks is zero. Please make sure this was your intention.
 0:     
 0: [NeMo W 2024-09-30 14:40:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:121: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.
 0:     
 0: CUDAGraphCallback: disable autocast cache.
 0: [NeMo W 2024-09-30 14:44:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:414: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:       warnings.warn(
 0:     
 0: [NeMo W 2024-09-30 14:44:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('global_step', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'global_step': ...})` instead.
 0:     
 0: [NeMo W 2024-09-30 14:44:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:213: You called `self.log('consumed_samples', ...)` in your `optimizer_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'consumed_samples': ...})` instead.
 0:     
 0: [NeMo I 2024-09-30 14:44:53 callbacks:158] CUDAGraphCallback: capturing CUDA graph for module MegatronLatentDiffusion.
 0: CUDAGraphCallback: set optimizer.zero_grad as nop during graph capturing.
 0: :::MLLOG {"namespace": "", "time_ms": 1727707505203, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707505205, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 359}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707505206, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707533284, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 102400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707533286, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 102400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707560479, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 204800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707560480, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 204800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707587745, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 307200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707587746, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 307200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707614989, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 409600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707614990, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 409600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707642158, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 512000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707642159, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 512000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707669402, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 614400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707669403, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 614400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707696676, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 716800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707696676, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 716800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707723885, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 819200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707723987, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 819200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707757663, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 921600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707757665, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 921600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707784933, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1024000}}
 0: Epoch 1, global step 1000: 'timestamp' reached 1727707784933.00000 (best 1727707784933.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt' as top 1
 0: Saving /tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1727707786612, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1024000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707812551, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1126400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707812552, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1126400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707839792, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1228800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707839793, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1228800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707867027, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1331200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707867028, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1331200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707894238, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1433600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707894239, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1433600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707921491, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1536000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707921492, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1536000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707948709, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1638400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707948892, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1638400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707980892, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1740800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727707980893, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1740800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708008132, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1843200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708008133, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1843200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708035331, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 1945600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708035332, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 1945600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708062582, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2048000}}
 0: Epoch 2, global step 2000: 'timestamp' reached 1727708062581.00000 (best 1727707784933.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt' as top 2
 0: Saving /tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1727708064318, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2048000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708090094, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2150400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708090095, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2150400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708117310, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2252800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708117311, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2252800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708144553, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2355200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708144554, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2355200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708171767, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2457600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708171993, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2457600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708202042, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2560000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708202044, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2560000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708229251, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2662400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708229252, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2662400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708256465, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2764800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708256466, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2764800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708283707, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2867200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708283708, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2867200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708310918, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 2969600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708310920, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 2969600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708338161, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3072000}}
 0: Epoch 3, global step 3000: 'timestamp' reached 1727708338160.00000 (best 1727707784933.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt' as top 3
 0: Saving /tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt in the background
 0: :::MLLOG {"namespace": "", "time_ms": 1727708339888, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3072000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708365645, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3174400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708365646, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3174400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708392855, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3276800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708395981, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3276800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708422738, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3379200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708422739, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3379200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708449971, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3481600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708449972, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3481600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708477252, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3584000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708477253, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3584000}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708504503, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3686400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708504504, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3686400}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708531742, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3788800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708531744, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3788800}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708558977, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3891200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708558978, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3891200}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708586220, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 3993600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708586221, "event_type": "INTERVAL_START", "key": "block_start", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 369, "samples_count": 3993600}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727708613451, "event_type": "INTERVAL_END", "key": "block_stop", "value": "training_step", "metadata": {"file": "/workspace/sd/callbacks.py", "lineno": 403, "samples_count": 4096000}}
 0: Epoch 4, global step 4000: 'timestamp' reached 1727708613451.00000 (best 1727707784933.00000), saving model to '/tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt' as top 4
 0: Saving /tmp/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt in the background
 0: `Trainer.fit` stopped: `max_steps=4000` reached.
 0: Moving checkpoints to nemologs
 0: total 0
 0: drwxrwxr-x 5 root root 200 Sep 30 14:49 stable-diffusion2-train-240930143624620066301
 0: NCCL version 2.21.5+cuda12.4
21: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
25: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 1: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 4: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
17: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
13: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 6: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
26: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
19: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
15: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
31: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
20: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
11: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 7: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
28: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 3: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
22: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
30: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 2: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
18: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
14: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
16: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
29: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 9: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
10: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
27: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 0: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 8: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
24: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
12: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
 5: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
23: CKPT_PATH=/nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/
21: FlashAttention Installed
25: FlashAttention Installed
 1: FlashAttention Installed
26: FlashAttention Installed
 4: FlashAttention Installed
17: FlashAttention Installed
 6: FlashAttention Installed
20: FlashAttention Installed
19: FlashAttention Installed
13: FlashAttention Installed
11: FlashAttention Installed
15: FlashAttention Installed
 2: FlashAttention Installed
28: FlashAttention Installed
31: FlashAttention Installed
18: FlashAttention Installed
30: FlashAttention Installed
16: FlashAttention Installed
21: Loaded ViT-H-14 model config.
 4: Loaded ViT-H-14 model config.
22: FlashAttention Installed
14: FlashAttention Installed
 1: Loaded ViT-H-14 model config.
17: Loaded ViT-H-14 model config.
 6: Loaded ViT-H-14 model config.
23: FlashAttention Installed
 7: FlashAttention Installed
 3: FlashAttention Installed
 0: FlashAttention Installed
 0: [NeMo W 2024-09-30 15:04:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
 0:     See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
 0:       ret = run_job(
 0:     
 0: [NeMo W 2024-09-30 15:04:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
 0:       warnings.warn(
 0:     
 0: [NeMo W 2024-09-30 15:04:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
 0:       warnings.warn(msg)
 0:     
20: Loaded ViT-H-14 model config.
26: Loaded ViT-H-14 model config.
19: Loaded ViT-H-14 model config.
25: Loaded ViT-H-14 model config.
12: FlashAttention Installed
24: FlashAttention Installed
29: FlashAttention Installed
27: FlashAttention Installed
 5: FlashAttention Installed
 8: FlashAttention Installed
13: Loaded ViT-H-14 model config.
10: FlashAttention Installed
11: Loaded ViT-H-14 model config.
 9: FlashAttention Installed
 2: Loaded ViT-H-14 model config.
15: Loaded ViT-H-14 model config.
16: Loaded ViT-H-14 model config.
28: Loaded ViT-H-14 model config.
31: Loaded ViT-H-14 model config.
18: Loaded ViT-H-14 model config.
30: Loaded ViT-H-14 model config.
22: Loaded ViT-H-14 model config.
23: Loaded ViT-H-14 model config.
14: Loaded ViT-H-14 model config.
 7: Loaded ViT-H-14 model config.
24: Loaded ViT-H-14 model config.
 0: Loaded ViT-H-14 model config.
 3: Loaded ViT-H-14 model config.
29: Loaded ViT-H-14 model config.
27: Loaded ViT-H-14 model config.
12: Loaded ViT-H-14 model config.
 8: Loaded ViT-H-14 model config.
 5: Loaded ViT-H-14 model config.
10: Loaded ViT-H-14 model config.
 9: Loaded ViT-H-14 model config.
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Using 16bit Automatic Mixed Precision (AMP)
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo W 2024-09-30 15:04:56 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
18: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
17: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 1: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 6: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 5: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: Found checkpoints:
 0: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 0: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:05:04 megatron_init:354] Rank 0 has embedding rank: 0
 3: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: 24-09-30 15:05:04 - PID:925939 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:05:04 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-09-30 15:05:04 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:04 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:05 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:06 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:07 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:07 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:05:07 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:05:07 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:05:08 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:05:08 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:05:08 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:05:08 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 9: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:05:08 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-09-30 15:05:08 ddpm:168] Use system random generator since CUDA graph enabled
20: Found checkpoints:
20: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
20: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
20: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
20: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
20: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
16: Found checkpoints:
16: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
16: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
16: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
16: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
23: Found checkpoints:
23: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
23: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
23: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
23: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
23: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
21: Found checkpoints:
21: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
21: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
21: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
21: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
21: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
19: Found checkpoints:
19: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
19: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
19: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
19: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
19: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
17: Found checkpoints:
17: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
17: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
17: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
17: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
17: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: making attention of type 'vanilla' with 512 in_channels
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
18: Found checkpoints:
18: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
18: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
18: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
18: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
18: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
 5: Found checkpoints:
 5: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 5: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
22: Found checkpoints:
22: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
22: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
22: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
22: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
22: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
 7: Found checkpoints:
 7: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 7: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 7: making attention of type 'vanilla' with 512 in_channels
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: making attention of type 'vanilla' with 512 in_channels
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 4: Found checkpoints:
 4: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 4: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 4: making attention of type 'vanilla' with 512 in_channels
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 1: Found checkpoints:
 1: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 1: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 1: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
 6: Found checkpoints:
 6: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 6: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
 2: Found checkpoints:
 2: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 2: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 2: making attention of type 'vanilla' with 512 in_channels
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
 3: Found checkpoints:
 3: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 3: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 3: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
13: Found checkpoints:
13: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
13: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
13: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
13: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
13: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
 9: Found checkpoints:
 9: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 9: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 9: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
14: Found checkpoints:
14: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
14: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
14: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
14: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
14: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
 8: Found checkpoints:
 8: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
 8: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
15: Found checkpoints:
15: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
15: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
15: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
15: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
15: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
30: Found checkpoints:
30: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
30: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
30: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
30: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
30: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
10: Found checkpoints:
10: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
10: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
10: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
10: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
29: Found checkpoints:
29: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
29: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
29: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
29: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
29: making attention of type 'vanilla' with 512 in_channels
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
24: Found checkpoints:
24: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
24: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
24: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
24: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
12: Found checkpoints:
12: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
12: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
12: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
12: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
12: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
28: Found checkpoints:
28: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
28: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
28: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
28: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
28: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
26: Found checkpoints:
26: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
26: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
26: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
26: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
26: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
31: Found checkpoints:
31: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
31: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
31: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
31: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
31: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
11: Found checkpoints:
11: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
11: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
11: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
11: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
11: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
27: Found checkpoints:
27: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
27: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
27: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
27: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
25: Found checkpoints:
25: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727707784933.0-step=1000-consumed_samples=1024000.0.ckpt
25: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708062581.0-step=2000-consumed_samples=2048000.0.ckpt
25: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708338160.0-step=3000-consumed_samples=3072000.0.ckpt
25: /nemologs/stable-diffusion2-train-240930143624620066301/checkpoints/stable-diffusion2-train-240930143624620066301--timestamp=1727708613451.0-step=4000-consumed_samples=4096000.0.ckpt
25: making attention of type 'vanilla' with 512 in_channels
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: making attention of type 'vanilla' with 512 in_channels
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-09-30 15:05:18 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-09-30 15:05:18 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-09-30 15:05:18 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-09-30 15:05:18 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-09-30 15:05:19 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-09-30 15:05:19 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
 0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
 0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
 0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
 0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
 0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
 0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
 0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
 0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
 0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
 0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
 0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
 0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
 0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
 0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
 0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
 0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
 0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
 0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
 0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
 0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
 0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
 0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
 0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
 0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
 0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
 0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
 0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
 0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
 0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
 0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
 0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
 0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
 0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
 0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
 0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
 0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
 0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
 0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
 0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
 0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
 0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
 0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
 0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
 0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
 0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
 0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
 0: [NeMo I 2024-09-30 15:05:19 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/32
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/32
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/32
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/32
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/32
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/32
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/32
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/32
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/32
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/32
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/32
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/32
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/32
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/32
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/32
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/32
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/32
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/32
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/32
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/32
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/32
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/32
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/32
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/32
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/32
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/32
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/32
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/32
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/32
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/32
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/32
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/32
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 32 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 0: Global ID: 0, local ID: 0, world size: 32
 0: Rank 0 before barrier
 0: NCCL version 2.21.5+cuda12.4
 1: Global ID: 1, local ID: 1, world size: 32
 1: Rank 1 before barrier
 2: Global ID: 2, local ID: 2, world size: 32
 2: Rank 2 before barrier
 3: Global ID: 3, local ID: 3, world size: 32
 3: Rank 3 before barrier
 4: Global ID: 4, local ID: 4, world size: 32
 4: Rank 4 before barrier
 5: Global ID: 5, local ID: 5, world size: 32
 5: Rank 5 before barrier
 6: Global ID: 6, local ID: 6, world size: 32
 6: Rank 6 before barrier
 7: Global ID: 7, local ID: 7, world size: 32
 7: Rank 7 before barrier
 8: Global ID: 8, local ID: 0, world size: 32
 8: Rank 8 before barrier
 9: Global ID: 9, local ID: 1, world size: 32
 9: Rank 9 before barrier
10: Global ID: 10, local ID: 2, world size: 32
10: Rank 10 before barrier
11: Global ID: 11, local ID: 3, world size: 32
11: Rank 11 before barrier
12: Global ID: 12, local ID: 4, world size: 32
12: Rank 12 before barrier
13: Global ID: 13, local ID: 5, world size: 32
13: Rank 13 before barrier
14: Global ID: 14, local ID: 6, world size: 32
14: Rank 14 before barrier
15: Global ID: 15, local ID: 7, world size: 32
15: Rank 15 before barrier
16: Global ID: 16, local ID: 0, world size: 32
16: Rank 16 before barrier
17: Global ID: 17, local ID: 1, world size: 32
17: Rank 17 before barrier
18: Global ID: 18, local ID: 2, world size: 32
18: Rank 18 before barrier
19: Global ID: 19, local ID: 3, world size: 32
19: Rank 19 before barrier
20: Global ID: 20, local ID: 4, world size: 32
20: Rank 20 before barrier
21: Global ID: 21, local ID: 5, world size: 32
21: Rank 21 before barrier
22: Global ID: 22, local ID: 6, world size: 32
22: Rank 22 before barrier
23: Global ID: 23, local ID: 7, world size: 32
23: Rank 23 before barrier
24: Global ID: 24, local ID: 0, world size: 32
24: Rank 24 before barrier
25: Global ID: 25, local ID: 1, world size: 32
25: Rank 25 before barrier
26: Global ID: 26, local ID: 2, world size: 32
26: Rank 26 before barrier
27: Global ID: 27, local ID: 3, world size: 32
27: Rank 27 before barrier
28: Global ID: 28, local ID: 4, world size: 32
28: Rank 28 before barrier
29: Global ID: 29, local ID: 5, world size: 32
29: Rank 29 before barrier
30: Global ID: 30, local ID: 6, world size: 32
30: Rank 30 before barrier
31: Global ID: 31, local ID: 7, world size: 32
31: Rank 31 before barrier
 0: :::MLLOG {"namespace": "", "time_ms": 1727708741614, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 1024000}}
 0: Assigned 938 prompts for this worker.
 0: :::MLLOG {"namespace": "", "time_ms": 1727709032038, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 1024000}}
 3: Assigned 938 prompts for this worker.
 4: Assigned 938 prompts for this worker.
17: Assigned 937 prompts for this worker.
 2: Assigned 938 prompts for this worker.
 5: Assigned 938 prompts for this worker.
 6: Assigned 938 prompts for this worker.
 7: Assigned 938 prompts for this worker.
29: Assigned 937 prompts for this worker.
10: Assigned 938 prompts for this worker.
21: Assigned 937 prompts for this worker.
18: Assigned 937 prompts for this worker.
19: Assigned 937 prompts for this worker.
20: Assigned 937 prompts for this worker.
22: Assigned 937 prompts for this worker.
23: Assigned 937 prompts for this worker.
16: Assigned 937 prompts for this worker.
26: Assigned 937 prompts for this worker.
24: Assigned 937 prompts for this worker.
27: Assigned 937 prompts for this worker.
30: Assigned 937 prompts for this worker.
28: Assigned 937 prompts for this worker.
31: Assigned 937 prompts for this worker.
 9: Assigned 938 prompts for this worker.
12: Assigned 938 prompts for this worker.
13: Assigned 938 prompts for this worker.
14: Assigned 938 prompts for this worker.
 8: Assigned 938 prompts for this worker.
15: Assigned 938 prompts for this worker.
11: Assigned 938 prompts for this worker.
 1: Assigned 938 prompts for this worker.
25: Assigned 937 prompts for this worker.
 0: Calculating FID activations:   0%|          | 0/30 [00:00<?, ?it/s]Calculating FID activations:   3%|▎         | 1/30 [00:01<00:33,  1.16s/it]Calculating FID activations:  33%|███▎      | 10/30 [00:01<00:01, 10.32it/s]Calculating FID activations:  50%|█████     | 15/30 [00:01<00:00, 15.09it/s]Calculating FID activations:  67%|██████▋   | 20/30 [00:01<00:00, 19.72it/s]Calculating FID activations:  83%|████████▎ | 25/30 [00:01<00:00, 23.96it/s]Calculating FID activations: 100%|██████████| 30/30 [00:01<00:00, 25.56it/s]Calculating FID activations: 100%|██████████| 30/30 [00:02<00:00, 14.22it/s]
 0: Computed feature activations of size torch.Size([938, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1727709042467, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 224.72231300677026, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 1024000, "metric": "FID"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727709058355, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.07193024456501007, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 1024000, "metric": "CLIP"}}
 0: Using 16bit Automatic Mixed Precision (AMP)
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo W 2024-09-30 15:10:58 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 3: [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 7: [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:287] Rank 0 has context parallel group: [0]
 2: [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:11:08 megatron_init:354] Rank 0 has embedding rank: 0
 1: [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:11:08 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-09-30 15:11:08 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 8: [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:11:08 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
17: [rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:11:09 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:11:10 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:11:11 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:11:11 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:11:11 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:11:11 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:11:12 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:11:12 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:11:12 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:11:12 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:11:12 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-09-30 15:11:12 ddpm:168] Use system random generator since CUDA graph enabled
 7: Computed feature activations of size torch.Size([938, 2048])
 7: making attention of type 'vanilla' with 512 in_channels
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: making attention of type 'vanilla' with 512 in_channels
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 2: Computed feature activations of size torch.Size([938, 2048])
 2: making attention of type 'vanilla' with 512 in_channels
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
 6: Computed feature activations of size torch.Size([938, 2048])
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
 4: Computed feature activations of size torch.Size([938, 2048])
 4: making attention of type 'vanilla' with 512 in_channels
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 5: Computed feature activations of size torch.Size([938, 2048])
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
 3: Computed feature activations of size torch.Size([938, 2048])
 3: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
13: Computed feature activations of size torch.Size([938, 2048])
13: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
10: Computed feature activations of size torch.Size([938, 2048])
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
15: Computed feature activations of size torch.Size([938, 2048])
15: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
11: Computed feature activations of size torch.Size([938, 2048])
11: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
14: Computed feature activations of size torch.Size([938, 2048])
14: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
 9: Computed feature activations of size torch.Size([938, 2048])
 9: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
 8: Computed feature activations of size torch.Size([938, 2048])
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
16: Computed feature activations of size torch.Size([937, 2048])
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
29: Computed feature activations of size torch.Size([937, 2048])
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
29: making attention of type 'vanilla' with 512 in_channels
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
20: Computed feature activations of size torch.Size([937, 2048])
20: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
22: Computed feature activations of size torch.Size([937, 2048])
22: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
17: Computed feature activations of size torch.Size([937, 2048])
17: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: making attention of type 'vanilla' with 512 in_channels
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
30: Computed feature activations of size torch.Size([937, 2048])
30: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
28: Computed feature activations of size torch.Size([937, 2048])
28: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
26: Computed feature activations of size torch.Size([937, 2048])
26: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
31: Computed feature activations of size torch.Size([937, 2048])
31: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
19: Computed feature activations of size torch.Size([937, 2048])
19: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
23: Computed feature activations of size torch.Size([937, 2048])
23: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
18: Computed feature activations of size torch.Size([937, 2048])
18: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
21: Computed feature activations of size torch.Size([937, 2048])
21: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
24: Computed feature activations of size torch.Size([937, 2048])
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
12: Computed feature activations of size torch.Size([938, 2048])
12: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
 1: Computed feature activations of size torch.Size([938, 2048])
 1: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
27: Computed feature activations of size torch.Size([937, 2048])
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
25: Computed feature activations of size torch.Size([937, 2048])
25: making attention of type 'vanilla' with 512 in_channels
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: making attention of type 'vanilla' with 512 in_channels
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-09-30 15:11:22 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-09-30 15:11:22 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-09-30 15:11:22 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-09-30 15:11:22 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 0: [NeMo I 2024-09-30 15:11:22 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-09-30 15:11:22 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
 0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
 0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
 0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
 0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
 0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
 0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
 0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
 0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
 0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
 0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
 0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
 0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
 0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
 0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
 0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
 0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
 0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
 0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
 0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
 0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
 0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
 0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
 0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
 0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
 0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
 0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
 0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
 0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
 0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
 0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
 0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
 0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
 0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
 0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
 0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
 0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
 0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
 0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
 0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
 0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
 0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
 0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
 0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
 0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
 0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
 0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
 0: [NeMo I 2024-09-30 15:11:22 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 0: Global ID: 0, local ID: 0, world size: 32
 0: Rank 0 before barrier
 1: Global ID: 1, local ID: 1, world size: 32
 1: Rank 1 before barrier
 2: Global ID: 2, local ID: 2, world size: 32
 2: Rank 2 before barrier
 3: Global ID: 3, local ID: 3, world size: 32
 3: Rank 3 before barrier
 4: Global ID: 4, local ID: 4, world size: 32
 4: Rank 4 before barrier
 5: Global ID: 5, local ID: 5, world size: 32
 5: Rank 5 before barrier
 6: Global ID: 6, local ID: 6, world size: 32
 6: Rank 6 before barrier
 7: Global ID: 7, local ID: 7, world size: 32
 7: Rank 7 before barrier
 8: Global ID: 8, local ID: 0, world size: 32
 8: Rank 8 before barrier
 9: Global ID: 9, local ID: 1, world size: 32
 9: Rank 9 before barrier
10: Global ID: 10, local ID: 2, world size: 32
10: Rank 10 before barrier
11: Global ID: 11, local ID: 3, world size: 32
11: Rank 11 before barrier
12: Global ID: 12, local ID: 4, world size: 32
12: Rank 12 before barrier
13: Global ID: 13, local ID: 5, world size: 32
13: Rank 13 before barrier
14: Global ID: 14, local ID: 6, world size: 32
14: Rank 14 before barrier
15: Global ID: 15, local ID: 7, world size: 32
15: Rank 15 before barrier
16: Global ID: 16, local ID: 0, world size: 32
16: Rank 16 before barrier
17: Global ID: 17, local ID: 1, world size: 32
17: Rank 17 before barrier
18: Global ID: 18, local ID: 2, world size: 32
18: Rank 18 before barrier
19: Global ID: 19, local ID: 3, world size: 32
19: Rank 19 before barrier
20: Global ID: 20, local ID: 4, world size: 32
20: Rank 20 before barrier
21: Global ID: 21, local ID: 5, world size: 32
21: Rank 21 before barrier
22: Global ID: 22, local ID: 6, world size: 32
22: Rank 22 before barrier
23: Global ID: 23, local ID: 7, world size: 32
23: Rank 23 before barrier
24: Global ID: 24, local ID: 0, world size: 32
24: Rank 24 before barrier
25: Global ID: 25, local ID: 1, world size: 32
25: Rank 25 before barrier
26: Global ID: 26, local ID: 2, world size: 32
26: Rank 26 before barrier
27: Global ID: 27, local ID: 3, world size: 32
27: Rank 27 before barrier
28: Global ID: 28, local ID: 4, world size: 32
28: Rank 28 before barrier
29: Global ID: 29, local ID: 5, world size: 32
29: Rank 29 before barrier
30: Global ID: 30, local ID: 6, world size: 32
30: Rank 30 before barrier
31: Global ID: 31, local ID: 7, world size: 32
31: Rank 31 before barrier
 0: :::MLLOG {"namespace": "", "time_ms": 1727709088403, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 2048000}}
 0: Assigned 938 prompts for this worker.
 0: :::MLLOG {"namespace": "", "time_ms": 1727709377865, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 2048000}}
 2: Assigned 938 prompts for this worker.
 6: Assigned 938 prompts for this worker.
 5: Assigned 938 prompts for this worker.
 3: Assigned 938 prompts for this worker.
 4: Assigned 938 prompts for this worker.
10: Assigned 938 prompts for this worker.
 1: Assigned 938 prompts for this worker.
 7: Assigned 938 prompts for this worker.
26: Assigned 937 prompts for this worker.
18: Assigned 937 prompts for this worker.
29: Assigned 937 prompts for this worker.
31: Assigned 937 prompts for this worker.
27: Assigned 937 prompts for this worker.
30: Assigned 937 prompts for this worker.
24: Assigned 937 prompts for this worker.
28: Assigned 937 prompts for this worker.
25: Assigned 937 prompts for this worker.
12: Assigned 938 prompts for this worker.
 8: Assigned 938 prompts for this worker.
14: Assigned 938 prompts for this worker.
 9: Assigned 938 prompts for this worker.
11: Assigned 938 prompts for this worker.
15: Assigned 938 prompts for this worker.
13: Assigned 938 prompts for this worker.
20: Assigned 937 prompts for this worker.
17: Assigned 937 prompts for this worker.
16: Assigned 937 prompts for this worker.
23: Assigned 937 prompts for this worker.
21: Assigned 937 prompts for this worker.
19: Assigned 937 prompts for this worker.
22: Assigned 937 prompts for this worker.
 0: Calculating FID activations:   0%|          | 0/30 [00:00<?, ?it/s]Calculating FID activations:   3%|▎         | 1/30 [00:00<00:19,  1.52it/s]Calculating FID activations:  20%|██        | 6/30 [00:00<00:02, 10.06it/s]Calculating FID activations:  30%|███       | 9/30 [00:00<00:01, 13.85it/s]Calculating FID activations:  43%|████▎     | 13/30 [00:00<00:00, 18.96it/s]Calculating FID activations:  57%|█████▋    | 17/30 [00:01<00:00, 23.26it/s]Calculating FID activations:  70%|███████   | 21/30 [00:01<00:00, 26.28it/s]Calculating FID activations:  83%|████████▎ | 25/30 [00:01<00:00, 28.27it/s]Calculating FID activations: 100%|██████████| 30/30 [00:01<00:00, 31.17it/s]Calculating FID activations: 100%|██████████| 30/30 [00:01<00:00, 16.19it/s]
 0: Computed feature activations of size torch.Size([938, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1727709385206, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 100.17490830250324, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 2048000, "metric": "FID"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727709400979, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.160847470164299, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 2048000, "metric": "CLIP"}}
 0: Using 16bit Automatic Mixed Precision (AMP)
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: IPU available: False, using: 0 IPUs
 0: HPU available: False, using: 0 HPUs
 0: [NeMo W 2024-09-30 15:16:40 utils:296] Loading from .ckpt checkpoint for inference is experimental! It doesn't support models with model parallelism!
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 4: [rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 5: [rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 1: [rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:265] Rank 0 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:271] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
 6: [rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:276] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:279] Ranks 0 has data parallel rank: 0
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:287] Rank 0 has context parallel group: [0]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:290] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:291] Ranks 0 has context parallel rank: 0
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:298] Rank 0 has model parallel group: [0]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:299] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:308] Rank 0 has tensor model parallel group: [0]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:312] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:313] Rank 0 has tensor model parallel rank: 0
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:333] Rank 0 has pipeline model parallel group: [0]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:345] Rank 0 has embedding group: [0]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:351] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:352] Rank 0 has pipeline model parallel rank 0
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:353] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]
 0: [NeMo I 2024-09-30 15:16:50 megatron_init:354] Rank 0 has embedding rank: 0
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tensor_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: tp_comm_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1150] The model: MegatronLatentDiffusion() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
 0: [NeMo W 2024-09-30 15:16:50 megatron_base_model:1161] hidden_size not found in {'precision': 16, 'micro_batch_size': 32, 'global_batch_size': 1024, 'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'images_moments', 'cond_stage_key': 'clip_encoded', 'image_size': 64, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'scale_by_std': False, 'ckpt_path': '/checkpoints/sd/512-base-ema.ckpt', 'load_vae': True, 'load_unet': False, 'load_encoder': True, 'ignore_keys': [], 'parameterization': 'v', 'clip_denoised': True, 'load_only_unet': False, 'cosine_s': 0.008, 'given_betas': None, 'original_elbo_weight': 0, 'v_posterior': 0, 'l_simple_weight': 1, 'use_positional_encodings': False, 'learn_logvar': False, 'logvar_init': 0, 'beta_schedule': 'linear', 'loss_type': 'l2', 'channels_last': True, 'concat_mode': True, 'cond_stage_forward': N
 0: one, 'text_embedding_dropout_rate': 0.0, 'fused_opt': True, 'inductor': False, 'inductor_cudagraphs': False, 'capture_cudagraph_iters': 15, 'unet_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel', 'from_pretrained': None, 'from_NeMo': None, 'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_head_channels': 64, 'use_spatial_transformer': True, 'use_linear_in_transformer': True, 'transformer_depth': 1, 'context_dim': 1024, 'use_checkpoint': False, 'legacy': False, 'use_flash_attention': True, 'resblock_gn_groups': 16, 'unet_precision': 'fp16', 'timesteps': 1000}, 'first_stage_config': {'_target_': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL', 'from_pretrained': None, 'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'i
 0: n_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}, 'cond_stage_config': {'_target_': 'nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder', 'arch': 'ViT-H-14', 'version': 'laion2b_s32b_b79k', 'freeze': True, 'layer': 'penultimate', 'cache_dir': '/checkpoints/clip'}, 'seed': 30346, 'resume_from_checkpoint': None, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'ddp_overlap': False, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'data': {'num_workers': 16, 'train': {'dataset_path': '/datasets/laion-400m/webdataset-moments-filtered-encoded/*.tar', 'augmentations': {'resize_smallest_side': 512, 'center_crop_h_w': '512, 512', 'horizontal_flip': False}, 'filterings': None}, 'webdataset': {'infinite_sampler': True, 'local_root_path': '/datasets/laion-400m/webdataset-moments-filte
 0: red-encoded'}}, 'optim': {'name': 'distributed_fused_adam', 'lr': 0.00012288, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'sched': {'name': 'WarmupHoldPolicy', 'warmup_steps': 1000, 'hold_steps': 10000000000000}, 'bucket_cap_mb': 288, 'overlap_grad_sync': True, 'overlap_param_sync': False, 'contiguous_grad_buffer': True, 'contiguous_param_buffer': True, 'store_params': True, 'dtype': 'torch.float32', 'grad_sync_dtype': 'torch.float16', 'param_sync_dtype': 'torch.float16', 'capturable': True, 'distribute_within_nodes': True}, 'target': 'nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.ddpm.MegatronLatentDiffusion', 'nemo_version': '2.0.0b0'}. Set this in model_parallel_config if using pipeline parallelism.
 0: [NeMo I 2024-09-30 15:16:50 ddpm:130] LatentDiffusion: Running in v-prediction mode
 0: [NeMo I 2024-09-30 15:16:50 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:16:50 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:16:50 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:16:51 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:16:51 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:16:51 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
12: [rank12]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [rank14]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: [rank8]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [rank13]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [rank9]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [rank15]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [rank11]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [rank10]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:16:52 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
17: [rank17]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
21: [rank21]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
18: [rank18]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
19: [rank19]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
23: [rank23]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
20: [rank20]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
16: [rank16]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
22: [rank22]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
24: [rank24]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
31: [rank31]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
27: [rank27]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
30: [rank30]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
28: [rank28]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
26: [rank26]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
29: [rank29]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
25: [rank25]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [NeMo I 2024-09-30 15:16:53 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:16:53 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:16:53 attention:436] constructing SpatialTransformer of depth 1 w/ 1280 channels and 20 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 640 channels and 10 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:16:54 attention:436] constructing SpatialTransformer of depth 1 w/ 320 channels and 5 heads
 0: [NeMo I 2024-09-30 15:16:54 utils:92] DiffusionWrapper has 865.91 M params.
 0: [NeMo I 2024-09-30 15:16:54 ddpm:168] Use system random generator since CUDA graph enabled
 4: Computed feature activations of size torch.Size([938, 2048])
 4: making attention of type 'vanilla' with 512 in_channels
 4: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 4: making attention of type 'vanilla' with 512 in_channels
 4: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 4: Loaded ViT-H-14 model config.
 3: Computed feature activations of size torch.Size([938, 2048])
 3: making attention of type 'vanilla' with 512 in_channels
 3: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 3: making attention of type 'vanilla' with 512 in_channels
 3: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 3: Loaded ViT-H-14 model config.
 5: Computed feature activations of size torch.Size([938, 2048])
 5: making attention of type 'vanilla' with 512 in_channels
 5: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 5: making attention of type 'vanilla' with 512 in_channels
 5: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 5: Loaded ViT-H-14 model config.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 0: making attention of type 'vanilla' with 512 in_channels
 0: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 0: Loaded ViT-H-14 model config.
 6: Computed feature activations of size torch.Size([938, 2048])
 6: making attention of type 'vanilla' with 512 in_channels
 6: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 6: making attention of type 'vanilla' with 512 in_channels
 6: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 6: Loaded ViT-H-14 model config.
 1: Computed feature activations of size torch.Size([938, 2048])
 1: making attention of type 'vanilla' with 512 in_channels
 1: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 1: making attention of type 'vanilla' with 512 in_channels
 1: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 1: Loaded ViT-H-14 model config.
 7: Computed feature activations of size torch.Size([938, 2048])
 7: making attention of type 'vanilla' with 512 in_channels
 7: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 7: making attention of type 'vanilla' with 512 in_channels
 7: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 7: Loaded ViT-H-14 model config.
 2: Computed feature activations of size torch.Size([938, 2048])
 2: making attention of type 'vanilla' with 512 in_channels
 2: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 2: making attention of type 'vanilla' with 512 in_channels
 2: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 2: Loaded ViT-H-14 model config.
10: Computed feature activations of size torch.Size([938, 2048])
10: making attention of type 'vanilla' with 512 in_channels
10: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
10: making attention of type 'vanilla' with 512 in_channels
10: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
10: Loaded ViT-H-14 model config.
14: Computed feature activations of size torch.Size([938, 2048])
14: making attention of type 'vanilla' with 512 in_channels
14: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
14: making attention of type 'vanilla' with 512 in_channels
14: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
14: Loaded ViT-H-14 model config.
13: Computed feature activations of size torch.Size([938, 2048])
13: making attention of type 'vanilla' with 512 in_channels
13: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
13: making attention of type 'vanilla' with 512 in_channels
13: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
13: Loaded ViT-H-14 model config.
 9: Computed feature activations of size torch.Size([938, 2048])
 9: making attention of type 'vanilla' with 512 in_channels
 9: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 9: making attention of type 'vanilla' with 512 in_channels
 9: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 9: Loaded ViT-H-14 model config.
11: Computed feature activations of size torch.Size([938, 2048])
11: making attention of type 'vanilla' with 512 in_channels
11: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
11: making attention of type 'vanilla' with 512 in_channels
11: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
11: Loaded ViT-H-14 model config.
 8: Computed feature activations of size torch.Size([938, 2048])
 8: making attention of type 'vanilla' with 512 in_channels
 8: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
 8: making attention of type 'vanilla' with 512 in_channels
 8: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
 8: Loaded ViT-H-14 model config.
12: Computed feature activations of size torch.Size([938, 2048])
12: making attention of type 'vanilla' with 512 in_channels
12: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
12: making attention of type 'vanilla' with 512 in_channels
12: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
12: Loaded ViT-H-14 model config.
15: Computed feature activations of size torch.Size([938, 2048])
15: making attention of type 'vanilla' with 512 in_channels
15: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
15: making attention of type 'vanilla' with 512 in_channels
15: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
15: Loaded ViT-H-14 model config.
26: Computed feature activations of size torch.Size([937, 2048])
26: making attention of type 'vanilla' with 512 in_channels
26: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
26: making attention of type 'vanilla' with 512 in_channels
26: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
26: Loaded ViT-H-14 model config.
18: Computed feature activations of size torch.Size([937, 2048])
18: making attention of type 'vanilla' with 512 in_channels
18: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
18: making attention of type 'vanilla' with 512 in_channels
18: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
18: Loaded ViT-H-14 model config.
21: Computed feature activations of size torch.Size([937, 2048])
21: making attention of type 'vanilla' with 512 in_channels
21: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
21: making attention of type 'vanilla' with 512 in_channels
21: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
21: Loaded ViT-H-14 model config.
17: Computed feature activations of size torch.Size([937, 2048])
17: making attention of type 'vanilla' with 512 in_channels
17: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
17: making attention of type 'vanilla' with 512 in_channels
17: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
17: Loaded ViT-H-14 model config.
23: Computed feature activations of size torch.Size([937, 2048])
23: making attention of type 'vanilla' with 512 in_channels
23: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
23: making attention of type 'vanilla' with 512 in_channels
23: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
23: Loaded ViT-H-14 model config.
22: Computed feature activations of size torch.Size([937, 2048])
22: making attention of type 'vanilla' with 512 in_channels
22: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
22: making attention of type 'vanilla' with 512 in_channels
22: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
22: Loaded ViT-H-14 model config.
30: Computed feature activations of size torch.Size([937, 2048])
30: making attention of type 'vanilla' with 512 in_channels
30: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
30: making attention of type 'vanilla' with 512 in_channels
30: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
30: Loaded ViT-H-14 model config.
27: Computed feature activations of size torch.Size([937, 2048])
27: making attention of type 'vanilla' with 512 in_channels
27: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
27: making attention of type 'vanilla' with 512 in_channels
27: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
27: Loaded ViT-H-14 model config.
16: Computed feature activations of size torch.Size([937, 2048])
16: making attention of type 'vanilla' with 512 in_channels
16: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
16: making attention of type 'vanilla' with 512 in_channels
16: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
16: Loaded ViT-H-14 model config.
29: Computed feature activations of size torch.Size([937, 2048])
29: making attention of type 'vanilla' with 512 in_channels
29: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
29: making attention of type 'vanilla' with 512 in_channels
29: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
29: Loaded ViT-H-14 model config.
28: Computed feature activations of size torch.Size([937, 2048])
28: making attention of type 'vanilla' with 512 in_channels
28: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
28: making attention of type 'vanilla' with 512 in_channels
28: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
28: Loaded ViT-H-14 model config.
19: Computed feature activations of size torch.Size([937, 2048])
19: making attention of type 'vanilla' with 512 in_channels
19: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
19: making attention of type 'vanilla' with 512 in_channels
19: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
19: Loaded ViT-H-14 model config.
20: Computed feature activations of size torch.Size([937, 2048])
20: making attention of type 'vanilla' with 512 in_channels
20: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
20: making attention of type 'vanilla' with 512 in_channels
20: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
20: Loaded ViT-H-14 model config.
31: Computed feature activations of size torch.Size([937, 2048])
31: making attention of type 'vanilla' with 512 in_channels
31: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
31: making attention of type 'vanilla' with 512 in_channels
31: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
31: Loaded ViT-H-14 model config.
24: Computed feature activations of size torch.Size([937, 2048])
24: making attention of type 'vanilla' with 512 in_channels
24: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
24: making attention of type 'vanilla' with 512 in_channels
24: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
24: Loaded ViT-H-14 model config.
25: Computed feature activations of size torch.Size([937, 2048])
25: making attention of type 'vanilla' with 512 in_channels
25: Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
25: making attention of type 'vanilla' with 512 in_channels
25: Downloading clip with ViT-H-14 laion2b_s32b_b79k /checkpoints/clip
25: Loaded ViT-H-14 model config.
 5: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 4: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 3: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 1: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 6: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 2: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 7: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
10: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
14: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 9: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
13: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
11: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
15: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 8: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
12: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
26: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
16: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
17: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
21: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
27: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
23: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
18: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
29: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
30: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
22: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
19: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
28: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
20: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
24: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
31: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
25: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
 0: [NeMo I 2024-09-30 15:17:04 ddpm:260] Loading /checkpoints/sd/512-base-ema.ckpt
 0: [NeMo I 2024-09-30 15:17:04 ddpm:261] It has 1242 entries
 0: [NeMo I 2024-09-30 15:17:05 ddpm:262] Existing model has 1240 entries
 0: [NeMo I 2024-09-30 15:17:05 ddpm:296] Deleted 686 keys from `model.diffusion_model` state_dict.
 0: [NeMo I 2024-09-30 15:17:05 ddpm:301] Restored from /checkpoints/sd/512-base-ema.ckpt with 686 missing and 2 unexpected keys
 0: [NeMo I 2024-09-30 15:17:05 ddpm:303] Missing Keys: ['model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1
 0: .norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'mod
 0: el.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.input_block
 0: s.2.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'mod
 0: el.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bi
 0: as', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.skip_co
 0: nnection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2
 0: .bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.i
 0: n_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight'
 0: , 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.trans
 0: former_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bi
 0: as', 'model.diffusion_model.input_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_bloc
 0: ks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blo
 0: cks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.
 0: to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_b
 0: locks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight'
 0: , 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.
 0: in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.1.weight', 'model.diffusion_model.middle_block.0.in_layers.1.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.2.weight', 'model.diffusion_model.middle_block.0.out_layers.2.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'mode
 0: l.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffus
 0: ion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.in_layers.0.weight', 'model.diffusion_model.middle_block.2.in_layers.0.bias', 'model.diffusion_model.middle_block.2.in_layers.1.weight', 'model.diffusion_model.middle_block.2.in_layers.1.bias', 'model.diffusion_model.middle_block.2.emb_layers.1.weight', 'model.diffusion_model.middle_block.2.emb_layers.1.bias', 'model.diffusion_model.middle_block.2.out_layers.0.weight', 'model.diffusion_model.middle_block.2.out_layers.0.bias', 'model.diffusion_model.middle_block.2.out_layers.2.weight', 'model.diffusion_model.middle_block.2.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', '
 0: model.diffusion_model.output_blocks.0.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blo
 0: cks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight
 0: ', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bi
 0: as', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',
 0:  'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.1.weight', 'model.diffusio
 0: n_model.output_blocks.4.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.di
 0: ffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.tra
 0: nsformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.2.weight', 'model.diffusion_m
 0: odel.output_blocks.5.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.tran
 0: sformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.
 0: 1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.conv.weight', 'model.diffusion_model.output_blocks.5.2.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6
 0: .1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_
 0: k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.1.weight', 'mode
 0: l.diffusion_model.output_blocks.7.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',
 0:  'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_bloc
 0: ks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.2.weight', 'model.d
 0: iffusion_model.output_blocks.8.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_block
 0: s.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output
 0: _blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.conv.weight', 'model.diffusion_model.output_blocks.8.2.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.outpu
 0: t_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0
 0: .attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.1.w
 0: eight', 'model.diffusion_model.output_blocks.10.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer
 0: _blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.we
 0: ight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusio
 0: n_model.output_blocks.11.0.out_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transf
 0: ormer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_mode
 0: l.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.1.weight', 'model.diffusion_model.out.1.bias']
 0: [NeMo I 2024-09-30 15:17:05 ddpm:305] Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
 0: Global ID: 0, local ID: 0, world size: 32
 0: Rank 0 before barrier
 1: Global ID: 1, local ID: 1, world size: 32
 1: Rank 1 before barrier
 2: Global ID: 2, local ID: 2, world size: 32
 2: Rank 2 before barrier
 3: Global ID: 3, local ID: 3, world size: 32
 3: Rank 3 before barrier
 4: Global ID: 4, local ID: 4, world size: 32
 4: Rank 4 before barrier
 5: Global ID: 5, local ID: 5, world size: 32
 5: Rank 5 before barrier
 6: Global ID: 6, local ID: 6, world size: 32
 6: Rank 6 before barrier
 7: Global ID: 7, local ID: 7, world size: 32
 7: Rank 7 before barrier
 8: Global ID: 8, local ID: 0, world size: 32
 8: Rank 8 before barrier
 9: Global ID: 9, local ID: 1, world size: 32
 9: Rank 9 before barrier
10: Global ID: 10, local ID: 2, world size: 32
10: Rank 10 before barrier
11: Global ID: 11, local ID: 3, world size: 32
11: Rank 11 before barrier
12: Global ID: 12, local ID: 4, world size: 32
12: Rank 12 before barrier
13: Global ID: 13, local ID: 5, world size: 32
13: Rank 13 before barrier
14: Global ID: 14, local ID: 6, world size: 32
14: Rank 14 before barrier
15: Global ID: 15, local ID: 7, world size: 32
15: Rank 15 before barrier
16: Global ID: 16, local ID: 0, world size: 32
16: Rank 16 before barrier
17: Global ID: 17, local ID: 1, world size: 32
17: Rank 17 before barrier
18: Global ID: 18, local ID: 2, world size: 32
18: Rank 18 before barrier
19: Global ID: 19, local ID: 3, world size: 32
19: Rank 19 before barrier
20: Global ID: 20, local ID: 4, world size: 32
20: Rank 20 before barrier
21: Global ID: 21, local ID: 5, world size: 32
21: Rank 21 before barrier
22: Global ID: 22, local ID: 6, world size: 32
22: Rank 22 before barrier
23: Global ID: 23, local ID: 7, world size: 32
23: Rank 23 before barrier
24: Global ID: 24, local ID: 0, world size: 32
24: Rank 24 before barrier
25: Global ID: 25, local ID: 1, world size: 32
25: Rank 25 before barrier
26: Global ID: 26, local ID: 2, world size: 32
26: Rank 26 before barrier
27: Global ID: 27, local ID: 3, world size: 32
27: Rank 27 before barrier
28: Global ID: 28, local ID: 4, world size: 32
28: Rank 28 before barrier
29: Global ID: 29, local ID: 5, world size: 32
29: Rank 29 before barrier
30: Global ID: 30, local ID: 6, world size: 32
30: Rank 30 before barrier
31: Global ID: 31, local ID: 7, world size: 32
31: Rank 31 before barrier
 0: :::MLLOG {"namespace": "", "time_ms": 1727709430686, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 97, "samples_count": 3072000}}
 0: Assigned 938 prompts for this worker.
 0: :::MLLOG {"namespace": "", "time_ms": 1727709719846, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 154, "samples_count": 3072000}}
 1: Assigned 938 prompts for this worker.
 2: Assigned 938 prompts for this worker.
 4: Assigned 938 prompts for this worker.
 7: Assigned 938 prompts for this worker.
 5: Assigned 938 prompts for this worker.
 3: Assigned 938 prompts for this worker.
 6: Assigned 938 prompts for this worker.
25: Assigned 937 prompts for this worker.
10: Assigned 938 prompts for this worker.
26: Assigned 937 prompts for this worker.
28: Assigned 937 prompts for this worker.
30: Assigned 937 prompts for this worker.
24: Assigned 937 prompts for this worker.
27: Assigned 937 prompts for this worker.
 9: Assigned 938 prompts for this worker.
11: Assigned 938 prompts for this worker.
14: Assigned 938 prompts for this worker.
29: Assigned 937 prompts for this worker.
13: Assigned 938 prompts for this worker.
31: Assigned 937 prompts for this worker.
15: Assigned 938 prompts for this worker.
12: Assigned 938 prompts for this worker.
 8: Assigned 938 prompts for this worker.
22: Assigned 937 prompts for this worker.
19: Assigned 937 prompts for this worker.
16: Assigned 937 prompts for this worker.
17: Assigned 937 prompts for this worker.
20: Assigned 937 prompts for this worker.
23: Assigned 937 prompts for this worker.
18: Assigned 937 prompts for this worker.
21: Assigned 937 prompts for this worker.
 0: Calculating FID activations:   0%|          | 0/30 [00:00<?, ?it/s]Calculating FID activations:   3%|▎         | 1/30 [00:00<00:20,  1.42it/s]Calculating FID activations:  23%|██▎       | 7/30 [00:00<00:02, 11.12it/s]Calculating FID activations:  37%|███▋      | 11/30 [00:00<00:01, 16.08it/s]Calculating FID activations:  50%|█████     | 15/30 [00:01<00:00, 20.78it/s]Calculating FID activations:  63%|██████▎   | 19/30 [00:01<00:00, 24.33it/s]Calculating FID activations:  77%|███████▋  | 23/30 [00:01<00:00, 27.45it/s]Calculating FID activations:  90%|█████████ | 27/30 [00:01<00:00, 29.62it/s]Calculating FID activations: 100%|██████████| 30/30 [00:01<00:00, 15.79it/s]
 0: Computed feature activations of size torch.Size([938, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1727709727310, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 71.66298219934208, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 198, "samples_count": 3072000, "metric": "FID"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1727709743195, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.18653474748134613, "metadata": {"file": "/workspace/sd/infer_and_eval.py", "lineno": 228, "samples_count": 3072000, "metric": "CLIP"}}
15: Computed feature activations of size torch.Size([938, 2048])
 6: Computed feature activations of size torch.Size([938, 2048])
 2: Computed feature activations of size torch.Size([938, 2048])
26: Computed feature activations of size torch.Size([937, 2048])
17: Computed feature activations of size torch.Size([937, 2048])
 5: Computed feature activations of size torch.Size([938, 2048])
 4: Computed feature activations of size torch.Size([938, 2048])
27: Computed feature activations of size torch.Size([937, 2048])
28: Computed feature activations of size torch.Size([937, 2048])
 7: Computed feature activations of size torch.Size([938, 2048])
29: Computed feature activations of size torch.Size([937, 2048])
 9: Computed feature activations of size torch.Size([938, 2048])
10: Computed feature activations of size torch.Size([938, 2048])
 3: Computed feature activations of size torch.Size([938, 2048])
11: Computed feature activations of size torch.Size([938, 2048])
12: Computed feature activations of size torch.Size([938, 2048])
13: Computed feature activations of size torch.Size([938, 2048])
14: Computed feature activations of size torch.Size([938, 2048])
 1: Computed feature activations of size torch.Size([938, 2048])
19: Computed feature activations of size torch.Size([937, 2048])
20: Computed feature activations of size torch.Size([937, 2048])
18: Computed feature activations of size torch.Size([937, 2048])
23: Computed feature activations of size torch.Size([937, 2048])
22: Computed feature activations of size torch.Size([937, 2048])
16: Computed feature activations of size torch.Size([937, 2048])
21: Computed feature activations of size torch.Size([937, 2048])
30: Computed feature activations of size torch.Size([937, 2048])
31: Computed feature activations of size torch.Size([937, 2048])
24: Computed feature activations of size torch.Size([937, 2048])
 8: Computed feature activations of size torch.Size([938, 2048])
25: Computed feature activations of size torch.Size([937, 2048])
 0: :::MLLOG {"namespace": "", "time_ms": 1727708338160, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py", "lineno": 186, "status": "success", "step_num": 3000}}
 0: ENDING TIMING RUN AT 2024-09-30 03:22:28 PM
 0: RESULT,stable_diffusion,2590,root,2024-09-30 02:39:18 PM
16: ENDING TIMING RUN AT 2024-09-30 03:22:28 PM
16: RESULT,stable_diffusion,2590,root,2024-09-30 02:39:18 PM
 8: ENDING TIMING RUN AT 2024-09-30 03:22:28 PM
 8: RESULT,stable_diffusion,2590,root,2024-09-30 02:39:18 PM
24: ENDING TIMING RUN AT 2024-09-30 03:22:28 PM
24: RESULT,stable_diffusion,2590,root,2024-09-30 02:39:18 PM
 1: ENDING TIMING RUN AT 2024-09-30 03:22:28 PM
 1: RESULT,stable_diffusion,2590,root,2024-09-30 02:39:18 PM
21: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
21: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
15: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
15: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
18: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
18: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
27: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
27: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
 3: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
 3: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
26: ENDING TIMING RUN AT 2024-09-30 03:22:29 PM
26: RESULT,stable_diffusion,2591,root,2024-09-30 02:39:18 PM
 5: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
 5: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
29: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
29: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
19: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
19: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
 9: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
 9: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
30: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
30: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
 7: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
 7: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
22: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
22: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
12: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
12: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
28: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
28: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
 6: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
 6: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
17: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
17: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
31: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
31: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
14: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
14: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
 2: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
 2: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
20: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
20: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
10: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
10: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
25: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
25: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
13: ENDING TIMING RUN AT 2024-09-30 03:22:30 PM
13: RESULT,stable_diffusion,2592,root,2024-09-30 02:39:18 PM
23: ENDING TIMING RUN AT 2024-09-30 03:22:31 PM
23: RESULT,stable_diffusion,2593,root,2024-09-30 02:39:18 PM
 4: ENDING TIMING RUN AT 2024-09-30 03:22:31 PM
 4: RESULT,stable_diffusion,2593,root,2024-09-30 02:39:18 PM
11: ENDING TIMING RUN AT 2024-09-30 03:22:31 PM
11: RESULT,stable_diffusion,2593,root,2024-09-30 02:39:18 PM
